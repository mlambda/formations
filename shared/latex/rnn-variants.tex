\subsection{Variation autour des RNN}

\begin{frame}
  \frametitle{Gated Recurrent Unit - GRU}
  \imgtw[0.8]{gru}
  $Z_{t}=\sigma (W_{Z}*x_{t}+U_{Z}*h_{t-1}+b_{Z})\,\,\,\,$(update gate $\approx$ input et forget gates) \\
  $R_{t}=\sigma (W_{R}*x_{t}+U_{R}*h_{t-1}+b_{R})\;\;$(reset gate $\approx$ output gate) \\
  $h_{t}=Z_{t}\circ h_{t-1}+(1-Z_{t})\circ \tanh(W_{h}*x_{t}+U_{h}(R_{t}\circ h_{t-1})+b_{h})$ \\
  Moins de paramètres que LSTM, aussi efficace dans beaucoup de tâches
\end{frame}

\begin{frame}
  \frametitle{RNN bi-directionnels}
  \imgtw[0.8]{bidirectional-rnn}
  Permet de ``gérer'' des séquences plus longues
\end{frame}

\begin{frame}
  \frametitle{Encore d'autres variantes...}
  Dans des réseaux récurrents profonds, on peut ajouter des récurrences entre les différentes couches... \\
\end{frame}

\begin{frame}
  \frametitle{jusqu'à l'indigestion...}
  \imgtw{spaghetti}
\end{frame}
