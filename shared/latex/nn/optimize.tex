\begin{frame}{Qu'est-ce qu'un hyper-paramètre~?}
  \begin{columns}
    \begin{column}[c]{0.5\textwidth}
      \V{["tikz/neural-networks/deep-ff-notext", "tw", 1] | image}
    \end{column}
    \begin{column}[c]{0.5\textwidth}
      \begin{itemize}
        \item Learning rate
        \item Taille de batch
        \item Nombre de couches
        \item Taille des couches
      \end{itemize}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Approche pour optimiser les hyper-paramètres}
  \begin{itemize}[<+->]
    \item Commencer avec une bonne baseline et une rapide recherche \frquote{à la main}
    \item Quand toute la pipeline est fonctionnelle, implémenter la recherche en utilisation une librairie spécialisée
  \end{itemize}
\end{frame}

\begin{frame}{Learning rate}
  \begin{minipage}{0.49\textwidth}
    \centering
    trop grand
  \end{minipage}\hfill
  \begin{minipage}{0.49\linewidth}
    \centering
    trop petit
  \end{minipage}\hfill

  \V{["img/learning-rate-big-small", "tw", 1] | image}
\end{frame}

\begin{frame}{Learning rate}
  Utilisation d'une échelle logarithmique (dans un premier temps) :
  \mintedpycode{gridsearch-template}
\end{frame}

\begin{frame}{Taille du batch}
  \V{["plt/batch-size-effect", "tw", 0.8] | image}
  Usuellement, des puissances de 2, pour optimiser les ressources GPU
\end{frame}

\begin{frame}{Taille des couches}
  Usuellement, des puissances de 2, pour optimiser les ressources GPU. \\
  Même intuition que pour le nombre de couches.
\end{frame}

\begin{frame}{Nombre de couches}
  \mintedpycode{addlayer-template}
\end{frame}

\begin{frame}{Pour aller plus loin}
  Entraîner des modèles coûte cher.
  Une bonne navigation de l'espace des hyper-paramètres est importante.

  $\Rightarrow$ Faire appel à une librairie dédiée comme Keras Tuner, Hyperopt, Nevergrad, Optuna ou Ray pour utiliser des processus gaussiens ou d'autres méthodes état de l'art.
\end{frame}
