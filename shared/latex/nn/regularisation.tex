\begin{frame}
  \frametitle{Sur-apprentissage (et sous-apprentissage)}
  \V{["img/overfitting-train-nzmog", "tw", 0.8] | image}
\end{frame}

\begin{frame}{Par la pénalisation de l'utilisation des paramètres}
  Coût supplémentaire pour l'utilisation des paramètres dans la fonction de perte~:
  \begin{description}[<+->]
    \item[L1] Produit beaucoup de poids à 0, s'appelle aussi Lasso~:
      \[
        E = E + \lambda ||\vect{\theta}||_1
      \]
    \item[L2] Pénalise les très gros poids, s'appelle aussi Ridge~:
      \[
        E = E + \lambda ||\vect{\theta}||_2^2
      \]
  \end{description}

  où $\lambda$ est un hyperparamètre
\end{frame}

\begin{frame}{Par early stopping}
  \V{["img/early-stopping-nzmog", "th", 0.7] | image}
\end{frame}

\begin{frame}{Par augmentation/bruitage des données}
  \V{["img/data-augmentation-nzmog", "tw", 1.0] | image}
\end{frame}

\begin{frame}{Par dropout}
  \V{["tikz/neural-networks/dropout", "tw", 1] | image}
\end{frame}

\begin{frame}{En entraînant sur plusieurs tâches}
  \V{["img/multi-task-v2", "tw", 0.8] | image}
\end{frame}

\begin{frame}{En opposant des réseaux de neurones}
  \V{["tikz/neural-networks/gan/gan", "tw", 0.8] | image}
\end{frame}
