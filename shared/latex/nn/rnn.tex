\begin{frame}{Introduction}
  Caractéristiques des réseaux récurrents~:

  \begin{itemize}[<+->]
    \item Gestion des séquences
    \item Traitement séquentiel des entrées
    \item Réplication d'un réseau autant de fois qu'il y a d'entrées
  \end{itemize}
\end{frame}

\fmg{\begin{frame}{Différence avec un réseau feedforward}
  \V{["nn-rnn-comparaison", "tw", 0.7] | image}
\end{frame}}

\begin{frame}{Modèle}
  \V{["rnn-unfold", "tw", 0.7] | image}
  $h_{t}=\sigma_{h}(Ux_{t}+Vh_{t-1}+b_{h})$ \\
  $o_{t}=\sigma_{o}(Wh_{t}+b_{o})$ \\
  \begin{itemize}
  \item $x_{t}$ : vecteur d'entrée
  \item $h_t$ : vecteur de la couche cachée
  \item $o_{t}$ : vecteur de sortie
  \item $U$, $V$, $W$, $b_h$ et $b_o$ : matrices et vecteurs (paramètres)
  \item $\sigma_{h}$ et $\sigma_o$ : fonctions d'activation (tanh)
  \end{itemize}
\end{frame}

\begin{frame}{Prédiction d'une sortie}
  Exemple~: détection de polarité.

  \begin{figure}
    \centering
    \V{["rnns/lastoutput", "th", 0.7] | image}
  \end{figure}
\end{frame}

\begin{frame}{Prédiction d'autants de sorties qu'il y a d'entrées}
  \V{["rnns/alloutputs", "tw", 0.5] | image}
\end{frame}

\begin{frame}{Prédiction d'un nombre arbitraire de sorties}
  \V{["rnns/seq2seq", "tw", 1] | image}
\end{frame}

\begin{frame}{Problème du gradient qui disparaît (vanishing gradient)}
  \V{["optimization/vanishing-gradients-rnn", "tw", 0.7] | image}
\end{frame}
