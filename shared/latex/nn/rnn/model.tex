\begin{frame}{Introduction}
  Caractéristiques des réseaux récurrents~:

  \begin{itemize}[<+->]
    \item Gestion des séquences
    \item Traitement séquentiel des entrées
  \end{itemize}
\end{frame}

\begin{frame}{Modèle}
  \V{"tikz/neural-networks/rnn/simple-unfold" | image("th", 0.25)}

  \begin{columns}
    \begin{column}{0.5\tw}
      \begin{description}
        \item[$x_t$] Entrée
        \item[$h_t$] Couche cachée
        \item[$o_t$] Sortie
        \item[$U, V, W$] Matrices de poids
        \item[$b_h, b_o$] Vecteurs de biais
        \item[$\sigma_h, \sigma_o$] Activations tanh
      \end{description}
    \end{column}
    \begin{column}{0.5\tw}
      \begin{align*}
      h_t & = \sigma_h(U x_t + V h_{t - 1} + b_h) \\
      o_t & = \sigma_o(W h_t + b_o) \\
      \end{align*}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Prédiction d'une sortie}
  Exemple~: détection de polarité.

  \begin{figure}
    \centering
    \V{"tikz/neural-networks/rnn/lastoutput" | image("th", 0.7)}
  \end{figure}
\end{frame}

\begin{frame}{Prédiction d'autant de sorties qu'il y a d'entrées}
  \V{"tikz/neural-networks/rnn/alloutputs" | image("tw", 0.5)}
\end{frame}

\begin{frame}{Prédiction d'un nombre arbitraire de sorties}
  \V{"tikz/neural-networks/rnn/seq2seq" | image("tw", 1)}
\end{frame}

\begin{frame}{Problème du gradient qui disparaît (vanishing gradient)}
  \V{"tikz/optimization/vanishing-gradients-rnn" | image("th", 0.7)}
\end{frame}
