\begin{frame}{Introduction}
  Caractéristiques des réseaux récurrents~:

  \begin{itemize}[<+->]
    \item Gestion des séquences
    \item Traitement séquentiel des entrées
    \item Réplication d'un réseau autant de fois qu'il y a d'entrées
  \end{itemize}
\end{frame}

\begin{frame}{Modèle}
  \V{["tikz/neural-networks/rnn/simple-unfold", "th", 0.25] | image}
  \begin{tabular}{ ll } 
    $x_{t}$ : entrée  & $h_{t}=\sigma_{h}(Ux_{t}+Vh_{t-1}+b_{h})$ \\
    $h_t$ : couche cachée  & $o_{t}=\sigma_{o}(Wh_{t}+b_{o})$ \\
    $o_{t}$ : sortie \\
    $U$, $V$, $W$, $b_h$ et $b_o$ : paramètres  \\
    $\sigma_{h}$ et $\sigma_o$ : activations tanh  \\
  \end{tabular}
\end{frame}

\begin{frame}{Prédiction d'une sortie}
  Exemple~: détection de polarité.

  \begin{figure}
    \centering
    \V{["tikz/neural-networks/rnn/lastoutput", "th", 0.7] | image}
  \end{figure}
\end{frame}

\begin{frame}{Prédiction d'autants de sorties qu'il y a d'entrées}
  \V{["tikz/neural-networks/rnn/alloutputs", "tw", 0.5] | image}
\end{frame}

\begin{frame}{Prédiction d'un nombre arbitraire de sorties}
  \V{["tikz/neural-networks/rnn/seq2seq", "tw", 1] | image}
\end{frame}

\begin{frame}{Problème du gradient qui disparaît (vanishing gradient)}
  \V{["tikz/optimization/vanishing-gradients-rnn", "tw", 0.7] | image}
\end{frame}
