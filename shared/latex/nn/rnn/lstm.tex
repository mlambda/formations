\begin{frame}{Principe}
  Lutter contre les gradients qui disparaissent~:
  \begin{itemize}[<+->]
    \item Introduction d'une mémoire
    \item Update additive de la mémoire (gradient plus facile à conserver)
    \item «~Protection~» de cette mémoire par des portes
  \end{itemize}
\end{frame}

\begin{frame}{Vanishing gradient résolu (ou presque)}
  \V{"tikz/optimization/vanishing-gradients-lstm" | image("th", 0.7)}
\end{frame}

\begin{frame}{Détails théoriques}
  \V{"tikz/neural-networks/rnn/lstm" | image("th", 0.37)} 
  \begin{tabular}{ lrll } 
    Forget gate & $F_t=$ & $\sigma(W_F \times x_t + U_F \times h_{t - 1} + b_F)$ & $\in[0;1]$ \\
    Input gate & $I_t=$ &  $\sigma(W_I \times x_t + U_I \times h_{t - 1} + b_I)$ & $\in[0;1]$ \\
    Output gate & $O_t=$ & $\sigma(W_O \times x_{t} + U_O \times h_{t - 1} + b_O)$ & $\in[0;1]$ \\
  \end{tabular}
  \begin{tabular}{ ll } 
    \hline
    Accumulateur & $\boldsymbol{c_t = F_t \times c_{t - 1} + I_t \times \tanh(W_c \times x_t + U_c \times h_{t-1} + b_c)}$ \\
    \hline
    Hidden & $h_t = O_t \times \tanh(c_t)$ \\
    Output & $o_t = f(W_o \times h_t + b_o)$ \\
  \end{tabular}
\end{frame}
