\begin{frame}{Lien avec la régression linéaire}
  \V{"img/regression-10" | image("tw", 0.9)}
\end{frame}

\begin{frame}{Lien avec la régression linéaire}
  \V{"img/separable-problem-linear-solution-nzmog" | image("tw", 0.9)}
\end{frame}

\begin{frame}{Lien avec la régression linéaire}
  \V{"img/xor-nzmog" | image("tw", 0.9)}
\end{frame}

\begin{frame}{Lien (ténu) avec la biologie}
  \V{"img/biological-neuron" | image("tw", 1)}
\end{frame}

\begin{frame}{Modélisation d'un neurone}
  \V{"tikz/neural-networks/model" | image("tw", 0.9)}
\end{frame}

\begin{frame}{Modélisation d'un réseau de neurones}
  Agencement de beaucoup de neurones~:
  \begin{description}
    \item[En parallèle] Calculent des résultats indépendamment dans la même couche
    \item[En série] Prennent en entrée les résultats des neurones de la couche précédente
  \end{description}
\end{frame}

\begin{frame}{Deux types de neurones}
  On distingue deux types de neurones~:
  \begin{description}
    \item[Neurones cachés] Neurones des couches intermédiaires. Améliorent l'expressivité du modèle
    \item[Neurones de sortie] Neurones de la couche finale. Contraints par le type de sortie attendu
  \end{description}
\end{frame}

\begin{frame}{Réseau sans couche cachée}
  \centering
  \V{"tikz/neural-networks/shallow-ff" | image("tw", 1)}
\end{frame}

\begin{frame}{Réseau avec une couche cachée}
  \centering
  \V{"tikz/neural-networks/one-hidden-layer-ff" | image("tw", 1)}
\end{frame}

\begin{frame}{Réseau profond}
  \centering
  \V{"tikz/neural-networks/deep-ff" | image("tw", 1)}
\end{frame}

\begin{frame}{Un potentiel infini}
  \textbf{Kurt Hornik, 1991~:} Théorème d'approximation universelle
  \V{"tikz/activations/approximation" | image("tw", 1)}

  \bluelink{https://youtu.be/5oEBiico-I4}{Illustration d'une approximation}
\end{frame}

\begin{frame}{Modélisation matricielle --- Échantillon}
  Représentable sous forme de vecteur à $d$ colonnes correspondant à $d$ caractéristiques~:

  \[
    \vect{x} = \begin{bmatrix}
      x_1 & x_2 & \dots & x_d
    \end{bmatrix}
  \]

  Voire même de matrice dans le cas d'un batch (groupe d'échantillons)~:

  \[
    \vect{X} = \begin{bmatrix}
      X_{1,1} & X_{1,2} & \dots & X_{1,d} \\
      X_{2,1} & X_{2,2} & \dots & X_{2,d}
    \end{bmatrix}
  \]
\end{frame}

\begin{frame}{Modélisation matricielle --- Poids}
  Représentables sous forme de matrice de poids~:

  \[
    \vect{\theta} = \begin{bmatrix}
      \theta_{0,1} & \theta_{0,2} & \dots  & \theta_{0,n} \\
      \theta_{1,1} & \theta_{1,2} & \dots  & \theta_{1,n} \\
      \vdots & \vdots & \ddots & \vdots \\
      \theta_{d,1} & \theta_{d,2} & \dots  & \theta_{d,n}
    \end{bmatrix}
  \]
\end{frame}

\begin{frame}{Modélisation matricielle complète}
  \footnotesize
  \begin{align*}
    \vect{O} & = \sigma (\begin{bmatrix}\begin{bmatrix}1 \\ \vdots \\ 1\end{bmatrix}\vect{X}\end{bmatrix}\vect{\theta}) \\
    & = \sigma \left(
    \begin{bmatrix}
      1 & X_{1,1} & X_{1,2} & \dots & X_{1,d} \\
      1 & X_{2,1} & X_{2,2} & \dots & X_{2,d}
    \end{bmatrix}
    \begin{bmatrix}
      \theta_{0,1} & \theta_{0,2} & \dots  & \theta_{0,n} \\
      \theta_{1,1} & \theta_{1,2} & \dots  & \theta_{1,n} \\
      \vdots & \vdots & \ddots & \vdots \\
      \theta_{d,1} & \theta_{d,2} & \dots  & \theta_{d,n}
    \end{bmatrix}
    \right) \\
    & = \begin{bmatrix}
      O_{1,1} & O_{1,2} & \dots & O_{1,n} \\
      O_{2,1} & O_{2,2} & \dots & O_{2,n}
    \end{bmatrix}
  \end{align*}
  \begin{description}
    \item[$\vect{X}$] Données en entrée de dimension $d$
    \item[$\vect{\theta}$] Paramètres à trouver des $n$ neurones de notre modèle
    \item[$\vect{\sigma}$] Fonction d'activation
    \item[$\vect{O}$] Sortie du réseau
  \end{description}
\end{frame}

\begin{frame}{Fonctions d'activation --- Critères de choix}
  \begin{itemize}
    \item Propriétés mathématiques (conservation du gradient)
    \item Propriétés d'apprentissage (éviter la création de poids morts)
    \item Rapidité de calcul
    \item Intervalle de sortie pour la dernière couche
  \end{itemize}
\end{frame}

\begin{frame}{Fonctions d'activation --- Les plus classiques}
  \begin{itemize}
  \item Sigmoïde
  \item Tanh
  \item Softmax
  \item ReLU
  \item …
  \end{itemize}
\end{frame}

\begin{frame}{Fonctions d'activation --- Sigmoïde}
  \begin{center}
    \V{"tikz/activations/sigmoid" | image("tw", 1)}
  \end{center}

  \begin{columns}
    \begin{column}{.5\tw}
      \centering
      \textbf{Définition}
      \[
        \phi(x) = \frac{1}{1 + e^{-x}}
      \]
      \end{column}
    \begin{column}{{.5\tw}}
      \centering
      \textbf{Dérivée}
      \[
        \phi'(x) = \phi(x)(1 - \phi(x))
      \]
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Fonctions d'activation --- Tangente hyperbolique}
  \begin{center}
    \V{"tikz/activations/tanh" | image("tw", 1)}
  \end{center}

  \begin{columns}
    \begin{column}{.5\tw}
      \centering
      \textbf{Définition}
      \[
        \tanh(x) = \frac{1 - e^{-2x}}{1 + e^{-2x}}
      \]
      \end{column}
    \begin{column}{{.5\tw}}
      \centering
      \textbf{Dérivée}
      \[
        \tanh'(x) = 1 - \tanh^2(x)
      \]
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Fonctions d'activation --- ReLU}
  \begin{center}
    \V{"tikz/activations/relu" | image("tw", 0.6)}
  \end{center}

  \begin{columns}
    \begin{column}{.5\tw}
      \centering
      \textbf{Définition}
      \[
        \ReLU(x) = \begin{cases}
                     0 & x \leq 0 \\
                     x & x > 0 \\
                   \end{cases}
      \]
      \end{column}
    \begin{column}{{.5\tw}}
      \centering
      \textbf{Dérivée}
      \[
        \ReLU'(x) =  \begin{cases}
                       0 & x \leq 0 \\
                       1 & x > 0\\
                     \end{cases}
      \]
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Fonctions d'activation --- Approximation d'une fonction}
  \centering
  \V{"tikz/activations/approximation" | image("tw", 1)}
  {\small
    \begin{align*}
      n_1 & = \ReLU(-5x - 7.7) & n_2 & = \ReLU(-1.2x - 1.3) & n_3 & = \ReLU(1.2x - 1) \\
      n_4 & = \ReLU(1.2x - 0.2) & n_5 & = \ReLU(2x - 1.1) & n_6 & = \ReLU(5x - 5) \\
    \end{align*}
  }
\end{frame}

\begin{frame}{Fonctions d'activation --- Softmax}
  \begin{description}
    \item[Définition] \[\softmax(x)_i = \frac{\exp(x_i)}{\sum_{k = 1}^n{\exp(x_k)}}\]
    \item[Propriété] \[\sum_{i = 1}^n{\softmax(x)_i} = 1\]
    \item[Gradient] \[
      \frac{\partial \softmax(x)_i}{\partial x_j} =
        \begin{cases}
          \softmax(x)_i(1 - \softmax(x)_j) & i = j \\
          -\softmax(x)_i\softmax(x)_j & i \neq j
        \end{cases}
      \]
  \end{description}
  
\end{frame}
