\begin{frame}{Lien avec la régression linéaire}
  \V{"img/regression-10" | image("tw", 0.9)}
\end{frame}

\begin{frame}{Lien avec la régression linéaire}
  \V{"img/separable-problem-linear-solution-nzmog" | image("tw", 0.9)}
\end{frame}

\begin{frame}{Lien avec la régression linéaire}
  \V{"img/xor-nzmog" | image("tw", 0.9)}
\end{frame}

\begin{frame}{Lien (ténu) avec la biologie}
  \V{"img/biological-neuron" | image("tw", 1)}
\end{frame}

\begin{frame}{Modélisation d'un neurone}
  \V{"tikz/neural-networks/model" | image("tw", 0.9)}
\end{frame}

\begin{frame}{Modélisation d'un réseau de neurones}
  Agencement de beaucoup de neurones~:
  \begin{description}
    \item[En parallèle] Calculent des résultats indépendamment dans la même couche
    \item[En série] Prennent en entrée les résultats des neurones de la couche précédente
  \end{description}
\end{frame}

\begin{frame}{Deux types de neurones}
  On distingue deux types de neurones~:
  \begin{description}
    \item[Neurones de sortie] Neurones de la couche finale. Contraints par le type de sortie attendu
    \item[Neurones cachés] Neurones des couches intermédiaires. Améliorent l'expressivité du modèle
  \end{description}
\end{frame}

\begin{frame}{Réseau sans couche cachée}
  \centering
  \V{"tikz/neural-networks/shallow-ff" | image("tw", 1)}
\end{frame}

\begin{frame}{Réseau avec une couche cachée}
  \centering
  \V{"tikz/neural-networks/one-hidden-layer-ff" | image("tw", 1)}
\end{frame}

\begin{frame}{Réseau profond}
  \centering
  \V{"tikz/neural-networks/deep-ff" | image("tw", 1)}
\end{frame}

\begin{frame}{Fonctions d'activation --- ReLU}
    \begin{center}
      \V{"tikz/activations/relu" | image("tw", 0.6)}
    \end{center}
  
    \begin{columns}
      \begin{column}{.5\tw}
        \centering
        \textbf{Définition}
        \[
          \ReLU(x) = \begin{cases}
                       0 & x \leq 0 \\
                       x & x > 0 \\
                     \end{cases}
        \]
        \end{column}
      \begin{column}{{.5\tw}}
        \centering
        \textbf{Dérivée}
        \[
          \ReLU'(x) =  \begin{cases}
                         0 & x \leq 0 \\
                         1 & x > 0\\
                       \end{cases}
        \]
      \end{column}
    \end{columns}
\end{frame}
  
\begin{frame}{Un potentiel infini}
  \textbf{Kurt Hornik, 1991~:} Théorème d'approximation universelle
  \V{"tikz/activations/approximation" | image("tw", 1)}

  \bluelink{https://youtu.be/5oEBiico-I4}{Illustration d'une approximation}
\end{frame}

\begin{frame}{Fonctions d'activation --- Approximation d'une fonction}
  \centering
  \V{"tikz/activations/approximation" | image("tw", 1)}
  {\small
    \begin{align*}
      n_1 & = \ReLU(-5x - 7.7) & n_2 & = \ReLU(-1.2x - 1.3) & n_3 & = \ReLU(1.2x - 1) \\
      n_4 & = \ReLU(1.2x - 0.2) & n_5 & = \ReLU(2x - 1.1) & n_6 & = \ReLU(5x - 5) \\
    \end{align*}
  }
\end{frame}
