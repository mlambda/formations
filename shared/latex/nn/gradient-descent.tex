
\begin{frame}{Principe}
  \begin{columns}
    \begin{column}{0.5\tw}
      Calcul du gradient de l'erreur par rapport aux paramètres:
      \[
        \frac{\partial{E}}{\partial{\theta_i}}
      \]
      Mise à jour :
      \[
        \theta_i \leftarrow \theta_i - \gamma\frac{\partial{E}}{\partial{\theta_i}}
      \]
      où $0 < \gamma < 1$ (pas d'apprentissage)
    \end{column}
    \begin{column}{0.5\tw}
      \V{["plt/linear-regression-error-surface", "tw", 1] | image}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Illustration}
  \bluelink{https://youtu.be/gK2dIZCbLZ0}{Illustration de la descente de gradient.}
\end{frame}

\begin{frame}{Algorithme}
  \begin{enumerate}[<+->]
    \item Initialisation aléatoire des paramètres $\theta_i$ du modèle $f_{\theta}, i\in[1..k]$
    \item Pour $n$ itérations~:
      \begin{itemize}
        \item Échantillonnage des données en batch de $m$ exemples
        \item Pour les $b$ batchs~:
        \begin{itemize}
          \item \textbf{Forward}~: Calcul des prédictions $\hat{Y_j} = f_{\theta}(X_j), j\in[1..m]$
          \item Calcul des erreurs $|\hat{Y_j}-Y_j|, j\in[1..m]$
          \item \textbf{Backward}~: Mise à jour :
          \[
            \theta_i \leftarrow \theta_i - \gamma\frac{\partial{E}}{\partial{\theta_i}}
          \]
          pour chaque paramètre $\theta_i$ de notre modèle $f_\theta$ \\$\,$

          \item Vérification des critères d'arrêt
        \end{itemize}
      \end{itemize}
  \end{enumerate}

  Où $k$, $n$, $m$ et $\gamma$ sont des hyperparamètres du modèle.
\end{frame}


\begin{frame}{Exemple de dérivation --- Régression linéaire}
  \begin{minipage}[l]{0.49\linewidth}
    \begin{align*}
      E & = \frac{1}{2n} \sum_{i=1}^n( \hat{y_i} - y_i )^2 \\
      E & = \frac{1}{2n}\sum_{i=1}^n( (\theta_0 + \theta_1 x_i) - y_i )^2 \\
      & ... \\
      \frac{\partial E}{\partial \theta_0} & = \frac{1}{n}\sum^n_{i=1}(\theta_0 + \theta_1 x_i - y_i) \\
      \frac{\partial E}{\partial \theta_1} & = \frac{1}{n}\sum^n_{i=1}(\theta_0 + \theta_1 x_i - y_i) x_i \\
    \end{align*}
  \end{minipage}\hfill
  \begin{minipage}[c]{0.49\linewidth}
    \begin{center}
      $\boxed{\hat{y_i} = \theta_0 + \theta_1 x_i}$
    \end{center}

    \begin{center}
      $\boxed{U^{2\;\prime}=2U' \times U}$
    \end{center}

    \begin{center}
      \textbf{Mise à jour}

      \begin{align*}
        \theta_0 & \leftarrow \theta_0 - \gamma\frac{\partial{E}}{\partial{\theta_0}} \\
        \theta_1 & \leftarrow \theta_1 - \gamma\frac{\partial{E}}{\partial{\theta_1}} \\
      \end{align*}

      où $0 < \gamma < 1$ (pas d'apprentissage)
    \end{center}
  \end{minipage}\hfill
\end{frame}
