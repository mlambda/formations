\begin{frame}{Rétropropagation du gradient}
  Pour calculer le gradient dans des réseaux profonds, on utilise la rétropropagation~:

  \begin{itemize}
    \item Algorithme de programmation dynamique
    \item Commence par calculer le gradient de la dernière couche
    \item Calcule le gradient d'une couche en se basant sur le gradient de la couche suivante
    \item Utilise la règle de dérivation des fonctions composées pour le faire
  \end{itemize}

  $\Rightarrow$ Évite de recalculer des sous-expressions.
\end{frame}

\begin{frame}{Disparition du gradient (ou son explosion)}
  \V{["tikz/optimization/vanishing-gradients", "tw", 1.0] | image}
\end{frame}

\begin{frame}{Disparition des gradients}
  Vanishing Gradient $\Rightarrow$ utilisation de ReLu plutôt que les sigmoïdes
  \vfill
  \V{["tikz/activations/relu", "th", 0.6] | image}
\end{frame}

\begin{frame}{Explosion des gradients}
  Exploding Gradient $\Rightarrow$ Gradient clipping
  \V{["plt/gradient-clipping", "th", 0.5] | image}
  Dans le cadre des réseaux réccurents $\Rightarrow$ initialisation avec des matrices orthogonales
\end{frame}

\begin{frame}{Algorithme d'optimisation}
  Optimisateur plus rapide que SGD
  \begin{itemize}[<+(1)->]
    \item Adaptative Gradient Algorithm (AdaGrad)
    \item Root Mean Square Propagation (RMSProp)
    \item Adaptative Moment Estimation (Adam)    $\Leftarrow$
    \item ... AdaBound (2019) ?
  \end{itemize}
\end{frame}

\begin{frame}{Root Mean Square Propagation (RMSProp)}
  \begin{itemize}[<+->]
    \item Un pas d'apprentissage par paramètre
    \item Empêche les grandes mises à jour fréquentes~:
      \begin{itemize}
        \item Calcule une moyenne glissante exponentielle~:

        \[v_t(\theta) \leftarrow \gamma v_{t-1}(\theta) + (1 - \gamma)(\nabla_\theta L)^2\]

        \item Met à jour chaque gradient en le divisant par sa moyenne glissante~:

        \[\theta \leftarrow \theta - \frac{\eta}{\sqrt{v_t(\theta)}}\nabla_\theta L\]
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Adaptative Gradient Algorithm (AdaGrad)}
  Pénalise les mises à jour fréquentes comme RMSProp mais calcule le facteur différement~:

  \[
    \theta \leftarrow \theta - \frac{\eta}{\sqrt{\sum_{\tau=1}^t(\nabla_{\theta,\tau} L)^2}}\nabla_{\theta} L
  \]
\end{frame}

\begin{frame}{Adam}
  Similaire à RMSProp dans l'algorithme. Garde deux valeurs au lieu d'une en mémoire pour chaque paramètre~: la moyenne $m$ et la variance $v$ de son gradient.

  \begin{align*}
    m_\theta^{(t+1)} &\leftarrow \beta_1m_\theta^{(t)} + (1 - \beta_1)\nabla_\theta L^{(t)} \\
    v_\theta^{(t+1)} &\leftarrow \beta_2v_\theta^{(t)} + (1 - \beta_2)(\nabla_\theta L^{(t)})^2 \\
    \hat{m_\theta} &= \frac{m_\theta^{(t+1)}}{1 - \beta_1} \\
    \hat{v_\theta} &= \frac{v_\theta^{(t+1)}}{1 - \beta_2} \\
    \theta^{(t+1)} &\leftarrow \theta^{(t)} - \eta\frac{\hat{m_\theta}}{\sqrt{\hat{v_\theta}} + \epsilon} \\
  \end{align*}

  $\beta_1$ et $\beta_2$ sont des facteurs d'oubli, $\epsilon$ est une petite valeur
\end{frame}
