\begin{frame}{Cellule transformer}
    \V{"tikz/neural-networks/transformer/transformer-cell" | image("th", 0.7)}
\end{frame}

\begin{frame}{Méchanisme d'auto-attention}
    \V{"tikz/neural-networks/transformer/self-attention" | image("tw", 0.8)}
    \[\vec{r}_i=\sum_{j=1}^{n} w_j*\vec{v}_j\]
    où $\left \{w_j\right \}$ sont les poids d'attention entre $\vec{q}_i$ et tous les $\vec{k}_j$
\end{frame}

\begin{frame}{Multiples têtes d'auto-attention}
    \V{"tikz/neural-networks/transformer/multi-head-attention" | image("th", 0.75)}
\end{frame}
