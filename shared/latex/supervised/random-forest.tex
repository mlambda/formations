\begin{frame}{Introduction}
  \begin{itemize}
    \item Les arbres de décision sur-apprennent facilement
    \item Ils sont rapides à apprendre
    \item En combiner beaucoup est faisable et réduit la variance
  \end{itemize}
  $\Rightarrow$ Création d'une forêt (ensemble d'arbres) aléatoire
\end{frame}

\begin{frame}{But}
  Produire des arbres décorrélés et moyenner leurs prédictions pour
  réduire la variance.
\end{frame}

\begin{frame}{Outil 1 — Bagging (échantillonnage de lignes)}
  \textbf{B}oostrap \textbf{agg}regat\textbf{ing} (Bagging) :
  \begin{itemize}
    \item Tirer un échantillon du dataset avec replacement
    \item Entraîner un arbre sur cet échantillon
    \item Répéter $B$ fois
  \end{itemize}
  Le bagging s'appelle aussi échantillonnage de lignes.
\end{frame}

\begin{frame}{Outil 2 — Échantillonnage de colonnes}
  \begin{itemize}
    \item À chaque split, considérer seulement un sous-ensemble des caractéristiques
    \item Valeurs conseillées~:
      \begin{description}
        \item[Classification] $\lvert\sqrt m\rvert$ caractéristiques par split
        \item[Regréssion] $\lvert\frac{m}{3}\rvert$ caractéristiques par split, 5 exemples par nœud minimum
      \end{description}
  \end{itemize}
\end{frame}

\begin{frame}{Illustration}
  \V{"img/random-forest-full" | image("tw", 1)}
\end{frame}

\begin{frame}{Conclusion}
  \begin{itemize}
    \item Les arbres sont interprétables, rapides à entraîner, combinables
    \item Une forêt aléatoire combine des arbres limités en un prédicteur polyvalent
  \end{itemize}
\end{frame}
