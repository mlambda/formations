\begin{frame}
  \frametitle{Régles fondamentales}
  \begin{itemize}
  \item Règle de sommation: $$p(X)=\sum_{Y}p(X,Y)$$
  \item Règle du produit: $$p(X,Y)=p(Y|X)p(X)$$
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Indépendance conditionnelle}
    Soit $X,Y,Z$ des variables aléatoires. \\
    $X$ et $Y$ sont indépendantes conditionnellement à $Z$ si et seulement si $$P (X,Y | Z)=P(X|Z)P(Y|Z)$$ \\
    Ce qui implique que $$P(X|Y,Z)=P (X|Z)$$
\end{frame}

\begin{frame}
  \frametitle{Indépendants et Identiquement Distribués (i.i.d)}
  \begin{center}
  Indépendance mutuelle des variables\\
  ET\\
  Toutes les variables ont la même distribution de probabilité.
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Modèlisation Statistique}
  On cherche $P_{\theta}$, un modèle probabiliste, pour expliquer nos données.
\end{frame}

\begin{frame}
  \frametitle{Modèle Statistique : Bernoulli}
Soit $X$ une variable aléatoire binaire et $\theta \in [0,1]$:
\[
\left\{
    \begin{array}{ll}
        P(X=1)=\theta\\
        P(X=0)=1-\theta
    \end{array}
\right.
\]
que l'on peut noter : 
$$p(X=x;\theta) = \theta^x(1-\theta)^{1-x}$$
ou encore :
$$X \sim Ber(\theta)$$
\end{frame}

\begin{frame}
  \frametitle{Modèle Statistique : Binomiale}
  $\text{Bin}(\theta, N)$ est la somme de $n$ i.i.d.\ Bernoulli variables aléatoire de même paramètre $\theta$ :
  $$P(N=k)=\binom{n}{k} {\theta}^k (1-\theta)^{n-k}.$$
\end{frame}

\begin{frame}
  \frametitle{Modèle Statistique : Multinomiale}
    Soit $Z$ une variable aléatoire pouvant prendre $K$ valeurs possibles. $Z$ peut alors être representée par $K$ variables binaires $X=(X_1,X_2,\ldots,X_K)$ où l'évenement $\{Z=k\}$ correspond à $$\{X_k=1 \text{ et } X_l=0, \forall l \neq k\}.$$ \\
    Si on paramètrise $P(Z=k)$ par un parmètre $\theta_k \in [0,1]$, alors 
    $$P(X_k=1) = \theta_k \ \ \forall k=1,2,\ldots,K,$$
    où $\sum_{k=1}^K \theta_k=1$. \\
    La distribution de probabilité sur $\x=(x_1,\ldots, x_k)$ peut donc être écrite ainsi
    $$p(\mathbf{x};\bm{\theta})=\prod_{k=1}^K\theta_k^{x_k}$$
    On notera cette distribution $\mathcal{M}(1,\theta)$ 
\end{frame}

\begin{frame}
  \frametitle{Modèle Statistique : Gaussienne}
  La distribution Gaussienne (ou Loi Normale) d'une variable aléatoire $X \in \mathbb{R}$ :
    \[
    \mathcal{N}(x; \, \mu,\sigma^2)=\frac{1}{(2\pi\sigma^2)^{1/2}}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)
    \]
    où $\mu$ est la moyenne et $\sigma^2$ la variance de $X$ \\
    \newline
    Pour un vecteur $X$ de dimension $d$, la loi gaussienne multidimensionnelle s'écrit alors :
    \[
    \mathcal{N}(\mathbf{x}|\bm{\mu},\bm{\Sigma})=\frac{1}{(2\pi)^{d/2}}\frac{1}{|\bm{\Sigma}|^{1/2}}\exp\left(-\frac{1}{2}(\mathbf{x}-\bm{\mu})^T\bm{\Sigma}^{-1}(\mathbf{x}-\bm{\mu})\right)
    \]
    où $\bm{\mu}$ est un vecteur de dimension $d$, $\bm{\Sigma}$ est une matrice symétrique définie positive $d\times d$, et $|\bm{\Sigma}|$ le déterminant de $\bm{\Sigma}$. \\
    $\bm{\Sigma}$ correspond à la matrice de covariance de $X$.
\end{frame}

\begin{frame}
  \frametitle{Maximum de vraisemblance (MLE)}
  Soit $x_1,x_2,\ldots,x_n$ un ensemble de $n$ i.i.d observations générées par une distribution $p(x_1,x_2,\ldots,x_n;\theta)$, on cherche le paramètre $\hat{\theta}$ tel que :
  $$\hat{\theta} = \argmax_\theta p(x_1,x_2,\ldots,x_n; \, \theta)$$
  Appelons \textit{vraisemblance} la fonction
  $$\mathcal{L}(\theta) = p(x_1,x_2,\ldots,x_n; \, \theta)$$
  Comme $x_1,x_2,\ldots,x_n$ est i.i.d :
  $$\mathcal{L}(\theta) = \prod_{i=1}^n p(x_i; \, \theta)$$
\end{frame}

\begin{frame}
  \frametitle{Maximum de vraisemblance (MLE)}
  Pour des raisons pratiques, on manipule le logarithme de la vraisemblance que l'on appelera \textit{log-vraisemblance}:
  \[
      l(\theta) = \sum_{i=1}^n \log p(x_i; \, \theta)
  \]
\end{frame}

\begin{frame}
  \frametitle{MLE pour Bernoulli}
Soit $N$ observations $x_1,x_2,\ldots,x_N$ d'une variable aléatoire $X$ suivant une loi de Bernoulli $Ber(\theta)$.
\begin{align*}
\ell(\theta) 	&= \sum_{i=1}^N \log p(x_i; \, \theta) \\
				&= \sum_{i=1}^N \log \theta^{x_i}(1-\theta)^{1-x_i}\\
				&= n\log(\theta)+(N-n)\log(1-\theta)
\end{align*} 
où $n=\sum_{i=1}^N x_i$.\\ 
\end{frame}

\begin{frame}
  \frametitle{MLE pour une loi de Bernoulli}
Comme $l(\theta)$ est concave, elle n'a qu'un seul maximum, au zéro de sa dérivé :
$$\nabla \ell(\theta)=\frac{\partial}{\partial\theta} \ell(\theta) = \frac{n}{\theta}-\frac{N-n}{1-\theta}.$$  
Ainsi $\nabla \ell(\theta) = 0 \Longleftrightarrow \theta =\frac{n}{N}$, donc
$$\hat{\theta} = \frac{n}{N} = \frac{x_1+x_2+\cdots+x_N}{N}.$$
\end{frame}

\begin{frame}
  \frametitle{MLE pour une loi Normale}
  Soit $N$ observations $x_1,x_2,\ldots,x_N$ d'une variable aléatoire $X$ suivant une distribution Gaussienne $\mathcal{N}(\mu,\sigma^2)$
\begin{align*}
\ell(\mu,\sigma^2) 	&= \sum_{i=1}^n \log p(x_i; \, \mu,\sigma^2) \\
				&= \sum_{i=1}^n \log\left[ \frac{1}{(2\pi\sigma^2)^{1/2}}\exp\left(-\frac{(x_i-\mu)^2}{2\sigma^2}\right)\right]\\
				&= -\frac{n}{2}\log(2\pi)-\frac{n}{2}\log(\sigma^2)-\frac{1}{2}\sum_{i=1}^n \frac{(x_i-\mu)^2}{\sigma^2}.				
\end{align*} 
\end{frame}

\begin{frame}
  \frametitle{MLE pour une loi Normale}
  On doit alors chercher le couple $(\mu, \sigma^2)$ pour lequel $l(\mu,\sigma^2)$ soit maximum. L'utilisation des dérivées partielles par rapport à $\mu$ et $\sigma^2$ respectivement nous donne le couple optimal $(\hat{\mu},\hat{\sigma}^2)$ :
\begin{align*}
\hat{\mu}&=\frac{1}{n}\sum_{i=1}^n x_i \\
\hat{\sigma}^2&=\frac{1}{n}\sum_{i=1}^n \left(x_i-\hat{\mu}\right)^2
\end{align*}
\end{frame}
