\begin{frame}{Introduction}
  AlphaGo un bon exemple de développement industriel en deep learning.
  \begin{itemize}[<+->]
  \item Réseaux de neurones profonds
  \item Monte Carlo Tree Search
  \item Apprentissage par renforcement
  \item Très gros entrainements (> 30 M\$)
  \end{itemize}
\end{frame}

\begin{frame}{Go}
  \begin{minipage}[c]{0.6\linewidth}
    \V{["img/go-regles", "tw", 1] | image}
  \end{minipage}\hfill
  \begin{minipage}[c]{0.33\linewidth}
    \begin{center}
      \underline{Go Vs Echecs} \\
      \textbf{Le nombre de coups} \\
      $\approx$ 200 contre $\approx$ 35 \\
      \textbf{Le nombre de tours} \\
      $\approx$ 300 contre $\approx$ 40 \\
      $\Rightarrow$ $\approx 10^{120}\lll \;\approx 10^{690}$
    \end{center}
  \end{minipage}\hfill
\end{frame}

\begin{frame}{Réseau de value}
  \begin{minipage}[c]{0.68\linewidth}
    \begin{itemize}[<+->]
      \item $\softmax(\text{convolutions}_{13}(19 \times 19 \times 48\text{bits}))$
      \item 48bits : input sur la position, la configuration
      \item Appris sur données supervisées d'abord
      \item Amélioré par renforcement ensuite
    \end{itemize}
  \end{minipage}\hfill
  \begin{minipage}[c]{0.31\linewidth}
  \V{["img/alphago-value", "tw", 1] | image}
  \end{minipage}\hfill
\end{frame}

\begin{frame}{Réseau de policy}
  \begin{minipage}[c]{0.50\linewidth}
    \V{["img/alphago-policy", "th", 0.8] | image}
  \end{minipage}\hfill
  \begin{minipage}[c]{0.49\linewidth}
    \begin{itemize}[<+->]
      \item Appris sur données supervisées d'abord
      \item Puis amélioré par self-play
      \item A l'aide d'une recherche MCTS boostée par les deux réseaux (value \& policy)
    \end{itemize}
  \end{minipage}\hfill
\end{frame}

\begin{frame}{Monte-Carlo Tree Search + réseaux de value \& policy}
  \V{["img/alphago-mcts", "tw", 0.8] | image}
  \begin{itemize}[<+->]
    \item Sélection par maximum de Q-value + u(P) où P est une proba à priori pour chaque coup
    \item Expansion en fonction de la policy qui produit des probabilités stockées pour chaque action
  \end{itemize}
\end{frame}

\begin{frame}{Phases d'apprentissage}
  \begin{description}[<+->]
    \item[Supervisé] 3 semaines (50 TPU) 
    \item[Renforcement] 1 journée (5000 TPU)
  \end{description}

  \onslide<+->{À titre indicatif, comparaison entre TPU et Core i7~:}

  \begin{description}[<+->]
    \item[50 TPU] $\approx$ 2000 TFLOPS
    \item[Core i7] $\approx$ 0.07 TFLOPS (donc il en faudrait $\approx$ 30 000)
  \end{description}
\end{frame}

\begin{frame}{Évaluation de performances au Go}
  \begin{minipage}[c]{0.7\linewidth}
    \V{["img/go-winrate-dan", "tw", 1] | image}
  \end{minipage}\hfill
  \begin{minipage}[c]{0.26\linewidth}
    \begin{adjustbox}{scale=0.8}
      \begin{tabular}{| r | l |}
        \hline
        EGF & Classement \\ \hline
        2940 & 9 dan pro \\ \hline
        2820 & 8 dan amateur \\ \hline
        2700 & 7 dan amateur \\ \hline
        2600 & 6 dan amateur \\ \hline
        2500 & 5 dan \\ \hline
        2400 & 4 dan \\ \hline
        2300 & 3 dan \\ \hline
        2200 & 2 dan \\ \hline
        2100 & 1 dan \\ \hline
        2000 & 1 kyu \\ \hline
        1900 & 2 kyu \\ \hline
        1800 & 3 kyu \\ \hline
        1500 & 6 kyu \\ \hline
        1000 & 11 kyu \\ \hline
        500 & 16 kyu \\ \hline
        100 & 20 kyu \\ \hline
      \end{tabular}
    \end{adjustbox}
  \end{minipage}\hfill
\end{frame}

\begin{frame}{Performances d'AlphaGo}
  Plusieurs versions :
  \begin{description}
  \item[AlphaGo] 5 à 0 en Octobre 2015
  \item[AlphaGo Lee] Bat Lee Sedol 4 à 1 en mars 2016 (prédiction de Rémi Coulom en 2016 : pas avant 15 ans)
  \item[AlphaGo Master] Bat les pros sur internet 60 à 0 en décembre 2016
  \item[AlphaGo Zero] Pas de phase supervisée. $>$ AlphaGo Master
  \item[Alpha Zero] $\geq$ AlphaGo Zero mais applicable aussi au shogi, échecs, …
  \item[Mu Zero] $\geq$ Alpha Zero mais pas d'hardcodage des règles
  \end{description}
\end{frame}

\begin{frame}{AlphaGo}
  \begin{center}
    AlphaGo vs Lee Sedol (ou Fan Hui)
  \end{center}
  \huge
  1 202 processeurs et 176 TPU
\end{frame}

\begin{frame}{AlphaGo Zero}
  \begin{description}[<+->]
    \item[Plus simple] 4 TPU
    \item[Plus fort] Bat alphago 100:0
    \item[Plus général] Pas de supervisé : renforcement par self-play seulement
  \end{description}
  \onslide<+->{\V{["img/alphagozero-lerningcurve", "tw", 1] | image}}
\end{frame}

\begin{frame}{Alpha Zero}
  \begin{itemize}[<+->]
    \item Encore plus général~: généralise les inputs et les outputs
    \item Bat tous les bot d'échecs, go et shogi
  \end{itemize}  
\end{frame}

\begin{frame}{Mu Zero}
  \begin{itemize}[<+->]
    \item Encore plus général~: infère les règles depuis les inputs et outputs
    \item Était état de l'art sur les jeux Atari, bat Alpha Zero
  \end{itemize}
\end{frame}

\begin{frame}{Relativisons}
  \begin{itemize}[<+->]
    \item Un cerveau humain consomme $\approx$ 20 Watts
    \item AlphaGo $\approx$ 440 000 Watts (440 grille-pains)
    \item Un tableur gagnerait n'importe quelle compétition de calcul mental
  \end{itemize}
\end{frame}

