\begin{frame}{Qualité de l'apprentissage}
  Entrainement supervisé d'un modèle — overfit

  \V{"plt/overfit-polynomial" | image("tw", 1)}

  \red{Problème : trop minimiser la perte n'est pas bon !}
\end{frame}

\begin{frame}{Qualité de l'apprentissage}

  \V{"plt/learning-curve" | image("th", 0.6)}

  → Minimiser la perte sur un ensemble de validation
\end{frame}

\begin{frame}{Cross-validation}
  La technique de réference pour évaluer des modèles prédictifs :
  \V{"img/k-fold-cross-validation" | image("tw", 0.9)}
  Ici, 4-fold cross-validation.
\end{frame}

\begin{frame}{Séparation des données}
  \begin{itemize}
  \item ensemble d'entrainement
  \item ensemble de validation pour mesurer la généralisation
  \item ensemble de test (pour éviter le biais statistique)
  \end{itemize}
  → Split 60/20/20 habituel.
\end{frame}

\begin{frame}{Régularisation}
  \begin{minipage}[l]{0.49\linewidth}
    \begin{center}
      Régularisation \\
      $\approx$\\
      empêcher le surapprentissage
    \end{center}
  \end{minipage}\hfill
  \begin{minipage}[l]{0.49\linewidth}
    \V{"plt/overfitting-train" | image("th", 0.3)}
  \end{minipage}\hfill
  Techniques variées en fonction du modèle :
  \begin{itemize}
  \item Pénalisation de la norme des paramètres
  \item Bruitage
  \item Dropout
  \item …
  \end{itemize}
\end{frame}

\begin{frame}{Optimisation des méta-paramètres}
  Méta-paramètres : paramètres \alert{non appris} par le modèle.
  \begin{exampleblock}{Exemples}
    \begin{description}
    \item[Forme] Nombre de couches ? De quelles tailles ? …
    \item [Optimisation] SGD, Adam, Lion ?
    \item [Régularisation] Pénalisation de la Norme des paramètres dans la loss, bruitage, dropout, …
    \item [Prétraitements] Feature engineering, Méthode de normalisation, …
    \end{description}
  \end{exampleblock}
  Optimisation par recherche aléatoire ou processus gaussien.
\end{frame}
