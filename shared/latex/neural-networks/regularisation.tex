\begin{frame}
  \frametitle{Sur-apprentissage (et sous-apprentissage)}
  \imgtw[0.6]{over-underfitting}
\end{frame}

\begin{frame}{Par la pénalisation de l'utilisation des paramètres}
  Coût supplémentaire pour l'utilisation des paramètres dans la fonction de perte~:

  \[
  \text{perte} = \text{perte} + \lambda\sum||w||^2
  \]

  où $\lambda$ est un hyperparamètre
\end{frame}

\begin{frame}{Par early stopping}
  \imgtw[0.6]{early-stopping}
\end{frame}

\begin{frame}{Par augmentation/bruitage des données}
  \imgtw[0.8]{data-augmentation}
\end{frame}

\begin{frame}{Par dropout}
  \imgtw[0.8]{dropout-2}
\end{frame}

\begin{frame}{En entraînant sur plusieurs tâches}
  \imgtw[0.8]{multi-task-v2}
\end{frame}

\begin{frame}{En opposant des réseaux de neurones}
  \imgtw[0.8]{gan-schema}
\end{frame}
