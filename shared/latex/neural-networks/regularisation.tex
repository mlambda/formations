\begin{frame}
  \frametitle{Sur-apprentissage (et sous-apprentissage)}
  \imgtw{over-underfitting}{0.6}
\end{frame}

\begin{frame}{Par la pénalisation de l'utilisation des paramètres}
  Coût supplémentaire pour l'utilisation des paramètres dans la fonction de perte~:

  \[
  \text{perte} = \text{perte} + \lambda\sum||w||^2
  \]

  où $\lambda$ est un hyperparamètre
\end{frame}

\begin{frame}{Par early stopping}
  \imgtw{early-stopping}{0.6}
\end{frame}

\begin{frame}{Par augmentation/bruitage des données}
  \imgtw{data-augmentation}{0.8}
\end{frame}

\begin{frame}{Par dropout}
  \imgtw{dropout-2}{0.8}
\end{frame}

\begin{frame}{En entraînant sur plusieurs tâches}
  \imgtw{multi-task-v2}{0.8}
\end{frame}

\begin{frame}{En opposant des réseaux de neurones}
  \imgtw{gan-schema}{0.8}
\end{frame}
