\begin{frame}{Introduction}
  Caractéristiques des réseaux récurrents~:

  \begin{itemize}[<+->]
    \item Gestion des séquences
    \item Traitement séquentiel des entrées
    \item Réplication d'un réseau autant de fois qu'il y a d'entrées
  \end{itemize}
\end{frame}

\fmg{\begin{frame}{Différence avec un réseau feedforward}
  \imgtw[0.7]{nn-rnn-comparaison}
\end{frame}}

\begin{frame}{Modèle}
  \imgtw[0.7]{rnn-unfold}
  $h_{t}=\sigma_{h}(Ux_{t}+Vh_{t-1}+b_{h})$ \\
  $o_{t}=\sigma_{o}(Wh_{t}+b_{o})$ \\
  \begin{itemize}
  \item $x_{t}$ : vecteur d'entrée
  \item $h_t$ : vecteur de la couche cachée
  \item $o_{t}$ : vecteur de sortie
  \item $U$, $V$, $W$, $b_h$ et $b_o$ : matrices et vecteurs (paramètres)
  \item $\sigma_{h}$ et $\sigma_o$ : fonctions d'activation (tanh)
  \end{itemize}
\end{frame}

\begin{frame}{Prédiction d'une sortie}
  Exemple~: détection de polarité.

  \begin{figure}
    \centering
    \imgth[0.7]{rnns/lastoutput}
  \end{figure}
\end{frame}

\begin{frame}{Prédiction d'autants de sorties qu'il y a d'entrées}
  \imgtw[0.5]{rnns/alloutputs}
\end{frame}

\begin{frame}{Prédiction d'un nombre arbitraire de sorties}
  \imgtw{rnns/seq2seq}
\end{frame}

\begin{frame}{Problème du gradient qui disparaît (vanishing gradient)}
  \imgtw{vanishing-gradient-rnn}
\end{frame}
