\begin{frame}{Différence avec un réseau feedforward}
  \imgtw[0.7]{nn-rnn-comparaison}
\end{frame}

\begin{frame}{Modèle}
  \imgtw[0.7]{rnn-unfold}
  $h_{t}=\sigma_{h}(Ux_{t}+Vh_{t-1}+b_{h})$ \\
  $o_{t}=\sigma_{o}(Wh_{t}+b_{o})$ \\
  \begin{itemize}
  \item $x_{t}$ : vecteur d'entrée
  \item $h_t$ : vecteur de la couche cachée
  \item $o_{t}$ : vecteur de sortie
  \item $U$, $V$, $W$, $b_h$ et $b_o$ : matrices et vecteurs (paramètres)
  \item $\sigma_{h}$ et $\sigma_o$ : fonctions d'activation (tanh)
  \end{itemize}
\end{frame}

\begin{frame}{Prédiction de la suite d'une séquence}
  Exemmple~: modèle de langage

  \imgtw[0.7]{rnn-language-model}

  Peut être utilisé pour générer de nouvelles séquences
\end{frame}

\begin{frame}{Prédiction d'une classe}
  \imgth[0.7]{rnn-predict-class-2}
\end{frame}

\begin{frame}{Génération d'une séquence (seq2seq)}
  \imgtw{rnn-seq2seq}
\end{frame}

\begin{frame}{Problème du gradient qui disparaît (vanishing gradient)}
  \imgtw{vanishing-gradient-rnn}
\end{frame}
