\begin{frame}{Lien avec la régression linéaire}
  \V{["regression-10", "tw", 0.9] | image}
\end{frame}

\begin{frame}{Lien avec la régression linéaire}
  \V{["separable-problem-linear-solution-nzmog", "tw", 0.9] | image}
\end{frame}

\begin{frame}{Lien avec la régression linéaire}
  \V{["xor-nzmog", "tw", 0.9] | image}
\end{frame}

\begin{frame}{Lien (ténu) avec la biologie}
  \V{["biological-neuron", "tw", 1] | image}
\end{frame}

\begin{frame}{Modélisation d'un neurone}
  \V{["neuron-model", "tw", 0.9] | image}
\end{frame}

\begin{frame}{Modélisation d'un réseau de neurones}
  Agencement de beaucoup de neurones~:
  \begin{description}[<+->]
    \item[En parallèle] Calculent des résultats indépendamment dans la même couche
    \item[En série] Prennent en entrée les résultats des neurones de la couche précédente
  \end{description}
\end{frame}

\begin{frame}{Deux types de neurones}
  On distingue deux types de neurones~:
  \begin{description}[<+->]
    \item[Neurones cachés] Neurones des couches intermédiaires. Améliorent l'expressivité du modèle
    \item[Neurones de sortie] Neurones de la couche finale. Contraints par le type de sortie attendu
  \end{description}
\end{frame}

\begin{frame}{Réseau sans couche cachée}
  \centering
  \V{["neural-networks/shallow-ff", "tw", 1] | image}
\end{frame}

\begin{frame}{Réseau avec une couche cachée}
  \centering
  \V{["neural-networks/one-hidden-layer-ff", "tw", 1] | image}
\end{frame}

\begin{frame}{Réseau profond}
  \centering
  \V{["neural-networks/deep-ff", "tw", 1] | image}
\end{frame}

\begin{frame}{Un potentiel infini}
  \textbf{Kurt Hornik, 1991~:} Théorème d'approximation universelle
  \V{["activations/approximation", "tw", 1] | image}
\end{frame}

\begin{frame}{Modélisation matricielle --- Échantillon}
  Représentable sous forme de vecteur à $d$ colonnes correspondant à $d$ caractéristiques~:

  \[
    \vect{x} = \begin{bmatrix}
      x_1 & x_2 & \dots & x_d
    \end{bmatrix}
  \]

  Voire même de matrice dans le cas d'un batch (groupe d'échantillons)~:

  \[
    \vect{X} = \begin{bmatrix}
      X_{1,1} & X_{1,2} & \dots & X_{1,d} \\
      X_{2,1} & X_{2,2} & \dots & X_{2,d}
    \end{bmatrix}
  \]
\end{frame}

\begin{frame}{Modélisation matricielle --- Poids}
  Représentables sous forme de matrice de poids et de vecteur de biais~:

  \begin{align*}
    \vect{W} & = \begin{bmatrix}
      W_{1,1} & W_{1,2} & \dots  & W_{1,n} \\
      W_{2,1} & W_{2,2} & \dots  & W_{2,n} \\
      \vdots & \vdots & \ddots & \vdots \\
      W_{d,1} & W_{d,2} & \dots  & W_{d,n}
    \end{bmatrix} \\
    \vect{b} & = \begin{bmatrix}
      b_1 & b_2 & \dots & b_n
    \end{bmatrix}
  \end{align*}
\end{frame}

\begin{frame}{Modélisation matricielle complète}
  \footnotesize
  \begin{align*}
    O & = \sigma (\vect{X}.\vect{W} + \vect{b}) \\
    & = \sigma \left(
    \begin{bmatrix}
      X_{1,1} & X_{1,2} & \dots & X_{1,d} \\
      X_{2,1} & X_{2,2} & \dots & X_{2,d}
    \end{bmatrix}
    \begin{bmatrix}
      W_{1,1} & W_{1,2} & \dots  & W_{1,n} \\
      W_{2,1} & W_{2,2} & \dots  & W_{2,n} \\
      \vdots & \vdots & \ddots & \vdots \\
      W_{d,1} & W_{d,2} & \dots  & W_{d,n}
    \end{bmatrix}
    +
    \begin{bmatrix}
      b_1 & b_2 & \dots & b_n
    \end{bmatrix}
    \right) \\
    & = \begin{bmatrix}
      O_{1,1} & O_{1,2} & \dots & O_{1,n} \\
      O_{2,1} & O_{2,2} & \dots & O_{2,n}
    \end{bmatrix}
  \end{align*}
  où :
  \begin{description}[<+->]
    \item[X] Données en entrée de dimension $d$
    \item[W \& b] Paramètres à trouver des $n$ neurones de notre modèle
    \item[$\bm{\sigma}$] Fonction d'activation
    \item[O] Sortie du réseau
  \end{description}
\end{frame}

\begin{frame}{Fonctions d'activation --- Critères de choix}
  \begin{itemize}[<+->]
    \item Propriétés mathématiques (conservation du gradient)
    \item Propriétés d'apprentissage (éviter la création de poids morts)
    \item Rapidité de calcul
    \item Intervalle de sortie pour la dernière couche
  \end{itemize}
\end{frame}

\begin{frame}{Fonctions d'activation --- Les plus classiques}
  \begin{itemize}
  \item Sigmoïde
  \item Tanh
  \item Softmax
  \item ReLU
  \item …
  \end{itemize}
\end{frame}

\begin{frame}{Fonctions d'activation --- Sigmoïde}
  \begin{center}
    \V{["activations/sigmoid", "tw", 1] | image}
  \end{center}

  \begin{description}[<+->]
    \item[Définition] \[\phi(x) = \frac{1}{1 + e^{-x}}\]
    \item[Dérivée] \[\phi'(x)=\phi(x)(1-\phi(x))\]
  \end{description}
\end{frame}

\begin{frame}{Fonctions d'activation --- Tangente hyperbolique}
  \begin{center}
    \V{["activations/tanh", "tw", 1] | image}
  \end{center}

  \begin{description}[<+->]
    \item[Définition] \[\tanh(x)=\frac{1-e^{-2x}}{1+e^{-2x}}\]
    \item[Dérivée] \[\tanh'(x)=1-\tanh^2(x)\]
  \end{description}
\end{frame}

\begin{frame}{Fonctions d'activation --- ReLU}
  \begin{center}
    \V{["activations/relu", "tw", 0.6] | image}
  \end{center}

  \begin{description}[<+->]
    \item[Définition] \[\ReLU(x)=\begin{cases} 0 & x \leq 0 \\ x & x > 0\end{cases}\]
    \item[Dérivée] \[\ReLU'(x)=\begin{cases} 0 & x \leq 0 \\ 1 & x > 0\end{cases}\]
  \end{description}
\end{frame}

\begin{frame}{Fonctions d'activation --- Approximation d'une fonction}
  \centering
    \V{["activations/approximation", "tw", 1] | image}
  {\small
    \begin{align*}
      n_1 & = \ReLU(-5x - 7.7) & n_4 & = \ReLU(1.2x - 0.2) \\
      n_2 & = \ReLU(-1.2x - 1.3) & n_5 & = \ReLU(2x - 1.1) \\
      n_3 & = \ReLU(1.2x - 1) & n_6 & = \ReLU(5x - 5) \\
    \end{align*}
  }
\end{frame}

\begin{frame}{Fonctions d'activation --- Softmax}
  \begin{description}[<+->]
    \item[Définition] \[\softmax(x_i) = \frac{\exp(x_i)}{\sum_{k = 1}^n{\exp(x_k)}}\]
    \item[Propriété] \[\sum_{i = 1}^n{\softmax(x_i)} = 1\]
    \item[Gradient] \[
      \frac{\partial \softmax(x_i)}{\partial x_j} =
        \begin{cases}
          \softmax(x_i)(1 - \softmax(x_j)) & i = j \\
          -\softmax(x_i)\softmax(x_j) & i \neq j
        \end{cases}
      \]
  \end{description}
  
\end{frame}
