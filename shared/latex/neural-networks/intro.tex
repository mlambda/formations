\begin{frame}
  \frametitle{Lien avec la régression linéaire}
  \imgtw[0.9]{regression-10}
\end{frame}

\begin{frame}
  \frametitle{Lien avec la régression linéaire}
  \imgtw[0.9]{separable-problem-linear-solution}
\end{frame}

\begin{frame}
  \frametitle{Lien avec la régression linéaire}
  \imgtw[0.9]{xor}
\end{frame}

\begin{frame}
  \frametitle{Lien (ténu) avec la biologie}
  \imgtw[0.9]{biological-neuron}
\end{frame}

\begin{frame}
  \frametitle{Modélisation}
  \imgtw[0.9]{neuron-model}
\end{frame}

\begin{frame}
  \frametitle{Modélisation matricielle}
  \[
  \sigma \left(
  \begin{bmatrix}
    x_{1} & x_{2} & \dots & x_{d}
  \end{bmatrix}
  *
  \begin{bmatrix}
    w_{11} & w_{12} & \dots  & w_{1n} \\
    w_{21} & w_{22} & \dots  & w_{2n} \\
    \vdots & \vdots & \ddots & \vdots \\
    w_{d1} & w_{d2} & \dots  & w_{dn}
  \end{bmatrix}
  +
  \begin{bmatrix}
    b_{1} & b_{2} & \dots & b_{n}
  \end{bmatrix}
  \right )
  \]
  \[
  =
  \begin{bmatrix}
    o_{1} & o_{2} & \dots & o_{n}
  \end{bmatrix}
  \]
  \newline
  où :
  \newline
  X est une donnée en entrée de dimension \textbf{d},
  \newline
  $w$ et $b$ sont les paramètres à trouver des \textbf{n} neurones de notre modèle.
  \newline
  $\sigma$ la fonction d'activation et
  \newline
  O la sortie du réseau
\end{frame}

\begin{frame}
  \frametitle{Fonctions d'activation $\sigma$}
  \begin{itemize}
  \item Sigmoïde
  \item Tanh
  \item Softmax
  \item ReLU
  \item ...
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Un potentiel infini}
  \textbf{Kurt Hornik, 1991~:} Théorème d'approximation universelle
  \imgtw[0.8]{universal-approximation}
\end{frame}

\begin{frame}
  \frametitle{Fonction d'activation sigmoïde}
  \imgth[0.6]{sigmoid}
  \[
  \frac{\partial{\phi(x)}}{\partial{x}}=\phi(x)*(1-\phi(x))
  \]
\end{frame}

\begin{frame}
  \frametitle{Fonction d'activation tanh}
  \[
  \tanh(x)=\frac{1-\exp{-2*x}}{1+\exp{-2*x}}
  \]
  \imgth[0.5]{tanh}
  \[
  \frac{\partial{\tanh(x)}}{\partial{x}}=1-\tanh^2(x)
  \]
\end{frame}

\begin{frame}
  \frametitle{Fonction d'activation softmax}
  \[
  \softmax(x_j) = \frac{e^{x_j}}{\sum_{i = 1}^n{e^{x_i}}}
  \]
  donc :
  \[
  \sum_{j = 1}^n{\softmax(x_j)} = 1
  \]
  dérivée (ou jacobien car le softmax est une fonction de $\mathbb{R}^n\rightarrow\mathbb{R}^n$):
  \[
  D_jS_i = S_i(\delta_{ij}-S_j)
  \]
  où
  $D_jS_i$ est la dérivée partielle de la i-ième sortie par rapport à la j-ième entrée
  \newline
  $\delta_{ij}$ est le delta de Kronecker
\end{frame}

\begin{frame}
  \frametitle{Fonction d'activation ReLU}
  \begin{minipage}[c]{0.39\linewidth}
    \imgtw[0.9]{relu}
  \end{minipage}\hfill
  \begin{minipage}[c]{0.59\linewidth}
    \imgtw[0.9]{relu-approx}
  \end{minipage}\hfill
\end{frame}

\begin{frame}
  \frametitle{Réseau sans couche cachée}
  \imgtw[0.8]{neural-network-small}
\end{frame}

\begin{frame}
  \frametitle{Réseau avec une couche cachée}
  \imgtw[0.8]{neural-network}
\end{frame}

\begin{frame}
  \frametitle{Réseau profond}
  \imgtw[0.8]{deep-net}
\end{frame}
