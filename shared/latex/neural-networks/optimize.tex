\begin{frame}{Qu'est-ce qu'un hyper-paramètre~?}
  \begin{columns}
    \begin{column}[c]{0.5\textwidth}
      \imgtw{neural-networks/deep-ff-notext}{1}
    \end{column}
    \begin{column}[c]{0.5\textwidth}
      \begin{itemize}
        \item Learning rate
        \item Taille de batch
        \item Nombre de couches
        \item Taille des couches
      \end{itemize}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Learning rate}
  \begin{minipage}{0.49\textwidth}
    \centering
    trop grand
  \end{minipage}\hfill
  \begin{minipage}{0.49\linewidth}
    \centering
    trop petit
  \end{minipage}\hfill

  \imgtw{learning-rate-big-small}{1}
\end{frame}

\begin{frame}{Learning rate}
  Utilisation d'une échelle logarithmique (dans un premier temps) :
  \mintedpycode{gridsearch-template}
\end{frame}

\begin{frame}{Taille du batch}
  Usuellement, des puissances de 2, pour optimiser les ressources GPU
  \imgtw{batch-size-effect}{0.8}
\end{frame}

\begin{frame}{Taille des couches}
  Usuellement, des puissances de 2, pour optimiser les ressources GPU. \\
  Même intuition que pour le nombre de couches.
\end{frame}

\begin{frame}{Nombre de couches}
  \mintedpycode{addlayer-template}
\end{frame}
