\begin{frame}{Qu'est-ce qu'un hyper-paramètre~?}
  \begin{minipage}[l]{0.49\linewidth}
    \imgtw[0.9]{deep-net}
  \end{minipage}\hfill
  \begin{minipage}[l]{0.49\linewidth}
    \begin{itemize}
      \item Learning rate
      \item Taille de batch
      \item Nombre de couches
      \item Taille des couches
    \end{itemize}
  \end{minipage}\hfill
\end{frame}

\begin{frame}{Learning rate}
  \begin{minipage}[l]{0.49\linewidth}
    \begin{center}trop grand\end{center}
  \end{minipage}\hfill
  \begin{minipage}[l]{0.49\linewidth}
    \begin{center}trop petit\end{center}
  \end{minipage}\hfill

  \imgtw{learning-rate-big-small}
\end{frame}

\begin{frame}{Learning rate}
  Utilisation d'une échelle logarithmique (dans un premier temps) :
  \mintedpycode{gridsearch-template}
\end{frame}

\begin{frame}{Taille du batch}
  Usuellement, des puissances de 2, pour optimiser les ressources GPU
  \imgtw[0.8]{batch-size-effect}
\end{frame}

\begin{frame}{Nombre de couches}
  \mintedpycode{addlayer-template}
\end{frame}

\begin{frame}{Taille des couches}
  Usuellement, des puissances de 2, pour optimiser les ressources GPU. \\
  Même intuition que pour le nombre de couches.
\end{frame}
