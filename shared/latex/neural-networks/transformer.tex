\fmg{\begin{frame}{Réseaux transformeurs}
  \imgtw[0.9]{transformer-fun}
\end{frame}}

\begin{frame}{Vue générale}
  \imgtw[0.9]{transformer-network-1}
\end{frame}

\begin{frame}{Architecture encodeur-décodeur}
  \imgtw[0.9]{transformer-network-2}
\end{frame}

\begin{frame}{Architecture encodeur-décodeur --- Rappel}
  Rappelez-vous, les encodeurs-décodeurs :
  \imgtw[0.9]{rnn-seq2seq}
\end{frame}

\begin{frame}{Architecture profonde}
  Couches «~simples~» empilées~:
  \imgtw[0.9]{transformer-network-3}
\end{frame}

\begin{frame}{Architecture globale de l'encodeur et du décodeur}
  \imgtw[0.9]{transformer-network-5}
\end{frame}

\begin{frame}{Architecture globale de l'encodeur}
  \imgtw[0.9]{transformer-network-4}
\end{frame}

\begin{frame}{Forme des données en jeu dans l'encodeur}
  \imgtw[0.9]{transformer-network-7}
\end{frame}

\begin{frame}{Forme des données en jeu dans l'encodeur}
  \imgtw[0.9]{transformer-network-8}
\end{frame}

\begin{frame}{Self-Attention --- Éléments utilisés}
  \imgtw[0.9]{transformer-network-9}
\end{frame}

\begin{frame}{Self-Attention --- Combinaison des éléments}
  \imgth[0.9]{transformer-network-13}
\end{frame}

\begin{frame}{Self-Attention --- Obtention du résultat}
  \imgtw[0.9]{transformer-network-14}
  Avec le softmax défini ainsi : $\sigma(z_j)=\frac {\mathrm{e}^{z_j}}{\sum _{k=1}^{K}\mathrm{e}^{z_{k}}}$
\end{frame}

\begin{frame}{Self-Attention --- Run sur 2 mots}
  \imgtw[0.9]{transformer-network-10}
\end{frame}

\begin{frame}{Self-Attention --- Run sur 2 mots}
  \imgtw[0.9]{transformer-network-11}
\end{frame}

\begin{frame}{Self-Attention --- Run sur 2 mots}
  \imgth[0.9]{transformer-network-12}
\end{frame}

\begin{frame}{Self-Attention --- Visualisation}
  \imgth[0.9]{transformer-network-self-attention}
\end{frame}

\begin{frame}{Attention multi-têtes --- Têtes d'attention}
  \imgtw[0.9]{transformer-network-15}
\end{frame}

\begin{frame}{Attention multi-têtes --- Combinaison}
  \imgtw[0.9]{transformer-network-16}
\end{frame}

\begin{frame}{Encodeur --- Résumé}
  Un modèle sans récurrence, uniquement des sommes pondérées.

  \imgtw[0.9]{transformer-network-17}
\end{frame}

\begin{frame}{Encodeur --- Gestion de la position}
  Pour l'instant, modèle invariant à l'ordre.

  $\Rightarrow$ Nécessité d'encoder la position (encodage positionnel).
\end{frame}

\begin{frame}{Encodeur --- Encodage positionnel --- Fonction hardcodée}
  \imgtw[0.9]{transformer-network-18}
  $\text{PE}_{\text{pos}, 2i} = \sin{(\text{pos} / 10000^{2i / d_{\text{model}}})}$ \\
  $\text{PE}_{\text{pos}, 2i + 1}= \cos{(\text{pos} / 10000^{2i / d_{\text{model}}})}$
\end{frame}

\begin{frame}{Encodeur --- Encodage positionnel --- Alternative}
  Il est aussi possible d'apprendre les embeddings de position, sans détérioration des performances.
\end{frame}

\begin{frame}{Encodeur --- Encodage positionnel --- Exemple}
  \imgtw[0.9]{transformer-network-19}
\end{frame}

\begin{frame}{Encodeur --- Encodage positionnel --- Visualisation}
  \imgtw[0.9]{positional-encoding}
\end{frame}

\begin{frame}{Décodeur --- Exemple de décodage}
  \imgtw[0.9]{transformer-decoding-init-0}
\end{frame}

\begin{frame}{Décodeur --- Exemple de décodage}
  \imgtw[0.9]{transformer-decoding-init-157}
\end{frame}

\begin{frame}{Décodeur --- Exemple de décodage}
  \imgtw[0.9]{transformer-decoding-0}
\end{frame}

\begin{frame}{Décodeur --- Exemple de décodage}
  \imgtw[0.9]{transformer-decoding-14}
\end{frame}

\begin{frame}{Décodeur --- Exemple de décodage}
  \imgtw[0.9]{transformer-decoding-38}
\end{frame}

\begin{frame}{Décodeur --- Exemple de décodage}
  \imgtw[0.9]{transformer-decoding-91}
\end{frame}

\begin{frame}{Décodeur --- Exemple de décodage}
  \imgtw[0.9]{transformer-decoding-112}
\end{frame}

\begin{frame}{Décodeur --- Exemple de décodage}
  \imgtw[0.9]{transformer-decoding-137}
\end{frame}

\begin{frame}{Décodeur --- Exemple de décodage}
  \imgtw[0.9]{transformer-decoding-188}
\end{frame}

\begin{frame}{Décodeur --- Exemple de décodage}
  \imgtw[0.9]{transformer-decoding-207}
\end{frame}

\begin{frame}{Décodeur --- Exemple de décodage}
  \imgtw[0.9]{transformer-decoding-234}
\end{frame}

\begin{frame}{Décodeur --- Exemple de décodage}
  \imgtw[0.9]{transformer-decoding-306}
\end{frame}

\begin{frame}{Décodeur --- Exemple de décodage}
  \imgtw[0.9]{transformer-decoding-333}
\end{frame}

\begin{frame}{Transformer Network}
  Les illustrations sont tirées du billet de blog de Jay Alammar~:
  \url{http://jalammar.github.io/illustrated-transformer/}
\end{frame}
