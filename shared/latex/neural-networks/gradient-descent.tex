
\begin{frame}{Principe}
  Calcul du gradient de l'erreur par rapport aux paramètres:
  \[
  \frac{\partial{Err}}{\partial{w_i}}
  \]
  Mise à jour :
  \[
  w_i' = w_i - \gamma * grad 
  \]
  où : $0 < \gamma < 1$ (learning rate)
  \V{["gradient", "tw", 0.6] | image}
\end{frame}

\begin{frame}{Algorithme}
  \begin{enumerate}[<+->]
    \item Initialisation aléatoire du modèle
    \item Tant qu'aucun critère d'arrêt n'est satisfait~:
      \begin{itemize}
        \item Selection aléatoire d'un \textbf{batch} de données
        \item \textbf{Forward} : Passe avant du \textbf{batch} dans le modèle
        \item Calcul de l'erreur par rapport aux sorties attendues
        \item \textbf{Backward} : Rétropropagation du gradient de l'erreur en fonction des paramètres dans le modèle (mise à jour du modèle)
        \item Calcul des critères d'arrêt
      \end{itemize}
  \end{enumerate}
\end{frame}


\begin{frame}{Exemple de dérivation --- Régression linéaire}
  \begin{minipage}[l]{0.49\linewidth}
    \begin{align*}
      E_\Omega & = \frac{1}{2n} \sum_{i=1}^n( \hat{y_i} - y_i )^2 \\
      E_\Omega & = \frac{1}{2n}\sum_{i=1}^n( \hat{y_i} - (ax_i + b) )^2 \\
      & ... \\
      \frac{\partial E_\Omega}{\partial a} & = \frac{1}{n}\sum^n_{i=1}(ax_i + b - \hat{y_i})x_i \\
      \frac{\partial E_\Omega}{\partial b} & = \frac{1}{n}\sum^n_{i=1}(ax_i + b - \hat{y_i}) \\
    \end{align*}
  \end{minipage}\hfill
  \begin{minipage}[c]{0.49\linewidth}
    \begin{center}
      $\boxed{y = ax+b}$
    \end{center}

    \begin{center}
      $\boxed{U^{2\;\prime}=2U' \times U}$
    \end{center}
    \vfill
    \begin{center}
      \textbf{Mise à jour}

      $a \leftarrow a - \gamma\frac{\partial{E_{\Omega}}}{\partial{a}}$ \\
      $b \leftarrow b - \gamma\frac{\partial{E_{\Omega}}}{\partial{b}}$ \\
      $\;$ \\
      où $0 < \gamma < 1$ (pas d'apprentissage)
    \end{center}
  \end{minipage}\hfill
\end{frame}

\begin{frame}{Exemple}
  Initialisation au hasard ($\gamma = 0.01$)
  \begin{itemize}
    \item a = 0.58 ($\hat{a} = 3.0$)
    \item b = 0.25 ($\hat{b} = 0.5$)
  \end{itemize}
  \V{["regression-1", "th", 0.6] | image}
\end{frame}

\begin{frame}{Exemple}
  \begin{itemize}
    \item a = 0.58 ($\hat{a} = 3.0$)
    \item b = 0.25 ($\hat{b} = 0.5$)
  \end{itemize}
  \V{["regression-error", "th", 0.6] | image}
\end{frame}

\begin{frame}{Exemple}
  \begin{itemize}
    \item a = 1.50 ($\hat{a} = 3.0$)
    \item b = 0.35 ($\hat{b} = 0.5$)
  \end{itemize}
  \V{["regression-2", "th", 0.6] | image}
\end{frame}

\begin{frame}{Exemple}
  \begin{itemize}
    \item a = 2.10 ($\hat{a} = 3.0$)
    \item b = 0.40 ($\hat{b} = 0.5$)
  \end{itemize}
  \V{["regression-3", "th", 0.6] | image}
\end{frame}

\begin{frame}{Exemple}
  \begin{itemize}
    \item a = 2.48 ($\hat{a} = 3.0$)
    \item b = 0.43 ($\hat{b} = 0.5$)
  \end{itemize}
  \V{["regression-4", "th", 0.6] | image}
\end{frame}

\begin{frame}{Exemple}
  \begin{itemize}
    \item a = 2.73 ($\hat{a} = 3.0$)
    \item b = 0.46 ($\hat{b} = 0.5$)
  \end{itemize}
  \V{["regression-5", "th", 0.6] | image}
\end{frame}

\begin{frame}{Exemple}
  \begin{itemize}
    \item a = 2.89 ($\hat{a} = 3.0$)
    \item b = 0.47 ($\hat{b} = 0.5$)
  \end{itemize}
  \V{["regression-6", "th", 0.6] | image}
\end{frame}

\begin{frame}{Exemple}
  \begin{itemize}
    \item a = 2.99 ($\hat{a} = 3.0$)
    \item b = 0.48 ($\hat{b} = 0.5$)
  \end{itemize}
  \V{["regression-7", "th", 0.6] | image}
\end{frame}

\begin{frame}{Exemple}
  \begin{itemize}
    \item a = 3.06 ($\hat{a} = 3.0$)
    \item b = 0.49 ($\hat{b} = 0.5$)
  \end{itemize}
  \V{["regression-8", "th", 0.6] | image}
\end{frame}

\begin{frame}{Exemple}
  \begin{itemize}
    \item a = 3.10 ($\hat{a} = 3.0$)
    \item b = 0.49 ($\hat{b} = 0.5$)
  \end{itemize}
  \V{["regression-9", "th", 0.6] | image}
\end{frame}

\begin{frame}{Exemple}
  \begin{itemize}
    \item a = 3.13 ($\hat{a} = 3.0$)
    \item b = 0.50 ($\hat{b} = 0.5$)
  \end{itemize}
  \V{["regression-10", "th", 0.6] | image}
\end{frame}
