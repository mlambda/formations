
\begin{frame}
  \frametitle{Optimisation}
  Calcul du gradient de l'erreur par rapport aux paramètres:
  \[
  \frac{\partial{Err}}{\partial{w_i}}
  \]
  Mise à jour :
  \[
  w_i' = w_i - \gamma * grad 
  \]
  où : $0 < \gamma < 1$ (learning rate)
  \imgtw[0.6]{gradient}
\end{frame}

\begin{frame}
  \frametitle{Optimisation}
  1 - initialisation aléatoire du modèle
  \newline
  2 - Tant que(critère arret == 0)
  \begin{itemize}
  \item Selection aléatoire d'un \textbf{batch} de données
  \item \textbf{Forward} : Passe avant du \textbf{batch} dans le modèle
  \item Calcul de l'erreur par rapport aux sorties attendues
  \item \textbf{Backward} : Rétropropagation du gradient de l'erreur en fonction des paramèrtres dans le modèle (mise à jour du modèle)
  \item Calcul critère arret
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Gradient et mise à jour}
  \begin{minipage}[l]{0.49\linewidth}
    \begin{center}
      $\boxed{y = a.x+b}$ \\
    \end{center}
    $E_{\Omega} = \frac{1}{2n}\underset{i=[1..n]}{\sum}( \hat{y_i} - y_i )^2$ \\
    $E_{\Omega} = \frac{1}{2n}\underset{i=[1..n]}{\sum}( \hat{y_i} - (a.x_i+b) )^2$ \\
    ... \\
    $\frac{\partial{E_{\Omega}}}{\partial{a}} = \frac{1}{n}\sum_{i=[1..n]}(a.x_i+b - \hat{y_i}).x$ \\
    $\frac{\partial{E_{\Omega}}}{\partial{b}} = \frac{1}{n}\sum_{i=[1..n]}(a.x_i+b - \hat{y_i})$ \\
  \end{minipage}\hfill
  \begin{minipage}[c]{0.49\linewidth}
    $\;$ \\
    $\;$ \\
    $\;$ \\
    \begin{center}
      $\boxed{U^{2\;\prime}=2U\;'*U}$
    \end{center}
  \end{minipage}\hfill
  \begin{center}
    \textbf{M.A.J :} \\
    $\;$ \\
    $a \leftarrow a - \gamma.\frac{\partial{E_{\Omega}}}{\partial{a}}$ \\
    $b \leftarrow b - \gamma.\frac{\partial{E_{\Omega}}}{\partial{b}}$ \\
    $\;$ \\
    où $1 > \gamma > 0$ (learning rate)
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Gradient et mise à jour}
  initialisation au hasard ( $\gamma$ = 0.01 )
  \begin{itemize}
  \item a = 0.58 ($\hat{a} = 3.0$)
  \item b = 0.25 ($\hat{b} = 0.5$)
  \end{itemize}
  \imgth[0.8]{regression-1}
\end{frame}

\begin{frame}
  \frametitle{Gradient et mise à jour}
  \begin{itemize}
  \item a = 0.58 ($\hat{a} = 3.0$)
  \item b = 0.25 ($\hat{b} = 0.5$)
  \end{itemize}
  \imgth[0.8]{regression-error}
\end{frame}

\begin{frame}
  \frametitle{Gradient et mise à jour}
  \begin{itemize}
  \item a = 1.50 ($\hat{a} = 3.0$)
  \item b = 0.35 ($\hat{b} = 0.5$)
  \end{itemize}
  \imgth[0.8]{regression-2}
\end{frame}

\begin{frame}
  \frametitle{Gradient et mise à jour}
  \begin{itemize}
  \item a = 2.10 ($\hat{a} = 3.0$)
  \item b = 0.40 ($\hat{b} = 0.5$)
  \end{itemize}
  \imgth[0.8]{regression-3}
\end{frame}

\begin{frame}
  \frametitle{Gradient et mise à jour}
  \begin{itemize}
  \item a = 2.48 ($\hat{a} = 3.0$)
  \item b = 0.43 ($\hat{b} = 0.5$)
  \end{itemize}
  \imgth[0.8]{regression-4}
\end{frame}

\begin{frame}
  \frametitle{Gradient et mise à jour}
  \begin{itemize}
  \item a = 2.73 ($\hat{a} = 3.0$)
  \item b = 0.46 ($\hat{b} = 0.5$)
  \end{itemize}
  \imgth[0.8]{regression-5}
\end{frame}

\begin{frame}
  \frametitle{Gradient et mise à jour}
  \begin{itemize}
  \item a = 2.89 ($\hat{a} = 3.0$)
  \item b = 0.47 ($\hat{b} = 0.5$)
  \end{itemize}
  \imgth[0.8]{regression-6}
\end{frame}

\begin{frame}
  \frametitle{Gradient et mise à jour}
  \begin{itemize}
  \item a = 2.99 ($\hat{a} = 3.0$)
  \item b = 0.48 ($\hat{b} = 0.5$)
  \end{itemize}
  \imgth[0.8]{regression-7}
\end{frame}

\begin{frame}
  \frametitle{Gradient et mise à jour}
  \begin{itemize}
  \item a = 3.06 ($\hat{a} = 3.0$)
  \item b = 0.49 ($\hat{b} = 0.5$)
  \end{itemize}
  \imgth[0.8]{regression-8}
\end{frame}

\begin{frame}
  \frametitle{Gradient et mise à jour}
  \begin{itemize}
  \item a = 3.10 ($\hat{a} = 3.0$)
  \item b = 0.49 ($\hat{b} = 0.5$)
  \end{itemize}
  \imgth[0.8]{regression-9}
\end{frame}

\begin{frame}
  \frametitle{Gradient et mise à jour}
  \begin{itemize}
  \item a = 3.13 ($\hat{a} = 3.0$)
  \item b = 0.50 ($\hat{b} = 0.5$)
  \end{itemize}
  \imgth[0.8]{regression-10}
\end{frame}
