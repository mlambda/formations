\begin{frame}{Introduction}
  Naive Bayes classification is:
  \begin{itemize}[<+(1)->]
    \item simple to compute
    \item simple to formulate
    \item often a good first baseline
    \item limited by its strong hypotheses
  \end{itemize}
\end{frame}

\begin{frame}{Example --- Goal}
  \def\banane{\text{banana}}%
  \def\orange{\text{orange}}%
  \def\tomate{\text{tomato}}%
  \def\jaune{\text{yellow}}%
  \def\longg{\text{long}}%
  \def\sucre{\text{sweet}}%

  Dataset of fruits (bananas, oranges and tomatoes).

  Features: \text{color}, \text{size}, \text{sweetness}.

  For a \jaune{}, \longg{} and \sucre{} sample, naive bayes prediction is the maximum among
  \begin{itemize}
  \item $P(C = \banane \mid F = \jaune \land \longg \land \sucre)$
  \item $P(C = \orange \mid F = \jaune \land \longg \land \sucre)$
  \item $P(C = \tomate \mid F = \jaune \land \longg \land \sucre)$
  \end{itemize}
\end{frame}

\begin{frame}{Example --- Computation}
  \def\banane{\text{banana}}%
  \def\orange{\text{orange}}%
  \def\tomate{\text{tomato}}%
  \def\jaune{\text{yellow}}%
  \def\longg{\text{long}}%
  \def\sucre{\text{sweet}}%
  Naive classification: variables are considered independent, so:

  \begin{center}
    $P(C = \banane \mid F = \jaune \land \longg \land \sucre) =$\\
    $\;$\\
    $ \frac{P(\jaune \mid \banane) \times P(\longg \mid \banane) \times P(\sucre \mid \banane) \times P(\banane)}%
    {P(\jaune) \times P(\longg) \times P(\sucre)}$
  \end{center}
  To compute those probabilities, we \emph{count} in our fruit dataset:

  \[
    P(\sucre \mid \banane) = \frac{\card{\banane \land \sucre}}{\card{\banane}}
  \]
\end{frame}
