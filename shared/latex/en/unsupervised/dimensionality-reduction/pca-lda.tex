\begin{frame}{Linear projections}
  Two main methods:
  \begin{itemize}
    \item Principal Component Analysis (PCA, unsupervised)
    \item Linear Discriminant Analysis (LDA, supervised)
  \end{itemize}
\end{frame}

\begin{frame}{PCA --- Specificities}
  \begin{itemize}
    \item Creates new columns (linear combinations of the original)
    \item These columns are decorrelated
    \item The first ones are the most informative
    \item To reduce the dimensionality: keep only the first $k$.
  \end{itemize}
\end{frame}

\begin{frame}{PCA --- Example 1}
  \V{["img/pca-nuage", "th", 0.7] | image}
\end{frame}

\fmg{
  \begin{frame}{PCA --- Example 2}
    \V{["img/pca-fish", "th", 0.7] | image}
  \end{frame}
}

\begin{frame}{PCA --- Start point}
  \[
  X = \begin{bmatrix}
    X_{1,1} & \dots  & X_{1,D} \\
    \vdots & \ddots & \vdots \\
    X_{N,1} & \dots  & X_{N,D}
  \end{bmatrix}
  \]
\end{frame}

\begin{frame}{PCA --- Process}
  Each column is standardized:
  \[
  \overline{X} =
  \begin{bmatrix}
    X_{1,1}-\overline{X_1} & \dots  & X_{1,D}-\overline{X_D} \\
    \vdots & \ddots & \vdots \\
    X_{N,1}-\overline{X_1} & \dots  & X_{N,D}-\overline{X_D}
  \end{bmatrix}
  \]
then
  \[
  \widetilde{X} =
  \begin{bmatrix}
    \frac{X_{1,1}-\overline{X_1}}{\sigma(X_1)} & \dots  & \frac{X_{1,D}-\overline{X_D}}{\sigma(X_D)} \\
    \vdots & \ddots & \vdots \\
    \frac{X_{N,1}-\overline{X_1}}{\sigma(X_1)} & \dots  & \frac{X_{N,D}-\overline{X_D}}{\sigma(X_D)}
  \end{bmatrix}
  \]
\end{frame}

\begin{frame}{PCA --- Process}
  \begin{itemize}
    \item Calculation of the covariance matrix (respectively correlation):
      \[
      \frac{1}{N} \times \overline{X^T} \times \overline{X}
      \quad
      ( \frac{1}{N} \times \widetilde{X^T} \times \widetilde{X} )
      \]
    \item Calculation of the matrix eigenvectors: they are the linear combinations of the new columns
    \item Sorting the vectors by decreasing eigenvalue
    \item Dimensionality reduction: we keep only the first $k$ eigenvectors
  \end{itemize}

  \bluelink{https://youtu.be/VrNw9TTVExY}{PCA illustration}
\end{frame}

\begin{frame}{LDA --- Linear Discriminant Analysis}
  Process:

  \begin{itemize}
    \item Estimation of the covariance matrix of each class
    \item PCA on each of these matrices
    \item Projection by maximizing the inter-class variance
  \end{itemize}
  \bluelink{https://youtu.be/sYrBSNp3jZc}{LDA illustration}
\end{frame}

\begin{frame}{LDA --- Linear Discriminant Analysis}
  \textbf{Warning} This technique is sensitive to:

  \begin{itemize}
    \item High dimensionality
    \item Multimodal distribution
    \item Homoscedasticity
  \end{itemize}
\end{frame}
