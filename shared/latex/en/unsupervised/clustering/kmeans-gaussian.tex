\begin{frame}{K-Means --- Introduction}
  Expectation-maximization algorithm
  \V{["img/cluster-2", "th", 0.7] | image}
\end{frame}

\begin{frame}{K-Means --- Algorithm}
  \begin{itemize}
    \item Choose $k$ centroids as initial clusters
    \item Iterate between phases E (expectation) and M (maximization):
      \begin{description}
        \item[E] Assign to each point to the closest centroids
        \item[M] Readjust the centroids to the center of all the data points assigned to it
      \end{description}
  \end{itemize}  
\end{frame}

\begin{frame}{K-Means --- Example}
  \bluelink{https://youtu.be/BVFG7fd1H30}{Video of learning examples}
\end{frame}

\begin{frame}{K-Means --- Distances}
  Euclidean, Manhattan, or Chebychev distances
  \V{["img/distances-euclidienne-manahattan-chebychev", "tw", 0.9] | image}
\end{frame}

\begin{frame}{K-Means++}
  Improves initialization:

  \begin{itemize}
    \item Choose a random point as initial cluster
    \item For the each centroids, iterate:
      \begin{itemize}
        \item Calculate the distance of each point in the cluster associated to the centroid
        \item Move the centroid using a weighted summation depending of the distance of the data points from the centroid
      \end{itemize}
  \end{itemize}

  \onslide<+->{$\rightarrow$ Much better convergence properties!}
\end{frame}

\begin{frame}{Gaussian Mixture Model --- Introduction}
  Clusters are represented by mean vector and a covariance matrix.
  \V{["img/gaussian-mixture", "tw", 0.7] | image}
\end{frame}

\begin{frame}{Gaussian Mixture Model --- Miscellaneous}
  \begin{itemize}
    \item "\textit{Soft}" clustering: each cluster is a probability density function
    \item Each data point has a probability to be in any cluster
    \item EM algorithm, often initialized with K-Means
  \end{itemize}
\end{frame}
