\begin{frame}{Paradigm}
  \V{["rl", "tw", 0.5] | image}
  Where

  \begin{description}
    \item[$S_t$] State of the environment at timestep $t$
    \item[$A_t$] Action chosen by the agent at timestep $t$
    \item[$R_t$] Reward received by the agent at timestep $t$
  \end{description}
\end{frame}

\begin{frame}{Goal}
  Maximize the expected cumulative reward:

  \[
    G_t = \sum_{k=0}^T R_{t+k+1}
  \]

  where

  \begin{description}
    \item[$G_t$] Expected cumulative reward
    \item[$R_t$] Reward received at timestep $t$
  \end{description}
\end{frame}

\begin{frame}{Main families}
  \begin{itemize}
    \item Value based
    \item Policy based
    \item Model based
  \end{itemize}
\end{frame}

\begin{frame}{Value based}
  Compute a value for each possible state: its expected (optionally \red{discounted}) cumulative reward.

  \[
    V_\pi(s) = \mathbb{E}\left[
      R_{t + 1}
      + \red{\gamma} R_{t + 2}
      + \red{\gamma^2} R_{t + 3}
      + \dots
      \mid S_t = s
    \right]
  \]

  \pause

  To compute $a$ from $V_\pi$: pick the action $a$ that leads to the state with the highest value ($V_\pi(s)$).

  \pause

  Good approach when the number of states reachable from each state is small.
\end{frame}

\begin{frame}{Example --- Problem}
  Go from start to finish. Each move costs $1$ (reward of $-1$).

  \begin{tabular}{c|c|c|c|c|c|c}
    \cline{2-6}
     & & \cellcolor{black} & & & & \\
    \cline{2-6}
     & & \cellcolor{black} & & \cellcolor{black} & \cellcolor{black} & \\
    \cline{2-6}
    Entrance $\rightarrow$ & \phantom{-5} & \phantom{-5} & \phantom{-5} & \phantom{-5} & \phantom{-5} & $\leftarrow$ Exit \\
    \cline{2-6}
     & & \cellcolor{black} & \cellcolor{black} & \cellcolor{black} & \cellcolor{black} & \\
    \cline{2-6}
     & & \cellcolor{black} & \cellcolor{black} & \cellcolor{black} & \cellcolor{black} & \\
    \cline{2-6}
  \end{tabular}
\end{frame}

\begin{frame}{Example --- Value-based solution}
  Compute $V(s)$ for all states:

  \begin{tabular}{c|c|c|c|c|c|c}
    \cline{2-6}
      & -7 & \cellcolor{black} & -5 & -6 & -7 & \\
      \cline{2-6}
      & -6 & \cellcolor{black} & -4 & \cellcolor{black} & \cellcolor{black} & \\
      \cline{2-6}
      Entrance $\rightarrow$ & -5 & -4 & -3 & -2 & -1 & $\leftarrow$ Exit \\
      \cline{2-6}
      & -6 & \cellcolor{black} & \cellcolor{black} & \cellcolor{black} & \cellcolor{black} & \\
      \cline{2-6}
      & -7 & \cellcolor{black} & \cellcolor{black} & \cellcolor{black} & \cellcolor{black} & \\
      \cline{2-6}
  \end{tabular}

  \pause

  Corresponding policy: pick the action that leads to the state with the highest value:

  \begin{tabular}{c|c|c|c|c|c|c}
    \cline{2-6}
      & $\downarrow$ & \cellcolor{black} & $\downarrow$ & $\leftarrow$ & $\leftarrow$ & \\
      \cline{2-6}
      & $\downarrow$ & \cellcolor{black} & $\downarrow$ & \cellcolor{black} & \cellcolor{black} & \\
      \cline{2-6}
      Entrance $\rightarrow$ & $\rightarrow$ & $\rightarrow$ & $\rightarrow$ & $\rightarrow$ & $\rightarrow$ & $\leftarrow$ Exit \\
      \cline{2-6}
      & $\uparrow$ & \cellcolor{black} & \cellcolor{black} & \cellcolor{black} & \cellcolor{black} & \\
      \cline{2-6}
      & $\uparrow$ & \cellcolor{black} & \cellcolor{black} & \cellcolor{black} & \cellcolor{black} & \\
      \cline{2-6}
  \end{tabular}
\end{frame}

\begin{frame}{Policy based}
  Define a policy (probability distribution over actions given a state).

  \[
    \pi(a \mid s) = \mathbb{P}[A_t = a \mid S_t = s]
  \]

  Pick the policy with the highest expected cumulative reward.

  Good choice when the state space is big.
\end{frame}

\begin{frame}{Example --- Policy-based solution}
  \textbf{Directly} compute a policy that picks the action that leads to the best expected cumulative reward given a state:

  \begin{tabular}{c|c|c|c|c|c|c}
    \cline{2-6}
      & $\downarrow$ & \cellcolor{black} & $\downarrow$ & $\leftarrow$ & $\leftarrow$ & \\
      \cline{2-6}
      & $\downarrow$ & \cellcolor{black} & $\downarrow$ & \cellcolor{black} & \cellcolor{black} & \\
      \cline{2-6}
      Entrance $\rightarrow$ & $\rightarrow$ & $\rightarrow$ & $\rightarrow$ & $\rightarrow$ & $\rightarrow$ & $\leftarrow$ Exit \\
      \cline{2-6}
      & $\uparrow$ & \cellcolor{black} & \cellcolor{black} & \cellcolor{black} & \cellcolor{black} & \\
      \cline{2-6}
      & $\uparrow$ & \cellcolor{black} & \cellcolor{black} & \cellcolor{black} & \cellcolor{black} & \\
      \cline{2-6}
  \end{tabular}
\end{frame}

\begin{frame}{Value-policy hybrids}
  It's possible to mix both approaches (\textit{e.g.} actor-critic methods or Alpha Zero).
\end{frame}

\begin{frame}{Model based}
  Allow the agent to build a representation of its environment.

  To be compared with model-free approaches where only state-action dynamics are learned.
\end{frame}

\begin{frame}{Technical \& theoretical constraints}
  \begin{itemize}[<+->]
    \item The environnement often cannot be simulated perfectly 
    \item The reward might not be immediate
    \item The space to explore might be big
  \end{itemize}
\end{frame}

\begin{frame}{Technical \& theoretical constraints --- Stock market}
  \V{["bourse-ny", "tw", 1] | image}
\end{frame}

\begin{frame}{Technical \& theoretical constraints --- Chess}
  \V{["echec", "th", 0.5] | image}
  \pause
  \begin{center}
    $\approx 10^{120}$ possible games $\ggg 6 x 10^{85}$
    \newline
    (number of atoms in the observable universe)
  \end{center}
\end{frame}

\begin{frame}{Technical \& theoretical constraints --- Go}
  \V{["go", "th", 0.6] | image}
  \begin{center}
    \huge{$\approx 10^{600}$}
  \end{center}
\end{frame}
