\begin{frame}
  \frametitle{TF-IDF}
  One document = one vector of the size of a dictionary. (thus with fixed dimension) \\
  \begin{center}
    $\boxed{w_{ij} = tf_{ij}\log{\frac{N}{df_i}}}$
  \end{center}
  where :

  \begin{description}
    \item[$tf_{ij}$] Number of occurrences of the word $i$ in the document $j$
    \item[$df_i$] Number of documents containing the word $i$
    \item[$N$] Total number of documents
  \end{description}
  $\Rightarrow$ Scalar product, SVM, trees, neural networks, ...
\end{frame}

\begin{frame}{TF-IDF --- Why $df_i$?}
  Zipf's law, justifying the use of the term $df_i$.
  \V{"img/zipf-law" | image("tw", 0.9)}
\end{frame}

\begin{frame}{TF-IDF}
  Use $n$-grams of words or characters.
  \begin{center}
  Example: "The dog eats meat"
  \end{center}

  Word bigrams:

  \begin{itemize}
  \item the-dog, dog-eats, eats-meat
  \end{itemize}

  Character trigrams:

  \begin{itemize}
  \item the, \_do, dog, og\_, g\_e, \_ea, eat, ats, ts\_, s\_m, â€¦
  \end{itemize}
\end{frame}

\begin{frame}{Latent semantic analysis}
  Algorithmic family of topic models:

  \begin{itemize}
    \item $\approx$ PCA on the matrix of documents $\rightarrow$ relations between words
    \item Principal components $\approx$ topics
  \end{itemize}

  For example: one axis corresponds to the sport lexical field, another might be economy, etc.
\end{frame}
