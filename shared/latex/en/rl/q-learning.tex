\begin{frame}{Introduction}
  \begin{itemize}[<+->]
    \item Q-learning is a value-based method
    \item It builds a table of values for actions in each state
    \item It learns the values by updating them with observed rewards
  \end{itemize}
\end{frame}

\begin{frame}{Example --- Problem}
  Go from start to finish. Each move costs $1$ (reward of $-1$).

  \begin{tabular}{c|c|c|c|c|c|c}
    \cline{2-6}
     & & \cellcolor{black} & & & & \\
    \cline{2-6}
     & & \cellcolor{black} & & \cellcolor{black} & \cellcolor{black} & \\
    \cline{2-6}
    Entrance $\rightarrow$ & \phantom{-5} & \phantom{-5} & \phantom{-5} & \phantom{-5} & \phantom{-5} & $\leftarrow$ Exit \\
    \cline{2-6}
     & & \cellcolor{black} & \cellcolor{black} & \cellcolor{black} & \cellcolor{black} & \\
    \cline{2-6}
     & & \cellcolor{black} & \cellcolor{black} & \cellcolor{black} & \cellcolor{black} & \\
    \cline{2-6}
  \end{tabular}
\end{frame}

\begin{frame}{Q-table}
  The corresponding Q-table, initialized to 0s, would be:

  \begin{tabular}{ccccc}
    \toprule
    \diagbox{State}{Action} & $\uparrow$ & $\rightarrow$ & $\downarrow$ & $\leftarrow$ \\
    \midrule
    $(1, 1)$ & 0 & 0 & 0 & 0 \\
    $(1, 2)$ & 0 & 0 & 0 & 0 \\
    $(1, 3)$ & 0 & 0 & 0 & 0 \\
    $(1, 4)$ & 0 & 0 & 0 & 0 \\
    $(1, 5)$ & 0 & 0 & 0 & 0 \\
    $(2, 1)$ & 0 & 0 & 0 & 0 \\
    \multicolumn{5}{c}{…} \\
    $(5, 5)$ & 0 & 0 & 0 & 0 \\
    \bottomrule
  \end{tabular}
\end{frame}

\begin{frame}{Learning the values of the Q-table}
  Use Bellman equation to update a single value in the table:
  \[
    Q_{\text{new}}(s_t, a_t)
    \leftarrow
    \underbrace{Q(s_t, a_t)}_{\mathclap{\text{Old value}}}
    +
    \alpha
    \underbrace{\left[
      \overbrace{
        r_t
        +
        \gamma \max_a Q\left(s_{t+1}, a\right)
      }^{\mathclap{\text{New value}}}
      - \overbrace{Q(s_t, a_t)}^{\mathclap{\text{Old value}}}
    \right]}_{\mathclap{\text{Temporal difference}}}
  \]
  where
  \begin{description}
    \item[$\alpha$] Learning rate ($0 \leq \alpha \leq 1$)
    \item[$r_t$] Reward for picking $a_t$ in state $s_t$
    \item[$\gamma$] Discount factor ($0 \leq \gamma \leq 1$)
    \item[$\max_a Q\left(s_{t+1}, a\right)$] Maximum reward in state of arrival
  \end{description}
\end{frame}

\begin{frame}{Q-table update example}
  We started in $(3, 1)$ and picked action $\uparrow$.
  \begin{alignat*}{9}
    Q_{\text{new}}(s_t, a_t) & =
    Q(s_t, a_t)
    && +
    \alpha
    &&[
        r_t
        && +
        \gamma \max_a Q\left(s_{t+1}, a\right)
      && - Q(s_t, a_t)
    ] \\
    Q_{\text{new}}((3, 1), \uparrow) & =
    Q((3, 1), \uparrow)
    && +
    0.1
    &&[
        -1
        && +
        \gamma \max_a Q\left((2, 1), a\right)
      && - Q((3, 1), \uparrow)
    ] \\
    Q_{\text{new}}((3, 1), \uparrow) & =
    && -0.1 && && && \\
  \end{alignat*}

  \begin{tabular}{ccccc}
    \toprule
    \diagbox{State}{Action} & $\uparrow$ & $\rightarrow$ & $\downarrow$ & $\leftarrow$ \\
    \midrule
    \multicolumn{5}{c}{…} \\
    $(3, 1)$ & $-0.1$ & 0 & 0 & 0 \\
    \multicolumn{5}{c}{…} \\
    \bottomrule
  \end{tabular}
\end{frame}

\begin{frame}{Limitations}
  \begin{itemize}[<+->]
    \item Not a fit for big state spaces
    \item Although it converges to optimal values, it might not be data efficient
  \end{itemize}
\end{frame}

\begin{frame}{Improvements --- Deep Q-learning}
  \begin{itemize}[<+->]
    \item Use a network to predict $Q(s, a)$
    \item Use Bellman equation as a loss to train the network
    \item Ok for big state spaces!
    \item Not easy to train, classical RL stability practices are needed
  \end{itemize}
\end{frame}

\begin{frame}{Improvements --- Experience replay}
  Instead of going through a full episode each time, select random transitions from random episodes.

  $\rightarrow$ Uncorrelated training samples. Big improvement in data efficiency and stability.
\end{frame}

\begin{frame}{Improvements --- Double Q-learning}
  Train two Q-learners jointly to avoid stability problems in Q-learning.

  Learner A provides the value estimation for learner B, while learner B provides value estimation for learner A.

  They are trained on different episodes.
\end{frame}

\begin{frame}{Introduction by DeepMind on Atari games}
  \begin{itemize}
  \item 49 Atari games
  \item $210 \times 160$ pixels
  \item 8 to 16 actions
  \end{itemize}
  \V{["img/atari", "th", 0.5, "en"] | image}
\end{frame}

\begin{frame}{Technical details}
  \begin{itemize}
  \item $84 \times 84 \times 4$ input
  \item $\{-1, 0, 1\}$ reward depending on score variations
  \item Same preprocessing for all games. (50M frames $\approx$ 38 days in-game)
  \item 29/49 games: DQN $\geq$ Human
    \begin{itemize}
    \item 22 where DQN $>$ Human
    \item Humans have 2H of training time
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{DQN --- Pong}
  \bluelink{https://www.youtube.com/watch?v=lcVg9hVya-c}{DQN applied to Pong}
\end{frame}

\begin{frame}{DQN successors}
  \begin{description}[<+->]
    \item[R2D2] Uses recurrent neural networks
    \item[Never Give Up] Uses memory networks and exploration mechanisms
    \item[Agent57] Shares exploration mechanisms and learnings between games
  \end{description}

  \onslide<+->{Agent57 beats humans on the 57 games of the dataset}

  \onslide<+->{\bluelink{https://youtu.be/M9Yn1kYZb6E}{Including exploration games}}
\end{frame}
