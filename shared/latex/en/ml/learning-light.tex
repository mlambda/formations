\begin{frame}{Learning (or fit) procedure}
  Can vary a lot depending on models. Usually along the lines of:
  \begin{enumerate}
    \item Initialize the model to a random or empty state
    \item Compute the output of the model for some samples
    \item Compute a loss (difference between output and expected output)
    \item Adjust parameters to reduce the loss
  \end{enumerate}
\end{frame}

\begin{frame}{Fit scenarios}
  Several possibilities for a fit:

  \V{"plt/overfit-polynomial-en" | image("tw", 1)}

  \begin{alertblock}{Warning}
    Minimizing loss too much on a training set is not good!
  \end{alertblock}
\end{frame}

\begin{frame}{Data split}
  To fight overfitting, split the data to evaluate generalization at several stages:
  \begin{description}
  \item[Training data] Used to train the model
  \item[Validation data] Used to evaluate generalization \textbf{during} training
  \item[Test data] Used to evaluate generalization \textbf{after} training
  \end{description}
  → 60\%/20\%/20\% splits can be a first approach.
\end{frame}

\begin{frame}{Best fit}

  \V{"plt/learning-curve-en" | image("th", 0.6)}

  → Minimize error on the \textbf{validation} split
\end{frame}
