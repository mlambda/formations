\begin{frame}{Goals}
  \begin{itemize}[<+->]
    \item Quantify the quality of the model predictions
    \item Assert whether a business objective is met or not
    \item Test regressions
  \end{itemize}
\end{frame}

\begin{frame}{Evaluation of classification models}
  The first two metrics are by far the most used:
  \begin{description}
  \item[Precision]
    \[
    \frac{\text{true positives}}{\text{true positives + false positives}}
    \]
  \item[Recall]
    \[
    \frac{\text{true positives}}{\text{true positives + false negatives}}
  \]
  \item[F-measure] harmonic mean between precision and recall (also called F1 score)
  \end{description}
\end{frame}

\begin{frame}{Precision and recall illustrated}
  \V{"img/precisionrecall" | image("th", 0.7, "en")}
\end{frame}

\begin{frame}{ROC curve}
  Measure model performance (TPR vs FPR) at various decision thresholds.
  \begin{columns}
    \begin{column}{.48\textwidth}
    \begin{itemize}
      \item True Positive Rate:\\recall ($\frac{\text{TP}}{\text{TP} + \text{FN}}$)
      \item False Positive Rate:\\\emph{False alarm} probability ($\frac{\text{FP}}{\text{TN} + \text{FP}}$)
    \end{itemize}
    \end{column}
    \begin{column}{.52\textwidth}
      \V{"img/roc-space" | image("tw", 0.8, "en")}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Confusion matrix}
  Show clearly which confusions are made by the model
  \V{"img/confusion-matrix-nzmog" | image("th", 0.6, "en")}
\end{frame}

\begin{frame}{Evaluation of regression models}
  Usually closer to the learning criterion than classification: mean absolute error, or mean squared error.
\end{frame}
