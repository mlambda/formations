\begin{frame}{Learning (or fit) procedure}
  Can vary a lot depending on models. Usually along the lines of:
  \begin{enumerate}[<+(1)->]
    \item Initialize the model to a random or empty state
    \item Compute the output of the model for some samples
    \item Compute a loss (difference between output and expected output)
    \item Adjust parameters to reduce the loss
  \end{enumerate}
\end{frame}

\begin{frame}{Fit scenarios}
  Several possibilities for a fit:

  \V{["img/fit-nzmog-en", "tw", 1] | image}

  \onslide<+(1)->{
    \begin{alertblock}{Warning}
      Minimizing loss too much on a training set is not good!
    \end{alertblock}
  }
\end{frame}

\begin{frame}{Data split}
  To fight overfitting, split the data to evaluate generalization at several stages:
  \begin{description}
  \item[Training data] Used to train the model
  \item[Validation data] Used to evaluate generalization \textbf{during} training
  \item[Test data] Used to evaluate generalization \textbf{after} training
  \end{description}
  → 60\%/20\%/20\% splits can be a first approach.
\end{frame}

\begin{frame}{Best fit}

  \V{["img/learning-curve-nzmog-en", "th", 0.6] | image}

  → Minimize error on the \textbf{validation} split
\end{frame}

\begin{frame}{Cross-validation}
  Improvement over simple data split. With a high enough $k$, good estimation of generalization.
  \V{["img/k-fold-cross-validation-en", "tw", 0.9] | image}
\end{frame}

\begin{frame}{Regularization}
  \begin{minipage}[l]{0.49\linewidth}
    \begin{center}
      Regularization \\
      $\approx$\\
      avoid overfitting
    \end{center}
  \end{minipage}\hfill
  \begin{minipage}[l]{0.49\linewidth}
    \V{["img/overfitting-train-nzmog", "th", 0.3, "en"] | image}
  \end{minipage}\hfill
  Many techniques depending on the model:
  \begin{itemize}
  \item Weigth normal penalty
  \item Random noise
  \item Dropout
  \item Data augmentation
  \item …
  \end{itemize}
\end{frame}

\begin{frame}{Hyperparameters optimization}
  Hyperparameters: parameters \alert{not learned} by the model.
  \begin{exampleblock}{Examples}
  \begin{description}
  \item[Architecture of a network] Number of layers? Size of layers? …
  \item[Optimization] SGD, AdaBoost, Adam, …
  \item[Regularization] Noise rate, dropout rate, …
  \end{description}
  \end{exampleblock}
  Optimization by specific methods (\emph{e.g.} Gaussian Processes).
\end{frame}

