\begin{frame}{Overfit (and underfit)}

  \V{["img/overfitting-nzmog-en", "tw", 1, "en"] | image}
\end{frame}

\begin{frame}{Weight penalty}
  Penalize neurons with big weights with a term in the loss function:

  \begin{description}[<+(1)->]
    \item[L2 or ridge]
    \[
      \text{Loss} = \text{Loss} + \lambda\norm{w}_2^2
    \]
    \item[L1 or lasso]
    \[
      \text{Loss} = \text{Loss} + \lambda\norm{w}_1
    \]
    \item[L1 \& L2 or elastic net]
    \[
      \text{Loss} = \text{Loss} + \lambda_{L1}\norm{w}_1 + \lambda_{L2}\norm{w}_2^2
    \]
  \end{description}

  \onslide<+(1)->{Where the $\lambda$s are parameters controlling the regularization strength.}
\end{frame}

\begin{frame}{Early stopping}
  \V{["plt/early-stopping-en", "th", 0.7] | image}
\end{frame}

\begin{frame}{Data augmentation}
  \V{["img/data-augmentation-nzmog-en", "tw", 1.0] | image}
\end{frame}

\begin{frame}{Dropout}
  \V{["tikz/neural-networks/dropout", "tw", 1] | image}
\end{frame}

\begin{frame}{Multi-task learning}
  \V{["img/multi-task-v2", "tw", 0.8] | image}
\end{frame}

\begin{frame}{Complex regularizations}
  Recently (since 2018), several research papers mentioned using GANs purely as a regularization mechanism

  \V{["tikz/neural-networks/gan/gan-en", "tw", 0.8] | image}
\end{frame}
