\begin{frame}{General principle}
  \V{["autoencoder-schema", "tw", 0.9] | image}
\end{frame}

\begin{frame}{Linear auto-encoder}
  With linear activations only, auto-encoder $\approx$ principal component analysis

  \V{["pca-nuage", "th", 0.5, "en"] | image}
\end{frame}

\begin{frame}{Deep auto-encoder}
  Also called stacked auto-encoders (even though they were known for a specific training method that is not used anymore).

  \V{["autoencoder-stacked", "tw", 0.9] | image}
\end{frame}

\begin{frame}{Denoising auto-encoder}
  Train a model to recover its input, with noise removed.

  \V{["autoencoder-denoising", "tw", 0.9] | image}
\end{frame}

\begin{frame}{Sparse auto-encoder}
  Keep only $k$ hidden units in the bottle-neck. The rest is set to 0.

  \V{["autoencoder-sparse", "tw", 0.7] | image}
\end{frame}

\begin{frame}{Variational auto-encoder}
  Bottleneck is comprised of gaussian distributions instead of scalars: the model learns their mean and variance.

  \V{["autoencoder-variational", "tw", 0.9] | image}
\end{frame}

\begin{frame}{LSTM auto-encoder}
  \V{["autoencoder-lstm", "tw", 0.9] | image}
\end{frame}
