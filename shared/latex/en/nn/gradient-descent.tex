
\begin{frame}{Principle}
  \begin{columns}
    \begin{column}{0.5\textwidth}
      Compute the error gradient \emph{w.r.t.} the parameters:
      \[
      \frac{\partial{E}}{\partial{\theta_i}}
      \]
      Update:
      \[
        \theta_i \leftarrow \theta_i - \gamma\frac{\partial{E}}{\partial{\theta_i}}
      \]
      where $0 < \gamma < 1$ (learning rate)
    \end{column}
    \begin{column}{0.5\textwidth}
      \V{"plt/linear-regression-error-surface-en" | image("tw", 1)}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Visualization}
  \bluelink{https://youtu.be/gK2dIZCbLZ0}{Gradient descent visualization}
\end{frame}

\begin{frame}{Algorithm}
  \begin{enumerate}
    \item Parameters $\theta_i$ random initialization for the model $f_{\theta}, i\in[1..k]$
    \item For $n$ epochs:
      \begin{itemize}
        \item Sampling $m$ data in $b$ \textbf{batchs}
        \item For each batch:
        \begin{itemize}
          \item \textbf{Forward}: Compute predictions $\hat{y}_j = f_{\theta}(x_j), j\in[1..m]$
          \item Computation of the error (loss) $|\hat{y}_j-y_j|, j\in[1..m]$
          \item \textbf{Backward}: Error gradient retropropagation (model update)
          \[
            \theta_i \leftarrow \theta_i - \gamma\frac{\partial{E}}{\partial{\theta_i}}
          \]
        \end{itemize}
      \item Stopping criteria computation
      \end{itemize}
  \end{enumerate}

  Where $k$, $n$, $m$ et $\gamma$ are model's hyperparameters.
\end{frame}


\begin{frame}{Derivation example --- Linear regression}
  \begin{minipage}[l]{0.49\linewidth}
    \begin{align*}
      E & = \frac{1}{2n} \sum_{i=1}^n( \hat{y}_i - y_i )^2 \\
      E & = \frac{1}{2n}\sum_{i=1}^n( (\theta_0 + \theta_1 x_i) - y_i )^2 \\
      & ... \\
      \frac{\partial E}{\partial \theta_0} & = \frac{1}{n}\sum^n_{i=1}(\theta_0 + \theta_1 x_i - y_i) \\
      \frac{\partial E}{\partial \theta_1} & = \frac{1}{n}\sum^n_{i=1}(\theta_0 + \theta_1 x_i - y_i) x_i \\
    \end{align*}
  \end{minipage}\hfill
  \begin{minipage}[c]{0.49\linewidth}
    \begin{center}
      $\boxed{\hat{y}_i = \theta_0 + \theta_1 x_i}$
    \end{center}

    \begin{center}
      $\boxed{U^{2\;\prime}=2U' \times U}$
    \end{center}
    \vfill
    \begin{center}
      \textbf{Update}

      \begin{align*}
        \theta_0 & \leftarrow \theta_0 - \gamma\frac{\partial{E}}{\partial{\theta_0}} \\
        \theta_1 & \leftarrow \theta_1 - \gamma\frac{\partial{E}}{\partial{\theta_1}} \\
      \end{align*}
      where $0 < \gamma < 1$ (learning rate)
    \end{center}
  \end{minipage}\hfill
\end{frame}
