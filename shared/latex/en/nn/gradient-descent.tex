
\begin{frame}{Principle}
  Compute the error gradient \emph{w.r.t.} the parameters:
  %TODO: check wrt
  \[
  \frac{\partial{Err}}{\partial{w_i}}
  \]
  Update:
  \[
  w_i' = w_i - \gamma * grad 
  \]
  o√π : $0 < \gamma < 1$ (learning rate)
  \V{["gradient-nzmog", "tw", 0.25, "en"] | image}
\end{frame}

\begin{frame}{Algorithm}
  \begin{enumerate}[<+->]
    \item Random initialization of the model
    \item While no stopping criterion is satisfied:
      \begin{itemize}
        \item Selection of a random \textbf{batch} of samples
        \item \textbf{Forward}: Model output computation on the selected batch
        \item Computation of the error (loss) compared to the expected output
        \item \textbf{Backward}: Error gradient retropropagation (model update)
        \item Stopping criteria computation
      \end{itemize}
  \end{enumerate}
\end{frame}


\begin{frame}{Derivation example --- Linear regression}
  \begin{minipage}[l]{0.49\linewidth}
    \begin{align*}
      E_\Omega & = \frac{1}{2n} \sum_{i=1}^n( \hat{y_i} - y_i )^2 \\
      E_\Omega & = \frac{1}{2n}\sum_{i=1}^n( \hat{y_i} - (ax_i + b) )^2 \\
      & ... \\
      \frac{\partial E_\Omega}{\partial a} & = \frac{1}{n}\sum^n_{i=1}(ax_i + b - \hat{y_i})x_i \\
      \frac{\partial E_\Omega}{\partial b} & = \frac{1}{n}\sum^n_{i=1}(ax_i + b - \hat{y_i}) \\
    \end{align*}
  \end{minipage}\hfill
  \begin{minipage}[c]{0.49\linewidth}
    \begin{center}
      $\boxed{y = ax+b}$
    \end{center}

    \begin{center}
      $\boxed{U^{2\;\prime}=2U' \times U}$
    \end{center}
    \vfill
    \begin{center}
      \textbf{Update}

      $a \leftarrow a - \gamma\frac{\partial{E_{\Omega}}}{\partial{a}}$ \\
      $b \leftarrow b - \gamma\frac{\partial{E_{\Omega}}}{\partial{b}}$ \\
      $\;$ \\
      where $0 < \gamma < 1$ (learning rate)
    \end{center}
  \end{minipage}\hfill
\end{frame}

\begin{frame}{Example}
  Random initialization ($\gamma = 0.01$)
  \begin{itemize}
    \item a = 0.58 ($\hat{a} = 3.0$)
    \item b = 0.25 ($\hat{b} = 0.5$)
  \end{itemize}
  \V{["regression-1", "th", 0.6] | image}
\end{frame}

\begin{frame}{Example}
  \begin{itemize}
    \item a = 0.58 ($\hat{a} = 3.0$)
    \item b = 0.25 ($\hat{b} = 0.5$)
  \end{itemize}
  \V{["regression-error", "th", 0.6] | image}
\end{frame}

\begin{frame}{Example}
  \begin{itemize}
    \item a = 1.50 ($\hat{a} = 3.0$)
    \item b = 0.35 ($\hat{b} = 0.5$)
  \end{itemize}
  \V{["regression-2", "th", 0.6] | image}
\end{frame}

\begin{frame}{Example}
  \begin{itemize}
    \item a = 2.10 ($\hat{a} = 3.0$)
    \item b = 0.40 ($\hat{b} = 0.5$)
  \end{itemize}
  \V{["regression-3", "th", 0.6] | image}
\end{frame}

\begin{frame}{Example}
  \begin{itemize}
    \item a = 2.48 ($\hat{a} = 3.0$)
    \item b = 0.43 ($\hat{b} = 0.5$)
  \end{itemize}
  \V{["regression-4", "th", 0.6] | image}
\end{frame}

\begin{frame}{Example}
  \begin{itemize}
    \item a = 2.73 ($\hat{a} = 3.0$)
    \item b = 0.46 ($\hat{b} = 0.5$)
  \end{itemize}
  \V{["regression-5", "th", 0.6] | image}
\end{frame}

\begin{frame}{Example}
  \begin{itemize}
    \item a = 2.89 ($\hat{a} = 3.0$)
    \item b = 0.47 ($\hat{b} = 0.5$)
  \end{itemize}
  \V{["regression-6", "th", 0.6] | image}
\end{frame}

\begin{frame}{Example}
  \begin{itemize}
    \item a = 2.99 ($\hat{a} = 3.0$)
    \item b = 0.48 ($\hat{b} = 0.5$)
  \end{itemize}
  \V{["regression-7", "th", 0.6] | image}
\end{frame}

\begin{frame}{Example}
  \begin{itemize}
    \item a = 3.06 ($\hat{a} = 3.0$)
    \item b = 0.49 ($\hat{b} = 0.5$)
  \end{itemize}
  \V{["regression-8", "th", 0.6] | image}
\end{frame}

\begin{frame}{Example}
  \begin{itemize}
    \item a = 3.10 ($\hat{a} = 3.0$)
    \item b = 0.49 ($\hat{b} = 0.5$)
  \end{itemize}
  \V{["regression-9", "th", 0.6] | image}
\end{frame}

\begin{frame}{Example}
  \begin{itemize}
    \item a = 3.13 ($\hat{a} = 3.0$)
    \item b = 0.50 ($\hat{b} = 0.5$)
  \end{itemize}
  \V{["regression-10", "th", 0.6] | image}
\end{frame}
