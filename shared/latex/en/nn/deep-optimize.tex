\begin{frame}{Vanishing and exploding gradients}
  \V{["optimization/vanishing-gradients-en", "tw", 1.0] | image}
\end{frame}

\begin{frame}{Vanishing gradient}
  Use ReLus instead of sigmoids
  \vfill
  \V{["activations/relu", "tw", 1] | image}
\end{frame}

\begin{frame}{Exploding gradients}
  Clip the gradient
  \V{["gradient-clipping-nzmog-en", "th", 0.5] | image}
  For recurrent neural networks $\Rightarrow$ orthogonal matrix initialization
\end{frame}

\begin{frame}{Optimization algorithm}
  Better than vanilla stochastic Gradient Descent
  \begin{itemize}[<+(1)->]
    \item Adaptative Gradient Algorithm (AdaGrad)
    \item Root Mean Square Propagation (RMSProp)
    \item Adaptative Moment Estimation (Adam)    $\Leftarrow$
    \item â€¦ AdaBound (2019)?
  \end{itemize}
\end{frame}

\begin{frame}{Adaptative Gradient Algorithm (AdaGrad)}
  Compute a learning rate per parameter and iteration.
\end{frame}

\begin{frame}{Root Mean Square Propagation (RMSProp)}
  Use inertia in the applied gradient:
  $$
    Grad_t = Grad_t + Grad_{t-1}
  $$
\end{frame}
