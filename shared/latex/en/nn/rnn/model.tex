\begin{frame}{Introduction}
  Caractéristiques des réseaux récurrents~:

  \begin{itemize}[<+->]
    \item Gestion des séquences
    \item Traitement séquentiel des entrées
    \item Réplication d'un réseau autant de fois qu'il y a d'entrées
  \end{itemize}
\end{frame}

\begin{frame}{Modèle}
  \V{["tikz/neural-networks/rnn/simple-unfold", "th", 0.25] | image}

  \begin{columns}
    \begin{column}{0.5\tw}
      \begin{description}
        \item[$x_t$] Entrée
        \item[$h_t$] Couche cachée
        \item[$o_t$] Sortie
        \item[$U, V, W$] Matrices de poids
        \item[$b_h, b_o$] Vecteurs de biais
        \item[$\sigma_h, \sigma_o$] Activations tanh
      \end{description}
    \end{column}
    \begin{column}{0.5\tw}
      \begin{align*}
      h_t & = \sigma_h(U x_t + V h_{t - 1} + b_h) \\
      o_t & = \sigma_o(W h_t + b_o) \\
      \end{align*}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Prédiction d'une sortie}
  Exemple~: détection de polarité.

  \begin{figure}
    \centering
    \V{["tikz/neural-networks/rnn/lastoutput", "th", 0.7] | image}
  \end{figure}
\end{frame}

\begin{frame}{Prédiction d'autant de sorties qu'il y a d'entrées}
  \V{["tikz/neural-networks/rnn/alloutputs", "tw", 0.5] | image}
\end{frame}

\begin{frame}{Prédiction d'un nombre arbitraire de sorties}
  \V{["tikz/neural-networks/rnn/seq2seq", "tw", 1] | image}
\end{frame}

\begin{frame}{Problème du gradient qui disparaît (vanishing gradient)}
  \V{["tikz/optimization/vanishing-gradients-rnn", "th", 0.7] | image}
\end{frame}
