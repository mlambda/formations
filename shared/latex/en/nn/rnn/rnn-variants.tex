\begin{frame}{Gated Recurrent Unit --- Principle}
  Very close to LSTM. Uses 2 gates instead of 3:
  \begin{description}[<+(1)->]
    \item[Update gate] $\approx$ Combined input \& forget LSTM gates
    \item[Reset gate] $\approx$ Influence of memory during the update
  \end{description}
  \onslide<+(1)->{$\rightarrow$ Less parameters than LSTM, can be as efficient on some datasets}
\end{frame}

\begin{frame}{Gated Recurrent Unit --- Theoretical details}
  \V{["neural-networks/rnn/gru", "tw", 0.55] | image}
  \begin{description}[<+->]
    \item[Update gate] $z_t = \sigma (W_z \times x_t + U_z \times h_{t - 1} + b_z)$
    \item[Reset gate] $r_t = \sigma (W_r \times x_t + U_r \times h_{t - 1} + b_r)$
    \item[Output] $h_t = (1 - z_t) \circ h_{t - 1} + z_t \circ \tanh(W_h \times x_t + U_h (r_t \circ h_{t - 1}) + b_h)$
  \end{description}
\end{frame}

\begin{frame}{Bidirectional recurrent neural networks}
  \V{["neural-networks/rnn/bidirectional-en", "tw", 0.8] | image}
  \begin{itemize}[<+(1)->]
    \item Can learn longer dependencies
    \item Model both left-right and right-left dependencies
  \end{itemize}
\end{frame}

\begin{frame}{Other variants}
  \begin{itemize}[<+->]
    \item Recurrent depth
    \item Skip-connections (\emph{e.g.} connections between layers 1 \& 3)
    \item Residuals (learn the difference from the input instead of the output directly)
    \item â€¦
  \end{itemize}
\end{frame}

\fmg{\begin{frame}{Other variants}
  \V{["spaghetti", "tw", 1] | image}
\end{frame}}
