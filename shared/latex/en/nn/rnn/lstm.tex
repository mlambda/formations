\begin{frame}{Principle}
  Avoid vanishing gradient:
  \begin{itemize}[<+->]
    \item Introduce a memory
    \item Use only additive updates to the memory (better for gradient conservation)
    \item “Protect” this memory with gates
  \end{itemize}
\end{frame}

\begin{frame}{Solved vanishing gradient problem (almost)}
  \V{"tikz/optimization/vanishing-gradients-lstm-en" | image("tw", 0.7)}
\end{frame}

\begin{frame}{Theoretical details}
  \V{"tikz/neural-networks/rnn/lstm" | image("th", 0.37)} 
  \begin{tabular}{ lrll } 
    Forget gate & $F_t=$ & $\sigma(W_F \times x_t + U_F \times h_{t - 1} + b_F)$ & $\in[0;1]$ \\
    Input gate & $I_t=$ &  $\sigma(W_I \times x_t + U_I \times h_{t - 1} + b_I)$ & $\in[0;1]$ \\
    Output gate & $O_t=$ & $\sigma(W_O \times x_{t} + U_O \times h_{t - 1} + b_O)$ & $\in[0;1]$ \\
  \end{tabular}
  \begin{tabular}{ ll } 
    \hline
    Accumulateur & $\boldsymbol{c_t = F_t \times c_{t - 1} + I_t \times \tanh(W_c \times x_t + U_c \times h_{t-1} + b_c)}$ \\
    \hline
    Hidden & $h_t = O_t \times \tanh(c_t)$ \\
    Output & $o_t = f(W_o \times h_t + b_o)$ \\
  \end{tabular}
\end{frame}
