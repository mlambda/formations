\begin{frame}{Principle}
  Avoid vanishing gradient:
  \begin{itemize}[<+->]
    \item Introduce a memory
    \item Use only additive updates to the memory (better for gradient conservation)
    \item “Protect” this memory with gates
  \end{itemize}
\end{frame}

\begin{frame}{Solved vanishing gradient problem (almost)}
  \V{["tikz/optimization/vanishing-gradients-lstm-en", "tw", 0.7] | image}
\end{frame}

\begin{frame}{Theoretical details}
  \V{["tikz/neural-networks/rnn/lstm-en", "th", 0.45] | image}
  \begin{description}
  \item[Forget gate] $F_t = \sigma(W_F \times x_t + U_F \times h_{t - 1} + b_F)$ 
  \item[Input gate] $I_t = \sigma(W_I \times x_t + U_I \times h_{t - 1} + b_I)$
  \item[Output gate] $O_t = \sigma(W_O \times x_{t} + U_O \times h_{t - 1} + b_O)$
  \item[Memory update] $c_t = F_t \circ c_{t - 1} + I_t \circ \tanh(W_c \times x_t + U_c \times h_{t-1} + b_c)$
  \item[Output] $h_t = O_t \circ \tanh(c_t)$
  \item[Transformed output] $o_t = f(W_o \times h_t + b_o)$
  \end{description}
\end{frame}
