\begin{frame}{Relation with linear regression}
  \V{["regression-10", "tw", 0.9] | image}
\end{frame}

\begin{frame}{Relation with linear regression}
  \V{["separable-problem-linear-solution-nzmog", "tw", 0.9] | image}
\end{frame}

\begin{frame}{Need to go beyond linear regression}
  \V{["xor-nzmog", "tw", 0.9] | image}
\end{frame}

\begin{frame}{(Weak) relation with biology}
  \V{["biological-neuron", "tw", 1] | image}
\end{frame}

\begin{frame}{Artificial neuron model}
  \V{["neuron-model", "tw", 0.9] | image}
\end{frame}

\begin{frame}{Combination to form a neural network}
  Combination of many neurons to form a neural network:
  \begin{description}[<+(1)->]
    \item[In parallel] Compute results independently in a same layer
    \item[In series] Take as inputs the output of the previous layer
  \end{description}
\end{frame}

\begin{frame}{Two types of neurons}
  Important distinction between two types of neurons:
  \begin{description}[<+(1)->]
    \item[Hidden neurons] Neurons of intermediate layers. Increase the network capacity
    \item[Output neurons] Neurons of the final layer. Depend \textbf{only} on the kind of output
  \end{description}
\end{frame}

\begin{frame}{Shallow network}
  \centering
  \V{["neural-networks/shallow-ff-en", "tw", 1] | image}
\end{frame}

\begin{frame}{One hidden layer network}
  \centering
  \V{["neural-networks/one-hidden-layer-ff-en", "tw", 1] | image}
\end{frame}

\begin{frame}{Deep network}
  \centering
  \V{["neural-networks/deep-ff-en", "tw", 1] | image}
\end{frame}

\begin{frame}{A theoretically interesting model}
  \textbf{Kurt Hornik, 1991:} Universal approximation theorem
  \V{["activations/approximation", "tw", 1] | image}
\end{frame}

\begin{frame}{Matrix formulation --- Sample}
  Represent a sample as a $d$ dimensional vector. Each dimension corresponds to a feature:

  \[
    \vect{x} = \begin{bmatrix}
      x_1 & x_2 & \dots & x_d
    \end{bmatrix}
  \]

  \begin{alertblock}{Warning}
    The orientation of the \vect{x} vector differs from the mathematical convention (column vector).
  \end{alertblock}

  Represent a dataset as a matrix:

  \[
    \vect{X} = \begin{bmatrix}
      X_{1,1} & X_{1,2} & \dots & X_{1,d} \\
      X_{2,1} & X_{2,2} & \dots & X_{2,d}
    \end{bmatrix}
  \]

\end{frame}

\begin{frame}{Matrix formulation --- Weights}
  Represent weights as a matrix: each column contains the weights of a neuron.
  The matrix contains the weights of a full layer.

  Biases can be separated into a vector.

  \begin{align*}
    \vect{W} & = \begin{bmatrix}
      W_{1,1} & W_{1,2} & \dots  & W_{1,n} \\
      W_{2,1} & W_{2,2} & \dots  & W_{2,n} \\
      \vdots & \vdots & \ddots & \vdots \\
      W_{d,1} & W_{d,2} & \dots  & W_{d,n}
    \end{bmatrix} \\
    \vect{b} & = \begin{bmatrix}
      b_1 & b_2 & \dots & b_n
    \end{bmatrix}
  \end{align*}
\end{frame}

\begin{frame}{Full matrix formulation}
  \footnotesize
  \begin{align*}
    O & = \sigma (\vect{X}.\vect{W} + \vect{b}) \\
    & = \sigma \left(
    \begin{bmatrix}
      X_{1,1} & X_{1,2} & \dots & X_{1,d} \\
      X_{2,1} & X_{2,2} & \dots & X_{2,d}
    \end{bmatrix}
    \begin{bmatrix}
      W_{1,1} & W_{1,2} & \dots  & W_{1,n} \\
      W_{2,1} & W_{2,2} & \dots  & W_{2,n} \\
      \vdots & \vdots & \ddots & \vdots \\
      W_{d,1} & W_{d,2} & \dots  & W_{d,n}
    \end{bmatrix}
    +
    \begin{bmatrix}
      b_1 & b_2 & \dots & b_n
    \end{bmatrix}
    \right) \\
    & = \begin{bmatrix}
      O_{1,1} & O_{1,2} & \dots & O_{1,n} \\
      O_{2,1} & O_{2,2} & \dots & O_{2,n}
    \end{bmatrix}
  \end{align*}
  Where:
  \begin{description}[<+->]
    \item[$\vect{X}$] $d$ dimensional input data
    \item[$\vect{W}$ \& $\vect{b}$] Weights of the $n$ neurons of the layer
    \item[$\bm{\sigma}$] Activation function
    \item[$\vect{O}$] Output of the network
  \end{description}
\end{frame}

\begin{frame}{Activation functions --- How to pick one}
  \begin{itemize}[<+->]
    \item Mathematical properties (\emph{e.g.} gradient conservation, …)
    \item Learning properties (\emph{e.g.} dead weights, …)
    \item Ease of computation
    \item Output interval and specific constraints for the last layer
  \end{itemize}
\end{frame}

\begin{frame}{Activation functions --- Most used}
  \begin{itemize}
  \item Sigmoid
  \item Tanh
  \item Softmax
  \item ReLU
  \item …
  \end{itemize}
\end{frame}

\begin{frame}{Activation functions --- Sigmoid}
  \begin{center}
    \V{["activations/sigmoid", "tw", 1] | image}
  \end{center}

  \begin{description}[<+->]
    \item[Definition] \[\phi(x) = \frac{1}{1 + e^{-x}}\]
    \item[Derivative] \[\phi'(x)=\phi(x)(1-\phi(x))\]
  \end{description}
\end{frame}

\begin{frame}{Activation functions --- Tanh}
  \begin{center}
    \V{["activations/tanh", "tw", 1] | image}
  \end{center}

  \begin{description}[<+->]
    \item[Definition] \[\tanh(x)=\frac{1-e^{-2x}}{1+e^{-2x}}\]
    \item[Derivative] \[\tanh'(x)=1-\tanh^2(x)\]
  \end{description}
\end{frame}

\begin{frame}{Activation functions--- ReLU}
  \begin{center}
    \V{["activations/relu", "tw", 0.6] | image}
  \end{center}

  \begin{description}[<+->]
    \item[Definition] \[\ReLU(x)=\begin{cases} 0 & x \leq 0 \\ x & x > 0\end{cases}\]
    \item[Derivative] \[\ReLU'(x)=\begin{cases} 0 & x \leq 0 \\ 1 & x > 0\end{cases}\]
  \end{description}
\end{frame}

\begin{frame}{Activation functions --- Function approximation}
  \centering
    \V{["activations/approximation", "tw", 1] | image}
  {\small
    \begin{align*}
      n_1 & = \ReLU(-5x - 7.7) & n_4 & = \ReLU(1.2x - 0.2) \\
      n_2 & = \ReLU(-1.2x - 1.3) & n_5 & = \ReLU(2x - 1.1) \\
      n_3 & = \ReLU(1.2x - 1) & n_6 & = \ReLU(5x - 5) \\
    \end{align*}
  }
\end{frame}

\begin{frame}{Activation functions --- Softmax}
  \begin{description}[<+->]
    \item[Definition] \[\softmax(x_i) = \frac{\exp(x_i)}{\sum_{k = 1}^n{\exp(x_k)}}\]
    \item[Key property] \[\sum_{i = 1}^n{\softmax(x_i)} = 1\]
    \item[Gradient] \[
      \frac{\partial \softmax(x_i)}{\partial x_j} =
        \begin{cases}
          \softmax(x_i)(1 - \softmax(x_j)) & i = j \\
          -\softmax(x_i)\softmax(x_j) & i \neq j
        \end{cases}
      \]
  \end{description}
  
\end{frame}
