\begin{frame}{Introduction}
  In 2012, AlexNet crushed the ImageNet competition.
  
  Since then, many architectures have been proposed.

  Top 5 classification error dropped from $15\%$ to $\sim2\%$.

  Most are available \emph{off-the-shelf} in the big deep learning libraries.
\end{frame}

\begin{frame}{AlexNet}
  \begin{columns}
    \begin{column}{0.5\textwidth}
      \begin{itemize}
        \item First deep CNN trained on GPUs
        \item Introduced ReLU as a very good non-linearity to train deep networks
        \item Otherwise similar to LeNet --- only deeper
      \end{itemize}
    \end{column}
    \begin{column}{0.5\textwidth}
      \V{["img/d2l/alexnet", "th", 0.7, "en"] | image}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{VGG}
  \begin{columns}
    \begin{column}{0.45\textwidth}
      \begin{itemize}
        \item Introduced the notion of architectural blocks in CNNs
        \item Otherwise similar to AlexNet --- only bigger \& deeper
        \item Was very popular as a fine-tuning base model
      \end{itemize}
    \end{column}
    \begin{column}{0.55\textwidth}
      \V{["img/d2l/vgg", "tw", 1, "en"] | image}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Network in Network}
  \begin{columns}
    \begin{column}{0.4\textwidth}
      Uses  $1 \times 1$ kernels to apply a “network in a network”.

      They do not change the spatial structure, only the number of channels.
    \end{column}
    \begin{column}{0.6\textwidth}
      \V{["img/d2l/nin", "tw", 1, "en"] | image}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Inception/GoogLeNet --- Block}
  Uses a block that combines kernels of different sizes.

  \V{["img/d2l/inception", "tw", 0.7, "en"] | image}

  What do the $1 \times 1$ kernels provide?
\end{frame}

\begin{frame}{Inception/GoogLeNet --- Model}
  \begin{columns}
    \begin{column}{0.6\textwidth}
      Apart from the new block design, the architecture is standard.
    \end{column}
    \begin{column}{0.4\textwidth}
      \V{["img/d2l/inception-full", "th", 0.7, "en"] | image}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{ResNet --- Residual blocks}
  \begin{columns}
    \begin{column}{0.5\textwidth}
      \begin{itemize}
        \item Major design contribution: the residual connection.
        \item Changes the learning problem from learning $y$ from $x$ to learning $y - x$.
        \item \emph{e.g.} the identity is now trivial to learn.
        \item  Gradient flows very well in those networks.
        \item  It also uses batch normalization (center and reduce data at each layer) to stabilize learning.
      \end{itemize}
    \end{column}
    \begin{column}{0.5\textwidth}
      \V{["img/d2l/residual-block", "tw", 1, "en"] | image}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{ResNet --- ResNet blocks}
  ResNet ---~like GoogLeNet~--- uses $1 \times 1$ kernels to reduce dimensionality.

  \V{["img/d2l/resnet-block", "th", 0.5, "en"] | image}
\end{frame}

\begin{frame}{ResNet --- Model}
  \begin{columns}
    \begin{column}{0.5\textwidth}
      Rest of the model is standard.
    \end{column}
    \begin{column}{0.5\textwidth}
      \V{["img/d2l/resnet18", "th", 0.7, "en"] | image}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{DenseNet --- Block}
  Variation on ResNet: use concatenation of the input instead of addition.

  \V{["img/d2l/densenet-block", "th", 0.5, "en"] | image}
\end{frame}

\begin{frame}{DenseNet --- Model}
  \V{["img/d2l/densenet", "tw", 1, "en"] | image}
\end{frame}

\begin{frame}{Comparison}
  \V{["img/cnn-comparison", "tw", 1, "en"] | image}
\end{frame}
