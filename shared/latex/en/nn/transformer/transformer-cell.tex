\begin{frame}{Transformer cell}
    \V{"tikz/neural-networks/transformer/transformer-cell-en" | image("th", 0.7)}
\end{frame}

\begin{frame}{Attention mechanism}
    \V{"tikz/neural-networks/transformer/self-attention" | image("tw", 0.8, "en")}
    \[\vec{r}_i=\sum_{j=1}^{n} w_j*\vec{v}_j\]
    o√π $\left \{w_j\right \}$ are the attention weights between $\vec{q}_i$ and all $\vec{k}_j$
\end{frame}

\begin{frame}{Multi-head attention}
    \V{"tikz/neural-networks/transformer/multi-head-attention-en" | image("th", 0.75)}
\end{frame}
