\begin{frame}{Transformer cell}
    \V{["tikz/neural-networks/transformer/transformer-cell-en", "th", 0.7] | image}
\end{frame}

\begin{frame}{Attention mechanism}
    \V{["tikz/neural-networks/transformer/self-attention", "tw", 0.8, "en"] | image}
    \[\vec{r}_i=\sum_{j=1}^{n} w_j*\vec{v}_j\]
    o√π $\left \{w_j\right \}$ are the attention weights between $\vec{q}_i$ and all $\vec{k}_j$
\end{frame}

\begin{frame}{Multi-head attention}
    \V{["tikz/neural-networks/transformer/multi-head-attention-en", "th", 0.75] | image}
\end{frame}
