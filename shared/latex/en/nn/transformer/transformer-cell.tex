\begin{frame}{Transformer cell}
    \V{["tikz/neural-networks/transformer/transformer-cell", "th", 0.7] | image}
\end{frame}

\begin{frame}{Attention mechanism}
    \V{["tikz/neural-networks/transformer/self-attention", "tw", 0.8] | image}
    \[\vec{r}_i=\sum_{j=1}^{n} w_j*\vec{v}_j\]
    o√π $\left \{w_j\right \}$ sont les poids d'attention entre $\vec{q}_i$ et tous les $\vec{k}_j$
\end{frame}

\begin{frame}{Multi-head attention}
    \V{["tikz/neural-networks/transformer/multi-head-attention", "th", 0.75] | image}
\end{frame}
