TODO DQN ATARI


Double DQN: Remove upward bias caused by $max_a Q( s , a , w )$
Current Q-network w is used to select actions
Older Q-network w − is used to evaluate actions $l = ( r + γ.Q ( s′ , argmax_{a′} Q ( s′ , a′ , w ) , w − ) − Q ( s , a , w ) )^2$

Prioritised replay:  Weight experience according to surprise
Store experience in priority queue according to DQN error
$\norme{ r + γ max_{a′} Q ( s′ , a′ , w − ) − Q ( s , a , w )}$

Duelling network:  Split Q-network into two channels
Action-independent value function $V ( s , v )$
Action-dependent advantage function $A ( s , a , w )$

$Q ( s , a ) = V ( s , v ) + A ( s , a , w )$

Combined algorithm:  3 x mean Atari score vs Nature DQN
