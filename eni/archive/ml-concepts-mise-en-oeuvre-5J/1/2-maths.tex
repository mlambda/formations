\documentclass{formation}
\title{Mathématiques pour le machine learning}
\subtitle{Module 2}

\begin{document}

\maketitle

\section{Objectifs}

\begin{frame}
  \frametitle{Objectifs}

  \begin{itemize}
  \item exprimer des transformations de données grâce à
    l'algèbre linéaire
  \item minimiser des fonctions analytiquement
  \item décrire l'incertain
  \item décrire des données
  \end{itemize}
\end{frame}

\section{Algèbre linéaire}

\begin{frame}
  \frametitle{Utilité}

  \begin{itemize}
  \item décrire des transformations simples sur un dataset entier avec
    des mécanismes adaptés
  \item comprendre les possibilités et les limites de ces
    transformations simples.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Transformation linéaire}

  \begin{itemize}
  \item algèbre linéaire = on se limite aux sommes pondérées des
    inputs.
  \item bonne nouvelle : énorme partie des opérations en machine
    learning
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Description des données — échantillon}
  \begin{columns}
    \begin{column}{0.5\textwidth}
      Python :
      \begin{minted}{python}
data = (1, 3)
      \end{minted}
    \end{column}
    \begin{column}{0.5\textwidth}
      Algèbre linéaire : \\[.3cm]

      \( \mathbf{d} = \begin{bmatrix}
        1 \\
        3 \\
      \end{bmatrix}
      \)
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Description des données — dataset}
  \begin{columns}
    \begin{column}{0.5\textwidth}
      Python :
      \begin{minted}{python}
data = [(1, 3),
        (2, 2),
        (4, 2)]
      \end{minted}
    \end{column}
    \begin{column}{0.5\textwidth}
      Algèbre linéaire : \\[.3cm]

      \(
      \mathbf{D} = \begin{bmatrix}
        1 & 2 & 4 \\
        3 & 2 & 2 \\
      \end{bmatrix}
      \)
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Description des transformations linéaires}
  \begin{columns}
    \begin{column}{0.5\textwidth}
      Python :
\begin{minted}{python}
def weights(x, y):
    return x * 2 + y / 2
\end{minted}
    \end{column}
    \begin{column}{0.5\textwidth}
      Algèbre linéaire : \\[.3cm]

      \(
      \mathbf{w} = \begin{bmatrix}
        2 & \frac{1}{2} \\
      \end{bmatrix}
      \)
    \end{column}
  \end{columns}
  \vfill
  Transformation linéaire = somme pondérée.
\end{frame}

\begin{frame}
  \frametitle{Application d'une transformation linéaire à un exemple}

  \imgtw[.7]{linear-algebra}

  Bonne intuition à garder : Verser les colonnes (les exemples du
  dataset) dans les lignes (les opérations).
\end{frame}

\begin{frame}
  \frametitle{Bonne intuition à garder}

  \imgtw[.7]{linear-algebra-run}
  Bonne intuition à garder : Verser les colonnes (les exemples du
  dataset) dans les lignes (les opérations).
\end{frame}

\begin{frame}[fragile]
  \frametitle{Application d'une transformation linéaire à un exemple}
  \begin{columns}
    \begin{column}{0.5\textwidth}
      Python :
\begin{minted}{python}
data = (1, 3)

def weights(x, y):
    return 2 * x + y / 2

res = weights(*data)
\end{minted}
    \end{column}
    \begin{column}{0.5\textwidth}
      Algèbre linéaire : \\[.3cm]

      \(
      \begin{aligned}
        f & = \begin{bmatrix}
          2 & \frac{1}{2} \\
        \end{bmatrix}
        \begin{bmatrix}
          1 \\
          3 \\
        \end{bmatrix} \\
        & = 2 \times 1 + \frac{1}{2} \times 3
      \end{aligned}
      \)
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Application d'une transformation linéaire à un dataset}
  \begin{columns}
    \begin{column}{0.5\textwidth}
      Python :
\begin{minted}{python}
data = [(1, 3),
        (2, 2),
        (4, 2)]

def f(x, y):
    return x * 2 + y / 2

res = [f(x, y)
       for x, y
       in data]
\end{minted}
    \end{column}
    \begin{column}{0.5\textwidth}
      Algèbre linéaire : \\[.3cm]

      \(
      \begin{aligned}
        \text{res} &
        = \begin{bmatrix}
          {\only<2>{\color{blue}}
            \only<3>{\color{green}}
            \only<4>{\color{red}}2} &
          {\only<2>{\color{blue}}
            \only<3>{\color{green}}
            \only<4>{\color{red}}\frac{1}{2}} \\
        \end{bmatrix}
        \begin{bmatrix}
          {\only<2>{\color{blue}}1} & {\only<3>{\color{green}}2} & {\only<4>{\color{red}}4} \\
          {\only<2>{\color{blue}}3} & {\only<3>{\color{green}}2} & {\only<4>{\color{red}}2} \\
        \end{bmatrix} \\
        \onslide<2->{
          & = \begin{bmatrix}
            \onslide<2->{\only<2>{\color{blue}}3,5} &
            \onslide<3->{\only<3>{\color{green}}5} &
            \onslide<4->{\only<4>{\color{red}}9}
            \\
          \end{bmatrix} \\
        }
      \end{aligned}
      \)
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Application de plusieurs transformations linéaires à un dataset}
  \begin{columns}
    \begin{column}{0.5\textwidth}
      Python :
\begin{minted}{python}
data = [(1, 3), (2, 2),
        (4, 2)]

def f(x, y):
    return x * 2 + y / 2

def g(x, y):
    return x / 2 + y * 2

res = [[t(x, y) for x, y
                in data]
       for t in [f, g]]
\end{minted}
    \end{column}
    \begin{column}{0.5\textwidth}
      Algèbre linéaire : \\[.3cm]

      \(
      \begin{aligned}
        \text{res} & = \begin{bmatrix}
            {\only<2>{\color{blue}}
              \only<3>{\color{green}}
              \only<4>{\color{red}}2} &
            {\only<2>{\color{blue}}
              \only<3>{\color{green}}
              \only<4>{\color{red}}\frac{1}{2}} \\
            {\only<5>{\color{blue}}
              \only<6>{\color{green}}
              \only<7>{\color{red}}\frac{1}{2}} &
            {\only<5>{\color{blue}}
              \only<6>{\color{green}}
              \only<7>{\color{red}}2} \\
          \end{bmatrix}
          \begin{bmatrix}
            {\only<2,5>{\color{blue}}1} & {\only<3,6>{\color{green}}2} & {\only<4,7>{\color{red}}4} \\
            {\only<2,5>{\color{blue}}3} & {\only<3,6>{\color{green}}2} & {\only<4,7>{\color{red}}2} \\
          \end{bmatrix} \\
        \onslide<2->{
          & = \begin{bmatrix}
            \onslide<2->{\only<2>{\color{blue}}3,5} &
            \onslide<3->{\only<3>{\color{green}}5} &
            \onslide<4->{\only<4>{\color{red}}9}
            \\
            \onslide<5->{\only<5>{\color{blue}}6,5} &
            \onslide<6->{\only<6>{\color{green}}5} &
            \onslide<7->{\only<7>{\color{red}}6}
            \\
          \end{bmatrix} \\
        }
      \end{aligned}
      \)
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}
  \frametitle{Bonne intuition à garder}

  \imgtw{linear-algebra}
\end{frame}

\begin{frame}
  \frametitle{Bonne intuition à garder}

  \imgtw{linear-algebra-run}
\end{frame}

\begin{frame}
  \frametitle{Exercice}
  \[
    \begin{bmatrix}
      1 & 2 \\
      2 & 0 \\
    \end{bmatrix}
    \begin{bmatrix}
      0 & 0 & 1 & 1 \\
      0 & 1 & 1 & 0 \\
    \end{bmatrix}
    = \green{?}
  \]
\end{frame}

\begin{frame}
  \frametitle{Exemple de transformation}

  \imgtw[.5]{linear-transformation}
  \[
    \begin{aligned}
      \text{Bleu} & = \text{Transformation} \times \text{Rouge} \\
      & = \begin{bmatrix}
        1 & 2 \\
        2 & 0 \\
      \end{bmatrix}
      \begin{bmatrix}
        0 & 0 & 1 & 1 \\
        0 & 1 & 1 & 0 \\
      \end{bmatrix} \\
      & = \begin{bmatrix}
        0 & 2 & 3 & 1 \\
        0 & 0 & 2 & 2 \\
      \end{bmatrix}
    \end{aligned}
  \]
\end{frame}

\begin{frame}
  \frametitle{Formes}
  \begin{figure}
    \centering
    \begin{tabular}{ccc}
      \toprule
                               & Taille   \\
      \midrule
      $\mathbf{D}$             & $(m, n)$ \\
      $\mathbf{W}$             & $(o, m)$ \\
      $\mathbf{W}\mathbf{D}$ & $(o, n)$ \\
    \end{tabular}
  \end{figure}

  → dataset avec $m$ features et $n$ lignes transformé par $o$
  opérations donne dataset de $o$ features et $n$ lignes.
\end{frame}

\begin{frame}
  \frametitle{Déterminant}

  \imgtw[.5]{linear-transformation}

  Facteur de dilatation de la transformation (ratio aire bleue sur
  aire rouge).

  \pause

  \green{Quel est le déterminant de cette transformation ?} \pause $4$.
\end{frame}

\begin{frame}
  \frametitle{Vecteur propre}

  \imgtw[.5]{linear-transformation}

  Vecteur partant de l'origine qui conserve sa direction malgré la transformation.

  \green{Pouvez-vous en trouver un ?} \pause
  $\begin{bmatrix}1 \\0\end{bmatrix}$ par exemple.
\end{frame}

\begin{frame}
  \frametitle{Valeur propre}

  \imgtw[.5]{linear-transformation}

  Facteur par lequel un vecteur propre est redimensionné.

  \green{Quelle est la valeur propre de $\begin{bmatrix}1 \\ 0\end{bmatrix}$ ?} \pause 2.
\end{frame}

\section{Analyse}

\begin{frame}
  \frametitle{Utilité}

  Souvent besoin de minimiser une fonction en machine learning.

  \onslide<2->{(trouver le $x$ pour lequel $f$ est minimale)}

  \onslide<3->{\green{Vous souvenez-vous de comment l'on procède ?}}
\end{frame}

\begin{frame}
  \frametitle{Idée clef}

  Décider d'un $x$ de départ puis suivre la pente jusqu'au minimum.

  \onslide<2->{Pente = dérivée}

  \onslide<3->{→ Modifier itérativement $x$ par un pas vers l'opposé
    de la dérivée.}
\end{frame}

\begin{frame}
  \frametitle{Pente positive}

  \imgtw[.8]{positive-slope}

  Opposé de la pente = $-2$. Avec un pas de $0,1$, on passe de $1$ à $0,8$.
\end{frame}

\begin{frame}
  \frametitle{Pente négative}

  \imgtw[.8]{negative-slope}

  Opposé de la pente = $2$. Avec un pas de $0,1$, on passe de $-1$ à $-0,8$.
\end{frame}

\begin{frame}
  \frametitle{Extension à plusieurs dimensions}
  \begin{itemize}
  \item dérivée → gradient
  \item identique sinon !
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Exemple en 2 dimensions}

  \imgth[.8]{gradient-descent}
\end{frame}

\section{Probabilités}

\begin{frame}
  \frametitle{Utilité}
  \begin{itemize}
  \item quantifier l'incertain
  \item support pour les statistiques
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Probabilité}

  \begin{itemize}
  \item la probabilité de l'événement $X$ est notée $P(X)$
  \item $P(X) \in [0, 1]$
  \item $P(X)=0 \iff \text{X est impossible}$
  \item $P(X)=1 \iff \text{X est certain}$
  \item $P(\neg X) = 1 - P(X)$
  
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Loi de probabilité}

  Décrit le comportement aléatoire d'un phénomène dépendant du hasard.
  \begin{columns}
    \begin{column}{.5\textwidth}
      \begin{itemize}
      \item $\sum_uP(X = u) = 1$ en discret
      \item $\int P(X)dX = 1$ en continu
      \item loi uniforme
      \item loi normale/gaussienne
      \end{itemize}
    \end{column}
    \begin{column}{.5\textwidth}
      \imgtw{normal}
    \end{column}
  \end{columns}
\end{frame}

\section{Statistiques}

\begin{frame}
  \frametitle{Utilité}
  \begin{itemize}
  \item description et compréhension des données
  \item correction pour faciliter les traitements
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Types de variables}
  \imgtw{variables}
\end{frame}

\begin{frame}
  \frametitle{Hypothèse}

  Pré-requis pour les mesures statistiques qui suivent (et la plupart
  du machine learning) :
  \begin{itemize}
  \item les données \textbf{doivent être issues d'une même loi}
  \item chaque échantillon doit être \textbf{indépendant} des autres
  \item \red{pas évident en pratique !} \pause \green{Pourquoi ?}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Variance}
  Mesure la dispersion d'une série statistique (ou d'une variable) :

  \[
    V(X) = \mathbb{E}\left[(X - \mathbb{E}[X])^2\right]
  \]

  Pour la calculer :

  \[
    V(X) = \frac{1}{n}\sum_{i = 1}^{n}(x_i - \bar{x})^2
  \]
\end{frame}

\begin{frame}[fragile]
  \frametitle{Écart-type}
  Racine carrée de la variance

  \[
    \sigma(X) = \sqrt{V(X)}
  \]

\end{frame}

\begin{frame}[fragile]
  \frametitle{Écart-type — règle des 68, 95 et 99,7}

  Pour les lois normales :

  \imgth[.7]{68-95-99,7}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Quartile}

  Les quartiles ($Q_1$, $Q_2$ et $Q_3$) divisent les données en 4
  intervalles contenant le même nombre d'observations.

  Déclinable en quantile de taille arbitraire (décile, percentile).

  \pause \green{Que veut dire être dans le 95\ieme{} percentile ?}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Boxplot}

  \imgth[.9]{boxplot}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Covariance}
  Mesure la variabilité jointe de deux variables aléatoires :

  \[
    V(X) = \mathbb{E}\left[(X - \mathbb{E}[X])(X - \mathbb{E}[X])\right]
  \]
  \[
    \cov(X, Y) = \mathbb{E}\left[(X - \mathbb{E}[X])(Y - \mathbb{E}[Y])\right]
  \]

  Pour la calculer :

  \[
    \cov(X, Y) = \frac{1}{n}\sum_{i = 1}^{n}(x_i - \bar{x})(y_i - \bar{y})
  \]
\end{frame}

\begin{frame}[fragile]
  \frametitle{Corrélation}
  Covariance divisée par le produit des écart-types :

  \[
    \corr(X, Y) = \frac{\cov(X, Y)}{\sigma_X\sigma_Y}
  \]

  \green{Intérêt ?} \pause Pas d'unité.
\end{frame}

\begin{frame}
  \frametitle{Test de normalité}

  Pour tester (et corriger) la normalité d'une distribution, on
  utilise deux mesures :

  \begin{itemize}
  \item l'asymétrie (\textit{skew})
  \item le kurtosis
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Asymétrie}

  \imgtw{skew}

  \[
    \asym(X) = \mathbb{E} \left[\left(\frac{X - \bar{X}}{\sigma}\right)^3\right]
  \]
\end{frame}

\begin{frame}
  \frametitle{Kurtosis}

  \imgth[.5]{kurtosis}

  \[
    \kurt(X) = \mathbb{E} \left[\left(\frac{X - \mu}{\sigma}\right)^4\right]
  \]
\end{frame}

\begin{frame}
  \frametitle{Transformation de Box-Cox}
  Asymétrie et kurtosis peuvent se corriger avec la transformation de
  Box-Cox ou des transformations log.
\end{frame}

\section{Quizz}

\begin{frame}
  \frametitle{Algèbre linéaire}

  \begin{itemize}[<+->]
  \item comment décrit-on on enregistrement ?
  \item comment décrit-on une transformation linéaire ?
  \item quel est le sens de la multiplication de matrices dans le
    contexte dataset/opérations ?
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Analyse}

  \begin{itemize}[<+->]
  \item intuitivement, qu'est-ce que nous apprend une dérivée ?
  \item à quelle valeur de la dérivée d'une fonction atteint-on un
    minimum ?
  \item si la dérivée est négative, dans quel sens faut-il faire
    évoluer $x$ ?
  \item est-ce que la pas d'apprentissage impacte seulement les
    performances en temps de calcul ?
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Probabilités}

  \begin{itemize}[<+->]
  \item quelle est la forme d'une gaussienne ?
  \item à combien somme une loi discrète ?
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Statistiques}

  \begin{itemize}[<+->]
  \item quelle est l'hypothèse que l'on fait dans la plupart des
    approches de statistiques / machine learning ?
  \item que nous apprennent la variance et l'écart-type ?
  \item que nous apprennent la covariance et la corrélation ?
  \item comment peut-on savoir si une distribution est normale ?
  \end{itemize}
\end{frame}

\section{Conclusion}

\begin{frame}
  \frametitle{Conclusion}

  \begin{itemize}
  \item algèbre linéaire → raisonner sur des opérations simples
    et les décrire efficacement
  \item minimiser une fonction continue → dérivée
  \item décrire l'incertain → probabilités
  \item caractériser une série de données → statistiques
  \end{itemize}
\end{frame}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
