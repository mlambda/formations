\documentclass{formation}
\title{Machine Learning}
\subtitle{Réseaux de Neurones}

\begin{document}

\maketitle

\begin{frame}
  \frametitle{Réseaux de Neurones}
  \[
  \sigma \left(
  \begin{bmatrix}
    x_{1} & x_{2} & \dots & x_{d}
  \end{bmatrix}
  *
  \begin{bmatrix}
    ?_{11} & ?_{12} & \dots  & ?_{1n} \\
    ?_{21} & ?_{22} & \dots  & ?_{2n} \\
    \vdots & \vdots & \ddots & \vdots \\
    ?_{d1} & ?_{d2} & \dots  & ?_{dn}
  \end{bmatrix}
  \right )
  =
  \begin{bmatrix}
    o_{1} & o_{2} & \dots & o_{n}
  \end{bmatrix}
  \]
  \newline
  où :
  \newline
  X est une donnée en entrée de dimension \textbf{d},
  \newline
  $?_{??}$ sont les paramètres à trouver des \textbf{n} neurones de notre modèle.
  \newline
  $\sigma$ la fonction d'activation et
  \newline
  O la sortie du réseau
\end{frame}

\begin{frame}
  \frametitle{Réseaux de neurones}
  Calcul de l'erreur
  \[
  \frac{1}{2}\left (
  \begin{bmatrix}
    o_{1}^* & o_{2}^* & \dots & o_{n}^*
  \end{bmatrix}
  -
  \begin{bmatrix}
    o_{1} & o_{2} & \dots & o_{n}
  \end{bmatrix}
  \right )^2
  =
  \begin{bmatrix}
    e_{1} & e_{2} & \dots & e_{n}
  \end{bmatrix}
  \]
  \newline
  où :
  \newline
  $O^*$ représente la sortie attendue du réseau,
  \newline
  O la sortie du réseau et
  \newline
  E l'erreur commise par chaque neurone de sortie.
  \newline
\end{frame}

\begin{frame}
  \frametitle{Réseaux de neurones}
  Mise à jour des poids (\textbf{backward})
  \[
  \Delta w_i = -\gamma * \frac{\partial{E}}{\partial{w_i}}
  \]
  \newline
  où :
  \newline
  $\Delta w_i$ est l'update du paramètre $w_i$ et
  \newline
  $\gamma$ est un méta-paramètre du modèle (learning rate)
\end{frame}

\begin{frame}
  \frametitle{Réseaux de neurones : Fonction d'activation $\sigma$}
  \begin{itemize}
  \item Sigmoïde
  \item Tanh
  \item Softmax
  \item ReLU
  \item ...
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Réseaux de neurones : Fonction d'activation $\sigma$}
  Sigmoïde
  \imgth[0.6]{sigmoid}
  \[
  \frac{\partial{\phi(x)}}{\partial{x}}=\phi(x)*(1-\phi(x))
  \]
\end{frame}

\begin{frame}
  \frametitle{Réseaux de neurones : Fonction d'activation $\sigma$}
  \[
  \tanh(x)=\frac{1-\exp{-2*x}}{1+\exp{-2*x}}
  \]
  \imgth[0.5]{tanh}
  \[
  \frac{\partial{\tanh(x)}}{\partial{x}}=1-\tanh^2(x)
  \]
\end{frame}

\begin{frame}
  \frametitle{Réseaux de neurones : Fonction d'activation $\sigma$}
  \[
  \mathit{Softmax}(x_j)=\frac{\exp{x_j}}{\sum_{i=1}^n{\exp{x_i}}}
  \]
  donc :
  \[
  \sum_{j=1}^n{\mathit{Softmax}(x_j)}=1
  \]
  dérivée (ou jacobien car le softmax est une fonction de $\mathbb{R}^n\rightarrow\mathbb{R}^n$):
  \[
  D_jS_i = S_i(\delta_{ij}-S_j)
  \]
  où
  $D_jS_i$ est la dérivée partielle de la i-ième sortie par rapport à la j-ième entrée
  \newline
  $\delta_{ij}$ est le delta de Kronecker
\end{frame}

\begin{frame}
  \frametitle{Réseaux de neurones : Fonction d'activation $\sigma$}
  \imgth[0.8]{relu}
\end{frame}

\begin{frame}
  \frametitle{Réseau de Neurones}
  
  \imgtw[0.8]{neural-network-small}
\end{frame}

\begin{frame}
  \frametitle{Réseau de Neurones}
  \imgtw[0.8]{neural-network}
\end{frame}

\begin{frame}
  \frametitle{Réseau de Neurones}
  \imgtw[0.8]{deep-net}
\end{frame}

\begin{frame}
  \frametitle{Réseau de Neurones}
  \imgtw[0.8]{deep-auto-encoder}
\end{frame}

\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
