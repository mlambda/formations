\documentclass{formation}
\title{Machine Learning}
\subtitle{Apprentissage de modèles}

\begin{document}

\maketitle

\begin{frame}
  \frametitle{Apprentissage de modèles}
  Entrainement supervisé d'un modèle — overfit
  \imgtw{fit}
  \red{Problème : trop minimiser la perte n'est pas bon !}
\end{frame}

\begin{frame}
  \frametitle{Apprentissage de modèles}
  \imgtw[.6]{learning-curve}
  → Minimiser la perte sur un ensemble de validation
\end{frame}

\begin{frame}
  \frametitle{Apprentissage de modèles}
  Séparation des données :
  \begin{itemize}
  \item ensemble d'entrainement
  \item ensemble de validation pour mesurer la généralisation
  \item ensemble de test (pour éviter le biais statistique)
  \end{itemize}
  → Split 60/20/20 habituel.
\end{frame}

\begin{frame}
  \frametitle{Apprentissage de modèles}
  Idéalement : Cross Validation
  Pour \og perdre\fg{} moins de données et mieux tester la
  généralisation, cross-validation :
  \imgtw[.9]{cv}
  Ici, 4-fold cross-validation.
\end{frame}

\begin{frame}
  \frametitle{Apprentissage de modèles}
  1 - initialisation aléatoire du modèle
  \newline
  2 - Tant que(critère arret == 0)
  \begin{itemize}
  \item Selection aléatoire d'un \textbf{batch} de données
  \item \textbf{Forward} : Passe avant du \textbf{batch} dans le modèle
  \item Calcul de l'erreur par rapport aux sorties attendues
  \item \textbf{Backward} : Rétropropagation du gradient de l'erreur en fonction des paramèrtres dans le modèle (mise à jour du modèle)
  \item Calcul critère arret
  \end{itemize}
  3 - Calcul de l'erreur sur un échantillon de données  \textbf{qui n'ont JAMAIS été vues par le modèle pendant l'apprentissage !}
\end{frame}

\begin{frame}
  \frametitle{Apprentissage de modèles non-supervisés}
  Étant donné des exemples $x_i$, trouver un modèle $h$ :
  \begin{itemize}[<+->]
  \item but moins défini qu'en supervisé :
    \begin{itemize}[<+->]
    \item clustering : $h(x_i) = \hat{y}_i = \text{cluster de }x_i$
    \item détection d'anomalies : $y_i = 1 \text{ si anomalie, }0
      \text{ sinon}$
    \item recommandations : $y_i = \text{liste d'items }x_{k\neq i}$
    \item réduction de dimensionnalité : $y_i = x_i \text{ projeté dans
      moins de features}$
    \end{itemize}
  \item on définit quand même une perte (loss) \\[.2cm]
    \onslide<+->{Densité intra- et inter-clusters en clustering}\\[.2cm]
    \onslide<+->{Ou utiliser des données supervisées...}
    \onslide<+->{Pourcentage d'info perdue pour la réduction de dimentionnalité}\\[.2cm]
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Évaluation — outils — précision, rappel}
  En classification :
  \begin{description}
  \item[Précision]
    \[
    \frac{\text{vrais positifs}}{\text{vrais positifs + faux positifs}}
    \]
  \item[Rappel]
    \[
    \frac{\text{vrais positifs}}{\text{vrais positifs + faux négatifs}}
  \]
  \item[F-mesure] moyenne harmonique entre précision et rappel (aussi
    appelée F1 score)
  \end{description}
\end{frame}

\begin{frame}
  \frametitle{Évaluation — outils — précision, rappel}
  \imgth[0.8]{precisionrecall}
\end{frame}

\begin{frame}
  \frametitle{Outils — courbe ROC}
  \imgtw[.6]{roc}
\end{frame}

\begin{frame}
  \frametitle{Outils — courbe ROC}
  \imgtw[.6]{roc-space}
\end{frame}

\begin{frame}
  \frametitle{Évaluation — outils — matrice de confusion}
  \imgtw[.6]{confusion-matrix}
\end{frame}

\begin{frame}
  \frametitle{Reproductibilité}
  \begin{itemize}
  \item extrêmement importante pour compléter les analyses après les retours business
  \item ensemble de bonnes pratiques
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Reproductibilité — bonnes pratiques}
  \begin{itemize}[<+->]
  \item garder une trace exacte du préprocessing
  \item de préférence utiliser des notebooks
  \item faire attention au random (utiliser des seeds)
  \item définir les datasets utilisés, dates comprises
  \item garder une trace de l'environnement
  \end{itemize}
\end{frame}

\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
