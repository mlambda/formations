\documentclass{formation}
\title{Machine Learning - Avancé}
\subtitle{QCM}

\begin{document}

\maketitle

\begin{frame}
  \frametitle{Question 1}
  Peut-on utiliser un réseau de neurone pour faire de la régression numérique ?
  \begin{enumerate}
  \item Oui
  \item Non
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{Question 2}
  Soit des données en dimension 10 et un sortie en dimension 5. On veut apprendre un réseau de neurones sans couche cachée. Combien de paramètres devont nous apprendre par descente de gradient ?
  \begin{enumerate}
  \item 15
  \item 42
  \item 50
  \item 55
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{Question 3}
  On veut maintenant utiliser un réseau avec 3 couches cachées de taille 20. Combien a-t-on maintenant de paramètres ?
\end{frame}

\begin{frame}
  \frametitle{Question 4}
  Dans un réseau profond et non-récurrent, la fonction d'activation adaptée à une couche cachée dans un problème de classification est la fonction :
  \begin{enumerate}
  \item sigmoïde
  \item tangente hyperbolique
  \item Rectified Linear Unit
  \item Softmax
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{Question 5}
  Une couche de dropout possède des paramètres appris durant l'optimisation :
  \begin{enumerate}
  \item Vrai
  \item Faux
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{Question 6}
  Un bon moyen de régulariser les réseaux récurrents consiste à utiliser des fonction d'activations ReLu :
  \begin{enumerate}
  \item Vrai
  \item Faux
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{Question 7}
  Quel est l'intêret principal des gates dans le LSTM par rapport aux RNN ?
  \begin{enumerate}
  \item Une plus grande versatilité
  \item Meilleure robustesse au données bruitées
  \item Les gradients ne disparaissent plus dans les séquences (raisonnablement) longues
  \item Une meilleure ``mémoire'' du modèle dans les séquences longues
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{Question 8}
  On a besoin de données supervisées pour apprendre des words embeddings
  \begin{enumerate}
  \item Vrai
  \item Faux
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{Question 9}
  Les réseaux de convolutions sont directement inspirés de ce qui se passe dans le cortex visuel :
  \begin{enumerate}
  \item Vrai
  \item Faux
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{Question 10}
  On effectue un convolution 5x5 sur une image de dimension 160x90. Quelle est la taille de l'image obtenue :
  \begin{enumerate}
  \item 190x60
  \item 160x90
  \item 158x88
  \item 156x86
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{Question 11}
  Quelle affirmation sur les auto-encodeurs est fausse ?
  \begin{enumerate}
  \item Ça permet la compression d'information
  \item Leur apprentissage est non-supervisé
  \item La taille du code doit toujours  être inférieure à la taille de l'entrée
  \end{enumerate}
\end{frame}

\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
