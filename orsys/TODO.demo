1) INTELLIGENCE ARTIFICIELLE
domaine.
- Intelligence collective : agréger une connaissance partagée par de nombreux agents virtuels.
- Algorithmes génétiques : faire évoluer une population d'agents virtuels par sélection.

5) DATASET
- Formater une donnée : décider d'un format d'entrée et de sortie, faire le lien avec la qualification du problème.
Echanges
- Définition d'un Dataset et sa différence avec un BDD usuel

2) RESEAUX DE NEURONES
- Data Augmentation : comment équilibrer un dataset ?
- Initialisations et régularisations d'un réseau de neurones : L1/L2 Regularization, Batch Normalization.
Démonstration
- Approximation d'une fonction et d'une distribution par un réseau de neurones.
- Présentation d'un algorithme de classification et de ses limites.

4) MACHINE LEARNING
Etude de cas
- Qualification d'une problématique pouvant être traitée avec l'IA

3) APPLICATIONS MACHINE LEARNING
Machine Learning.
textuelles.
Démonstration
- Classification d'images médicales. Prévision des images suivant une séquence vidéo. Contrôle de simulations numériques.

4) Convolutional Neural Networks (CNN)
- Fonctionnement fondamental d'un CNN : couche convolutionnelle, utilisation d'un kernel, padding et stride...
- Architectures CNN ayant porté l'état de l'art en classification d'images : LeNet, VGG Networks, Network in Network...
- CNNs pour la génération : super-résolution, segmentation pixel à pixel.
- Principales stratégies d'augmentation des Feature Maps pour la génération d'une image.
Etude de cas
- Innovations apportées par chaque architecture CNN et leurs applications plus globales (convolution 1x1 ou connexions résiduelles).

5) Recurrent Neural Networks (RNN)
Démonstration
- Différents états et évolutions apportées par les architectures Gated Recurrent Units et Long Short Term
Memory.

6) Modèles générationnels : VAE et GAN
- Présentation des modèles générationnels Variational AutoEncoder (VAE) et Generative Adversarial Networks (GAN).
- Variational AutoEncoder : modèle générationnel et approximation de la distribution d'une donnée.
- Définition et utilisation de l'espace latent. Reparameterization trick.
- Convergence d'un GAN et difficultés rencontrées.
- Convergence améliorée : Wasserstein GAN, BeGAN. Earth Moving Distance.
Démonstration
- Applications des modèles générationnels et utilisation de l'espace latent.

7) Deep Reinforcement Learning
- Optimisations de la politique d'apprentissage. On-policy et off-policy. Actor critic architecture. A3C.
Démonstration
- Contrôle d'un agent dans un environnement défini par un état et des actions possibles.

6) METHODOLOGIE
- Choix d'une direction de recherche, localisation de publications ou de projets similaires existants.
- Conservation d'un banc de comparaison transversal.
- Arriver à une solution optimale.
Etude de cas
- Grouper et balancer un ensemble de solutions pour obtenir une solution optimale.

7) OUTILS
- Quels outils existe-t-il aujourd'hui ?
- Quels outils pour la recherche et quels outils pour l'industrie ?
- De Keras/Lasagne à Caffe en passant par Torch, Theano, Tensorflow ou Apache Spark ou Hadoop.
- Industrialiser un réseau de neurones par un encadrement strict de son processus et un monitoring continu.
- Mise en place de réapprentissages successifs pour conserver un réseau à jour et optimal.
- Former des utilisateurs à la compréhension du réseau.
- Outils de gestion de donnée : Apache Spark, Apache Hadoop.
- Outils Machine Learning usuel : Numpy, Scipy, Sci-kit.
- Frameworks DL haut niveau : PyTorch, Keras, Lasagne.
- Frameworks DL bas niveau : Theano, Torch, Caffe, Tensorflow.
Démonstration
- Applications et limites des outils présentés (Apache Spark, Apache Hadoop, Numpy, Scipy, Sci-kit, PyTorch, Keras, Lasagne, Theano, Torch, Caffe, Tensorflow)
- Mise en place de réapprentissages successifs.
