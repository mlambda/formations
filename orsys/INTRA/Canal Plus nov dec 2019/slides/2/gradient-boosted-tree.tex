\section{Gradient boosted trees}

\begin{frame}
  \frametitle{Introduction}

  Arbres qui s'améliorent successivement.
  \begin{figure}
    \centering \imgth[.4]{tree-training-modes}
    \scriptsize{\href{https://quantdare.com/what-is-the-difference-between-bagging-and-boosting/}%
        {quantdare.com/what-is-the-difference-between-bagging-and-boosting/}}
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Approximation 1}

  \begin{enumerate}
  \item partir d'un arbre grossier
  \item entraîner un nouvel arbre sur les résiduels du premier
  \item concaténer le nouvel arbre au premier
  \item \texttt{goto 2.}
  \end{enumerate}

  \green{Quel est l'effet d'entraîner sur les résiduels ?}
\end{frame}

\begin{frame}
  \frametitle{Approximation 2}

  \begin{enumerate}
  \item partir d'un arbre grossier
  \item entraîner un nouvel arbre sur les \textbf{pseudo-résiduels} du
    premier
  \item concaténer le nouvel arbre au premier
  \item \texttt{goto 2.}
  \end{enumerate}

\end{frame}

\begin{frame}
  \frametitle{Pseudo-résiduels}
  \begin{itemize}
  \item chosir une fonction de coût
  \item calculer les pas de descente de gradient étant donné les
    couples $(\hat{y}_i, y_i)$
  \item \textbf{se servir de ces valeurs comme de résiduels}
  \end{itemize}

  → Intérêt : pouvoir utiliser n'importe quel loss dérivable.
\end{frame}

\begin{frame}
  \frametitle{Approximation 3}
  \begin{enumerate}
  \item partir d'un arbre grossier
  \item entraîner un nouvel arbre sur les pseudo-résiduels du premier
  \item \textbf{calculer un multiplicateur pour que l'arbre produit
      minimise le coût}
  \item concaténer le nouvel arbre au premier
  \item \texttt{goto 2.}
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{Véritable modèle}
  \begin{enumerate}
  \item partir d'un arbre grossier
  \item entraîner un nouvel arbre sur les pseudo-résiduels du premier
  \item calculer un multiplicateur pour que l'arbre produit minimise
    le coût
  \item \textbf{appliquer un learning rate}
  \item concaténer le nouvel arbre au premier
  \item \texttt{goto 2.}
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{Idée à retenir}

  \begin{itemize}
  \item séquence d'arbres qui s'entrainent à corriger les erreurs de
    l'arbre d'avant
  \item modélisation de la correction de l'erreur par un pas de
    descente de gradient pour plus de flexibilité.
  \end{itemize}
   
\end{frame}

\begin{frame}
  \frametitle{Extensions}

  \begin{itemize}
  \item row sampling
  \item column sampling
  \item tree structure cost
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Avantages}

  \begin{itemize}
  \item modèle extrêmement performant et versatile
  \item entrainement parallélisable
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Désavantages}

  Sujet à l'overfit si pas assez régularisé (tree structure cost) et
  randomisé (row \& column sampling)
\end{frame}
