\documentclass{formation}
\title{Deep Learning par la Pratique}
\subtitle{Cours 2 : Keras et Convolutional Neural Networks}

\begin{document}

\maketitle

\begin{frame}
  \frametitle{Rappels}
  \imgtw[0.8]{deep-net}  
\end{frame}

\begin{frame}
  \frametitle{Rappels}
  \imgtw[0.6]{gradient}
\end{frame}

\begin{frame}
  \frametitle{Rappels}
  Exploding Gradient $\Rightarrow$ Gradient clipping
  \imgtw[0.9]{gradient-clipping}
\end{frame}

\begin{frame}
  \frametitle{Rappels}
  Vanishing Gradient $\Rightarrow$ utilisation de ReLu plutôt que les sigmoïdes
  \imgtw[0.9]{relu}
\end{frame}

\begin{frame}
  \frametitle{Rappels}
  Optimisateur plus rapide que SGD
  \begin{itemize}
  \item Root Mean Square Propagation (RMSProp)
  \item Adapative Gradient Algorithm (AdaGrad)
  \item Adaptative Moment Estimation (Adam)    $\Leftarrow$
  \item ... AdaBound (2019) ?
 \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Rappels}
  \imgtw[0.8]{dropout-2}
\end{frame}

\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
