\documentclass{formation}
\title{Big Data Analytics}
\subtitle{Vecteurs Textuels}

\begin{document}

\maketitle

\begin{frame}
  \frametitle{Vecteurs Textuels}
  Représentation TF-IDF. \\
  Un document = un vecteur de la taille d'un dictionnaire. \\
  \begin{center}
    $\boxed{w_{i,j} = tf_{i,j}*\log{\frac{N}{df_i}}}$
  \end{center}
  où : \\
  $tf_{i,j} = $nombre d'occurence du mot i dans le document j, \\
  $df_i = $nombre de documents contenant le mot i, \\
  $N = $nombre total de documents \\
  \newline
  $\Rightarrow$ produict scalaire, SVM, arbres, réseaux de neurones, ...
\end{frame}

\begin{frame}
  \frametitle{Vecteurs Textuels}
  Loi de Zipf, justifiant l'utilisation du terme $df_i$
  \imgtw[0.9]{zipf-law}
\end{frame}

\begin{frame}
  \frametitle{Vecteurs Textuels}
  Utilisation de N-gram de mots ou de caractères. \\
  \newline
  Ex : ``Le chien mange de la viande''\\
  Dictionnaire 2-gram de mots : \\
  \newline
  \begin{itemize}
  \item {[le-chien,chien-mange,mange-de,de-la,la-viande]}
  \end{itemize}
  Dictionnaire 3-gram de caractères :
  \begin{itemize}
  \item {[le\_,e\_c,\_ch,chi,hie,ien,en\_,n\_m,\_ma,man,...]}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Vecteurs Textuels}
  Latent Semantic Analysis : \\
  \newline
  $\approx$ ACP sur la matrice des documents, de telle sorte qu'on obtient des relations entre les mots. \\
  Les directions principales de projection nous donne des ``concepts'' génraux liés au langage. \\
  \newline
  Par exemple : un axe correspond au ``stop words'', un autre au champ lexical du sport, un autre à l'économie, etc...  
\end{frame}

\begin{frame}
  \frametitle{Traitement du langage : word embeddings} 
  mot = indice dans un dictionnaire (dimension > 30000)
  \newline
  mot = vecteur ``sémantique'' (dimension < 1000) 
  \begin{itemize}
  \item word2vec
  \item CBOW/Skip-Gram
  \item GloVe
  \item Thought vector (pour des phrases ou même des documents entiers)
  \item ...
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Traitement du langage : word embeddings} 
  \imgtw{word2vec}
\end{frame}

\begin{frame}
  \frametitle{Traitement du langage : word embeddings} 
  \imgtw[0.6]{word2vec-analogy}
  \begin{center}
    \blue{\underline{\href{https://projector.tensorflow.org/}{Visualisation de l'espace word2vec}}} \\
    \blue{\underline{\href{http://vectors.nlpl.eu/repository/}{Word Embeddings à télécharger}}}
  \end{center}
\end{frame}

\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
