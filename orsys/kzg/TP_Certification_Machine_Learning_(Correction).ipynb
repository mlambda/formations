{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TP - Certification Machine Learning (Correction)",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXeUsBVrtdHg",
        "colab_type": "text"
      },
      "source": [
        "#ANALYSE DE SENTIMENTS\n",
        "\n",
        "Pour cette certification vous aurez à étudier un corpus de données extrait du **Large Movie Review Dataset v1.0**. Ce corpus est composé de critiques en anglais de films tirées de **IMDb** et préprocessées :\n",
        "- Le Corpus est déjà séparé en `train/` et `test/`. \n",
        "- Les commentaires positifs sont dans un dossier `pos/`, les négatifs dans `neg/`.\n",
        "- Chaque critique est dans un fichier texte distinct, sur une ligne, sans retour chariot.\n",
        "\n",
        "L'objectif de ce TP de certification est de produire des modèles prédictifs capables de donner la polarité d'une critique de film, ainsi que de commenter les résultats obtenus.\n",
        "\n",
        "Dans un premier temps, il vous sera demandé de **collecter les données**, dans un second d'utiliser une **représentation en sac de mots pondérés par TF-IDF** ainsi qu'un algorithme de machine learning adapté. Enfin, vous devrez utiliser un **réseaux profond récurrent** avec et sans **word embeddings** déjà appris.\n",
        "\n",
        "Pour des raisons évidentes de **temps limité**, ce TP n'a pas pour but de vous faire chercher le meilleur modèle avec les meilleures performances. Les **méta-paramètres** d'apprentissage **vous seront toujours fournis** dans le but de rendre les executions raisonnablement courtes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Iu2RmV9yqfN",
        "colab_type": "text"
      },
      "source": [
        "## Récupération des données\n",
        "\n",
        "Les données se trouvent dans le repository github suivant : https://github.com/nzmonzmp/sentiment-aclimdb.git"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3D6weN7WpaEt",
        "colab_type": "code",
        "outputId": "4c9ea883-8f54-4b80-da75-097b6533f130",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "! git clone https://github.com/nzmonzmp/sentiment-aclimdb.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'sentiment-aclimdb'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 78582 (delta 0), reused 3 (delta 0), pack-reused 78579\u001b[K\n",
            "Receiving objects: 100% (78582/78582), 36.33 MiB | 10.23 MiB/s, done.\n",
            "Resolving deltas: 100% (28972/28972), done.\n",
            "Checking out files: 100% (50002/50002), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7_GbZyLnjDU",
        "colab_type": "text"
      },
      "source": [
        "## Ressource extérieure \n",
        "\n",
        "Cette ressource est nécéssaire pour la Dernière partie de cet examen. Son **téléchargement étant un peu long (~10mn)**, si vous voulez commencer par lancer cette cellule pendant que vous parcourez l'intégralité de l'examen, vous gagnerez de précieuses minutes pour plus tard."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkGgZEwmc5hx",
        "colab_type": "code",
        "outputId": "2d7c0027-0194-4443-b765-706a9ef02dc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "source": [
        "! wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "! unzip glove.6B.zip\n",
        "\n",
        "def glove_filename(embedding_dim):\n",
        "  if embedding_dim in {50, 100, 200, 300}:\n",
        "    return \"glove.6B.{}d.txt\".format(embedding_dim)\n",
        "  else:\n",
        "    raise ValueError(\"EMBEDDING_DIM must be in {50, 100, 200, 300}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-25 07:16:58--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-05-25 07:16:58--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-05-25 07:16:59--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  1.96MB/s    in 6m 29s  \n",
            "\n",
            "2020-05-25 07:23:28 (2.11 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5xelpJ-PW8Z",
        "colab_type": "text"
      },
      "source": [
        "#Partie 1 : COLLECTE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcisU1jIzfnx",
        "colab_type": "text"
      },
      "source": [
        "Après un rapide coup d'oeil aux données dans le dossier `sentiment-aclimdb/`, **implémentez une fonction** qui va vous permettre d'instancier les variables suivantes :\n",
        "- `X_train_raw` et `X_test_raw`, des listes contenant une critique par indice\n",
        "- `Y_train` et `Y_test`, des listes contenant la cible sous forme d'entier (`0` : négatif , `1` : positif)\n",
        "- `X_train_names` et `X_test_names`, les listes contenant les chemins des fichiers correspondant à chaque exemple\n",
        "\n",
        "Instanciez-les."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkPZmpqNuJNV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pathlib import Path\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "def get_corpus(directory: Path):\n",
        "    reviews = []\n",
        "    labels = []\n",
        "    file_names = []\n",
        "    for p in ['neg','pos']:\n",
        "      curr_dir = directory / p\n",
        "      target = 1 if p == \"pos\" else 0\n",
        "      for file_path in curr_dir.iterdir():\n",
        "        text = file_path.read_text(encoding=\"utf8\")\n",
        "        reviews.append(text)\n",
        "        labels.append(target)\n",
        "        file_names.append(str(file_path))\n",
        "\n",
        "    return shuffle(reviews, labels, file_names, random_state=817328462)\n",
        "        \n",
        "X_train_raw, Y_train, X_train_names = get_corpus(Path(\"sentiment-aclimdb\") / \"train\")\n",
        "X_test_raw , Y_test,  X_test_names  = get_corpus(Path(\"sentiment-aclimdb\") / \"test\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dFUEidiPeCi",
        "colab_type": "text"
      },
      "source": [
        "#Partie 2 : TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igRflZwhgl1-",
        "colab_type": "text"
      },
      "source": [
        "## Représentation\n",
        "\n",
        "Transformez `X_train_raw` et `X_test_raw` à l'aide de [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) de la librairie **scikit-learn**.\n",
        "\n",
        "On ne conservera que **1000 mots** dans le dictionnaire."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rgNEFzic_Lj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=1000)\n",
        "vectorizer.fit(X_train_raw)\n",
        "X_train = vectorizer.transform(X_train_raw)\n",
        "X_test = vectorizer.transform(X_test_raw)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0_2OFdZ4zlc",
        "colab_type": "text"
      },
      "source": [
        "## Modélisation\n",
        "\n",
        "Choisissez un modèle **adapté** à la classification de représentations TF-IDF.\n",
        "\n",
        "Apprenez-le et calculez son score d'accuracy en apprentissage comme en test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFmdlGZLAjJ3",
        "colab_type": "code",
        "outputId": "137027a0-69b6-4c9a-a4a5-b10e596aa0d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "from sklearn import svm\n",
        "clf = svm.LinearSVC()\n",
        "clf.fit(X_train,Y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
              "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
              "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
              "          verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrGa9K1Nzp40",
        "colab_type": "code",
        "outputId": "0111a54a-f833-49ae-9587-021621ba1769",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(\"Accuracy sur train : {:.3f}\".format(clf.score(X_train,Y_train)))\n",
        "print(\"Accuracy sur test  : {:.3f}\".format(clf.score(X_test,Y_test)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy sur train : 0.881\n",
            "Accuracy sur test  : 0.864\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ei6YErPZ6gl7",
        "colab_type": "text"
      },
      "source": [
        "## Analyse d'erreur\n",
        "\n",
        "Affichez les 5 critiques positives les plus mal classées de la base de test.\n",
        "\n",
        "Idem pour les négatives les mieux classées."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kS3E4H65GPpz",
        "colab_type": "code",
        "outputId": "d0013861-d020-446f-c6b7-f32d11262f46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "Y_pred = clf.decision_function(X_test)\n",
        "\n",
        "ind_pos = np.where(Y_test == 1)[0]\n",
        "ind_neg = np.where(Y_test == 0)[0]\n",
        "\n",
        "worst_pos = ind_pos[np.argsort(Y_pred[ind_pos])[:5]]\n",
        "best_neg = ind_neg[np.argsort(-Y_pred[ind_neg])[:5]]\n",
        "\n",
        "print(\"Critiques prédites comme étant très négative mais avec un label cible positif :\")\n",
        "for i in worst_pos:\n",
        "  print(\"  {:.3f} : {}\".format(Y_pred[i], X_test_raw[i]))\n",
        "print()\n",
        "print(\"--------------------------------------------------------------------------------\")\n",
        "print()\n",
        "print(\"Critiques prédites comme étant très positives mais avec un label cible négatif :\")\n",
        "for i in best_neg:\n",
        "  print(\"  {:.3f} : {}\".format(Y_pred[i], X_test_raw[i]))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Critiques prédites comme étant très négative mais avec un label cible positif :\n",
            "  -2.208 : It's not Citizen Kane, but it does deliver. Cleavage, and lots of it.Badly acted and directed, poorly scripted. Who cares? I didn't watch it for the dialog.\n",
            "  -1.908 : David Morse and Andre Braugher are very talented actors, which is why I'm trying so hard to support this program. Unfortunately, an irrational plot, and very poor writing is making it difficult for me. I'm hoping that the show gets a serious overhaul, or that the actors find new projects that are worthy of them.\n",
            "  -1.881 : **SPOILERS AHEAD**It is really unfortunate that a movie so well produced turns out to besuch a disappointment. I thought this was full of (silly) clichés andthat it basically tried to hard. To the (American) guys out there: how many of you spend yourtime jumping on your girlfriend's bed and making monkeysounds? To the (married) girls: how many of you have suddenlygone from prudes to nymphos overnight--but not with yourhusband? To the French: would you really ask about someonebeing \"à la fac\" when you know they don't speak French? Wouldn'tyou use a more common word like \"université\"? I lived in France for a while and I sort of do know and understandEurope (and I love it), but my (German) roommate and I found thispretty insulting overall. It looked like a movie funded by theEuropean Parliament, and it tried too hard basically. It had allsorts of differences that it tried to tie together (not a bad thing initself) but the result is at best awkward, but in fact ridiculous--toomany clashes that wouldn't really happen. Then the end of themovie--the last 10 minutes--ruined all the rest. Why doesn't Xaviertalk to the Erasmus students he meets back in Paris? Why doeshe just walk off? Why does he just run away from his job, is that\"freedom\"? And in the end, is the new Europe supposed to rest ona bunch of people who smoke up and shag all day? Is this whatit's made up of? Besides, the acting was pretty horrible. I can't believe JudithGodrèche's role and acting. Why was she made to look likeEmanuelle Béart so much? At first I thought Xavier was OK butwith retrospect I think he was pretty bad. And that's all really too bad, because technically (opening credits,scenes when he's asking what papers he needs) it was reallygood (except for sound editing around the British siblings), and thesoundtrack was great too. So the form was good, but the contentpretty horrible.\n",
            "  -1.848 : My sisters and my cousin(female) forced me to see this chick movie. Its not the kind of movie I would prefer to see, but it really wasn't that bad. I wouldn't want to see this movie, but after watching it I couldn't say it was bad, just not my kind of film. It was very accurate in acting out what woman really talk about and do in certain situations. As a guy, I wasn't TOO amused by the jokes, but man the women sure were. They related to the movie and yeah it was funny to see themselves on the big screen. \"OH EM GEE I DO THAT TO HAHA\" I've never seen the original, nor would I care to. If you are a girl, this is a must see, but maybe girls would say this movie sucked, probably. I don't see a 13 year old going, \"Wow I remember going threw that when I was 30.\"No man would have this in there collection, but I can't say it was horrible, so meh.\n",
            "  -1.811 : This was Laurel and Hardy's last silent film for Roach Studios. However, since the public had a real thirst for \"talkies\", this same short was re-made by the team just a few years later with only a few small plot changes. LAUGHING GRAVY was essentially the same plot except that Stan and Ollie were trying to hide a cute puppy from their grouchy landlord--not a goat like in ANGORA LOVE. This whole goat angle is the worst part of the film. While you could understand the boys wanting to keep a cute little dog (after all, it is snowy outside), why exactly they bring a goat home is just contrived and pointless. According to the plot, the goat followed them home and so they got tired of shooing it away and kept it. Huh?! This just doesn't make any sense--if it had been a giraffe or a cow, would they have done the same thing?! Apart from being an unconvincing plot, the movie itself is pure Laurel and Hardy, with a familiar plot and familiar roles for the comedians. This film features quite a few laughs, but unfortunately isn't one of their better films to wrap up their silent careers. This aspect of their careers just seems to have ended with a whimper.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Critiques prédites comme étant très positives mais avec un label cible négatif :\n",
            "  2.982 : Mickey Rourke hunts Diane Lane in Elmore Leonard's Killshot It is not like Mickey Rourke ever really disappeared. He has had a steady string of appearances before he burst back on the scene. He was memorable in: Domino, Sin City, Man on Fire, Once Upon a Time in Mexico, and Get Carter. But in his powerful dramatic performance in The Wrestler (2008), we see a full blown presentation of the character only hinted at in Get Carter. Whenever we get to know him, Rourke remains a cool, but sleazy, muscle bound slim ball.This is an Elmore Leonard story, and production. Leonard wrote such notable movies as taunt western thriller 3:10 to Yuma, Be Cool, Jackie Brown, Get Shorty, 52 Pick-Up, and Joe Kidd. This means that we get tough guys, some good, some not so good.It also means we get tight, realistic plots with characters doing what is best for them in each situation, weaving complications into violent conclusions. Killshot is no different. Tough, slim ball killer Rourke stalks unhappily married witness Lane. Think History of Violence meets No Country for Old Men. It is not as intense, bloody or gory as those two, but it is almost as good. If you like those two, including David Croneberg's equally wonderful Eastern Promises, you will like Killshot also.Director John Madden has not done a lot of movies. His last few were enjoyable, if not successful: Proof, Captain Corelli's Mandolin and Shakespeare in Love.Diana Lane hasn't had a powerful movie role since she and Richard Gere gave incredible performances in Unfaithful. Lately she is charming and appealing in romantic stories such as Nights in Rodanthe, Must Love Dogs, and Under the Tuscan Sun. Here she is right on mark, balancing her sexy appeal with reserved tension.This is a small part for Rosario Dawson. Yet Dawson does a good job with it. You see a lot more of Lane, including an underwear scene to rival Sigourney Weaver in Aliens and Nicole Kidman in Eyes Wide Shut.While you are in the crime drama section, also pick up Kiss, Kiss, Bang, Bang, and Gone Baby Gone, and Before the Devil Knows Your Dead. The last has wonderful performances by Phillip Seymour Hoffman, Ethan Hawke, Marisa Tomei and Albert Finney.Killshot flopped at the box office. More is our luck. It is certainly worth a 3-4 dollar rental, if you like this genre. 6/20/2009\n",
            "  2.149 : I enjoyed the beautiful scenery in this movie the first time I saw it when I was 9 . Dunderklumpen is kind of cute for kiddies in a corny way. It reminded me of HRPUFFINSTUFF on sat mornings, Its Swedish backdrops make it easy on the eyes . Don't expect older kids to be interested as the live action/animation is way behind the times and most older kids will get bored.This is definitely an under 10 age set movie and a nice bit of memories for those of us who were little kids in 1974.\n",
            "  2.114 : This film has the language, the style and the attitude down ... plus greats rides from Occy (a world champ) and the great Jerry Lopez. John Philbin as Turtle has the surf pidgin down, and the surfing scenes are still the best ever. A true classic that can be seen many times. Nia Peeples is a babe, and Laird Hamilton shows the early stuff that has made him the world's number one extreme surfer.\n",
            "  1.988 : First off let me say, If you haven't enjoyed a Van Damme movie since bloodsport, you probably will not like this movie. Most of these movies may not have the best plots or best actors but I enjoy these kinds of movies for what they are. This movie is much better than any of the movies the other action guys (Segal and Dolph) have thought about putting out the past few years. Van Damme is good in the movie, the movie is only worth watching to Van Damme fans. It is not as good as Wake of Death (which i highly recommend to anyone of likes Van Damme) or In hell but, in my opinion it's worth watching. It has the same type of feel to it as Nowhere to Run. Good fun stuff!\n",
            "  1.910 : This movie was pure genius. John Waters is brilliant. It is hilarious and I am not sick of it even after seeing it about 20 times since I bought it a few months ago. The acting is great, although Ricki Lake could have been better. And Johnny Depp is magnificent. He is such a beautiful man and a very talented actor. And seeing most of Johnny's movies, this is probably my favorite. I give it 9.5/10. Rent it today!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DrDgdYv4EqT",
        "colab_type": "text"
      },
      "source": [
        "## Question\n",
        "\n",
        "Les représentations en sac de mots pondérés par TF-IDF possèdent des limitations quand il s'agit de capter la polarité d'un texte. \n",
        "\n",
        "**Lesquelles pouvez-vous citer ?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8bsQPXT2N_Q",
        "colab_type": "text"
      },
      "source": [
        "## Réponse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hi-c4buoNyWD",
        "colab_type": "text"
      },
      "source": [
        "- Perte de l'articulation des mots dans les phrases.\n",
        "- Quand plusieurs phrases constituent une argumentation, il est impossible avec une représentation TF-IDF de savoir quelle parties du discours sont positives/négatives."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwiJzruSO-7z",
        "colab_type": "text"
      },
      "source": [
        "# Partie 3 : UTILISATIONS DE RÉSEAUX RÉCURRENT SUR DES SÉQUENCES DE MOTS\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwYkkIXCvqUH",
        "colab_type": "text"
      },
      "source": [
        "## Préparation du dictionnaire et des séquences\n",
        "\n",
        "À l'aide de l'outil [`tensorflow.keras.preprocessing.text.Tokenizer`](https://keras.io/api/preprocessing/text/#tokenizer-class) :\n",
        "- Constituez un dictionnaire sur le Corpus `X_train_raw`\n",
        "- Quelle est la taille du vocabulaire ?\n",
        "- Quelle est la taille maximum, en nombre de mots, d'une critique ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLCeBm86gUfa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer_obj = Tokenizer()\n",
        "tokenizer_obj.fit_on_texts(X_train_raw)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2wOTyj54Yl4",
        "colab_type": "code",
        "outputId": "dbc50691-e654-4942-e2a1-b058459296d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "vocab_size = len(tokenizer_obj.word_index) + 1\n",
        "max_length = max(len(s.split()) for s in X_train_raw)\n",
        "print(\"Taille du vocabulaire : {}\".format(vocab_size))\n",
        "print(\"Taille de la plus grande séquence de mot : {}\".format(max_length))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Taille du vocabulaire : 89663\n",
            "Taille de la plus grande séquence de mot : 2450\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8ZCcOI0RqRH",
        "colab_type": "text"
      },
      "source": [
        "À l'aide des outils [`tensorflow.keras.preprocessing.text.Tokenizer`](https://keras.io/api/preprocessing/text/#tokenizer-class) et [`tensorflow.keras.preprocessing.sequence.pad_sequences`](https://keras.io/api/preprocessing/timeseries/#padsequences-function) :\n",
        "- Transformez `X_train_raw` et `X_test_raw` en séquences d'indices de mots dans un dictionnaire.\n",
        "- Effectuez l'opération de **padding** sur les séquences afin qu'elles aient une taille raisonnable pour être traitées par un GRU bidirectionnel.\n",
        "- Justifiez vos choix pour les options `maxlen` et `truncating` de la fonction `pad_sequences`\n",
        "- Stockez les séquences obtenues dans `X_train_pad` et `X_test_pad`\n",
        "- Pour des raisons de compatibilité pour la suite, transformez `X_train_pad`, `X_test_pad`, `Y_train`, `Y_test` en `numpy.ndarray` (en utilisant la fonction `numpy.array`)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZae4HhR46et",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "max_length = 150\n",
        "\n",
        "X_train_tokens = tokenizer_obj.texts_to_sequences(X_train_raw)\n",
        "X_test_tokens = tokenizer_obj.texts_to_sequences(X_test_raw)\n",
        "\n",
        "X_train_pad = pad_sequences(X_train_tokens, maxlen=max_length, truncating=\"pre\")\n",
        "X_test_pad = pad_sequences(X_test_tokens, maxlen=max_length, truncating=\"pre\")\n",
        "\n",
        "X_train_pad = np.array(X_train_pad)\n",
        "X_test_pad = np.array(X_test_pad)\n",
        "Y_train = np.array(Y_train)\n",
        "Y_test = np.array(Y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpnuLJ9MTP17",
        "colab_type": "text"
      },
      "source": [
        "## Justifications\n",
        "\n",
        "- `maxlen=150` : les GRU bidirectionnels conservent une information de qualité pour des séquences de mots de cet ordre de grandeur de longueur.\n",
        "- `truncating=\"pre\"` : on ne conserve que la fin de la critique, plus susceptible de contenir une information pertinente quand on fait une critique, i.e la conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTrg_YGAWKYl",
        "colab_type": "text"
      },
      "source": [
        "## Modélisation\n",
        "\n",
        "Construisez un modèle avec pour caractéristiques :\n",
        "  - Une couche d'embeddings de taille 300\n",
        "  - Une couche de GRU bidirectionnels :\n",
        "    - de taille `16`\n",
        "    - une initialisation **des** matrices de poids orthogonales\n",
        "    - un paramètre de dropout à `0.2` pour les parties forward et récurrentes\n",
        "  - Une couche de réseau de neurones à activation `softmax` qui prend en entrée la dernière sortie de la couche [`GRU`](https://keras.io/api/layers/recurrent_layers/gru/)\n",
        "  - Une fonction de perte basée sur l'**entropie croisée**\n",
        "  - L'optimiseur `adam`\n",
        "  - l'`accuracy` comme métrique d'évaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4mpE2iumh9e",
        "colab_type": "code",
        "outputId": "9f8ce93a-874e-4e12-c81a-94c3e93d2bbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, GRU , Bidirectional\n",
        "EMBEDDING_DIM = 300\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, EMBEDDING_DIM, input_length=max_length))\n",
        "model.add(Bidirectional(GRU(16, kernel_initializer=\"orthogonal\", recurrent_initializer=\"orthogonal\", dropout=0.2, recurrent_dropout=0.2)))\n",
        "model.add(Dense(2, activation=\"softmax\"))\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer gru will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer gru will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer gru will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa1QMqbjYnen",
        "colab_type": "text"
      },
      "source": [
        "### Questions\n",
        "\n",
        "Quel est l'intêret de :\n",
        "- l'initialisation **orthogonale** des matrices de poids ?\n",
        "- la version **bidirectionnel** des GRU ?\n",
        "- du **dropout** ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7r6Lk237ZVgL",
        "colab_type": "text"
      },
      "source": [
        "### Réponses\n",
        "\n",
        "- L'initialisation orthogonale permet d'**éviter l'explosion des gradients** dans les réseaux récurrents\n",
        "- Les RNN bidirectionnels, permetttent de traiter plus efficacement des **séquences longues** tout en améliorant la qualité de la **contextualisation** de l'information\n",
        "- Le dropout est une technique de **régularisation** et donc essaye d'empêcher le suraprentiage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGIvkVXaaTco",
        "colab_type": "text"
      },
      "source": [
        "## Apprentissage\n",
        "\n",
        "Apprenez votre modèle sur `X_train_pad` :\n",
        "- avec des batch de taille `256`\n",
        "- pendant `10` itérations\n",
        "- en utilisant un split `0.3` (30%) de la base d'apprentissage pour validation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEzpBrJm7Mct",
        "colab_type": "code",
        "outputId": "d8f4115b-ce74-47dd-bb0e-512807521fbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "source": [
        "model.fit(X_train_pad, Y_train, batch_size=256, epochs=10, validation_split=0.3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "69/69 [==============================] - 119s 2s/step - loss: 0.6325 - accuracy: 0.6403 - val_loss: 0.5246 - val_accuracy: 0.7287\n",
            "Epoch 2/10\n",
            "69/69 [==============================] - 117s 2s/step - loss: 0.3467 - accuracy: 0.8475 - val_loss: 0.3618 - val_accuracy: 0.8479\n",
            "Epoch 3/10\n",
            "69/69 [==============================] - 117s 2s/step - loss: 0.1695 - accuracy: 0.9371 - val_loss: 0.3894 - val_accuracy: 0.8555\n",
            "Epoch 4/10\n",
            "69/69 [==============================] - 117s 2s/step - loss: 0.0821 - accuracy: 0.9737 - val_loss: 0.4238 - val_accuracy: 0.8553\n",
            "Epoch 5/10\n",
            "69/69 [==============================] - 117s 2s/step - loss: 0.0443 - accuracy: 0.9866 - val_loss: 0.4939 - val_accuracy: 0.8439\n",
            "Epoch 6/10\n",
            "69/69 [==============================] - 117s 2s/step - loss: 0.0242 - accuracy: 0.9938 - val_loss: 0.5230 - val_accuracy: 0.8503\n",
            "Epoch 7/10\n",
            "69/69 [==============================] - 118s 2s/step - loss: 0.0145 - accuracy: 0.9966 - val_loss: 0.5881 - val_accuracy: 0.8507\n",
            "Epoch 8/10\n",
            "69/69 [==============================] - 118s 2s/step - loss: 0.0124 - accuracy: 0.9968 - val_loss: 0.6223 - val_accuracy: 0.8412\n",
            "Epoch 9/10\n",
            "69/69 [==============================] - 118s 2s/step - loss: 0.0104 - accuracy: 0.9973 - val_loss: 0.6402 - val_accuracy: 0.8417\n",
            "Epoch 10/10\n",
            "69/69 [==============================] - 119s 2s/step - loss: 0.0081 - accuracy: 0.9981 - val_loss: 0.6890 - val_accuracy: 0.8473\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fcf3ce34518>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zromBdoPbV91",
        "colab_type": "text"
      },
      "source": [
        "## Évaluation\n",
        "\n",
        "Évaluez les performances de votre modèle sur la base de test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "26d552e7-31df-45aa-afdf-71e62c60c08d",
        "id": "syRQojTExvA1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "test_loss, test_accuracy = model.evaluate(X_test_pad, Y_test)\n",
        "print(\"Accuracy sur la base de test : {:.3f}\".format(test_accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 71s 90ms/step - loss: 0.7814 - accuracy: 0.8312\n",
            "Accuracy sur la base de test : 0.831\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVYT_SAkbt0z",
        "colab_type": "text"
      },
      "source": [
        "### Question\n",
        "\n",
        "Que peut-on conclure de l'apprentissage de notre modèle ? Justifiez"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjLLabomb7q8",
        "colab_type": "text"
      },
      "source": [
        "### Réponse\n",
        "\n",
        "D'après l'évolution de l'accuracy sur train et sur valid, on observe très nettement un phénomène de surapprentissage :\n",
        "- train accuracy > 99%\n",
        "- valid accuracy < 85%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Olcx_9SUk8o_",
        "colab_type": "text"
      },
      "source": [
        "# Partie 4 : UTILISATION DE WORD EMBEDDINGS PRÉ-APPRIS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hGtfPIdvSP_",
        "colab_type": "text"
      },
      "source": [
        "Afin d'obtenir de meilleurs résultats, vous allez utiliser les embeddings pré-appris  **GloVe**¹, plutôt que d'apprendre des embeddings spécifiques comme précédemment.\n",
        "\n",
        "\n",
        "\n",
        "¹ Jeffrey Pennington, Richard Socher, and Christopher D. Manning. [*GloVe: Global Vectors for Word Representation*](https://nlp.stanford.edu/pubs/glove.pdf), 2014.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYIBEwyfhqEe",
        "colab_type": "text"
      },
      "source": [
        "## Réutilisation des Word Embeddings **GloVe**\n",
        "\n",
        "Après un rapide coup d'oeil aux fichiers `glove.6B.*.txt` :\n",
        "- Créez un dictionnaire `embeddings_dic` avec les mots pour clés et leur vecteurs de `float32` respectifs en valeur.\n",
        "- Créez un layer `embedding_layer` de type [`tensorflow.keras.layers.Embedding`](https://keras.io/api/layers/core_layers/embedding/) :\n",
        "  - Initialisez-le avec les embeddings GloVe. \n",
        "  - Initialisez les mots de notre vocabulaire qui ne seraient pas dans GloVe avec le vecteur nul\n",
        "  - Utilisez le flag approprié pour empêcher le changement de ces embeddings pendant l'apprentissage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpVnzpWhrOAJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings_dic = {}\n",
        "f = open(glove_filename(EMBEDDING_DIM))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_dic[word] = coefs\n",
        "f.close()\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
        "cpt = 0\n",
        "for word, i in tokenizer_obj.word_index.items():\n",
        "    embedding_vector = embeddings_dic.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        cpt = cpt+1\n",
        "print(\"found \"+str(cpt)+\" embeddings over \"+str(vocab_size)+\" words in vocab\")\n",
        "\n",
        "embedding_layer = Embedding(vocab_size,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=max_length,\n",
        "                            trainable=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lzUOg8-k0m9",
        "colab_type": "text"
      },
      "source": [
        "## Modélisation, Apprentissage et Évaluation\n",
        "\n",
        "- Créez un modèle identique à celui de la partie 3, à ceci près que la couche d'**Embeddings** est celle que l'on vient d'instancier à partir de **GloVe**\n",
        "- Entrainez ce modèle avec les mêmes paramètres que dans la partie 3\n",
        "- Évaluez-le sur les données de test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGonw7W4tsEu",
        "colab_type": "code",
        "outputId": "8cf070c1-0592-4110-e9e7-6f37ae15d0f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(embedding_layer)\n",
        "model.add(Bidirectional(GRU(16, kernel_initializer='orthogonal' , recurrent_initializer='orthogonal', dropout=0.2, recurrent_dropout=0.2)))\n",
        "model.add(Dense(2 , activation ='softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy' , optimizer='adam', metrics=[\"accuracy\"])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer gru_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer gru_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer gru_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "St0sjo_ru07D",
        "colab_type": "code",
        "outputId": "97e30b9a-b13d-4ff0-c25d-4aef4c3291e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "source": [
        "model.fit(X_train_pad, Y_train, batch_size=256, epochs=10, validation_split=0.3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "69/69 [==============================] - 61s 889ms/step - loss: 0.6427 - accuracy: 0.6234 - val_loss: 0.5739 - val_accuracy: 0.7083\n",
            "Epoch 2/10\n",
            "69/69 [==============================] - 60s 875ms/step - loss: 0.4872 - accuracy: 0.7663 - val_loss: 0.4497 - val_accuracy: 0.7971\n",
            "Epoch 3/10\n",
            "69/69 [==============================] - 60s 864ms/step - loss: 0.4176 - accuracy: 0.8133 - val_loss: 0.3962 - val_accuracy: 0.8219\n",
            "Epoch 4/10\n",
            "69/69 [==============================] - 59s 860ms/step - loss: 0.3818 - accuracy: 0.8351 - val_loss: 0.3783 - val_accuracy: 0.8341\n",
            "Epoch 5/10\n",
            "69/69 [==============================] - 59s 856ms/step - loss: 0.3559 - accuracy: 0.8479 - val_loss: 0.3654 - val_accuracy: 0.8439\n",
            "Epoch 6/10\n",
            "69/69 [==============================] - 59s 856ms/step - loss: 0.3360 - accuracy: 0.8561 - val_loss: 0.3463 - val_accuracy: 0.8501\n",
            "Epoch 7/10\n",
            "69/69 [==============================] - 59s 861ms/step - loss: 0.3228 - accuracy: 0.8638 - val_loss: 0.3397 - val_accuracy: 0.8532\n",
            "Epoch 8/10\n",
            "69/69 [==============================] - 59s 850ms/step - loss: 0.3087 - accuracy: 0.8692 - val_loss: 0.3790 - val_accuracy: 0.8359\n",
            "Epoch 9/10\n",
            "69/69 [==============================] - 59s 855ms/step - loss: 0.3026 - accuracy: 0.8727 - val_loss: 0.3317 - val_accuracy: 0.8563\n",
            "Epoch 10/10\n",
            "69/69 [==============================] - 59s 859ms/step - loss: 0.2867 - accuracy: 0.8797 - val_loss: 0.3285 - val_accuracy: 0.8580\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f6fdb1ddd68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fyq5RVX4xPAA",
        "colab_type": "code",
        "outputId": "20f7fa90-edce-460f-dc9c-141613177cf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "model.evaluate(X_test_pad, Y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 49s 63ms/step - loss: 0.3195 - accuracy: 0.8658\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3194841742515564, 0.8658000230789185]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uzaSB99BlzzB"
      },
      "source": [
        "## Question\n",
        "\n",
        "Que peut-on conclure de l'apprentissage de notre modèle ? Justifiez"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O4saC3OJl2ud"
      },
      "source": [
        "## Réponse\n",
        "\n",
        "D'après l'évolution de l'accuracy sur train et sur valid, le phénomène de surapprentissage à quasiement disparu : les performances en train, valid et test sont toutes dans le même ordre de grandeur."
      ]
    }
  ]
}