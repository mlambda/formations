\begin{frame}
  \frametitle{Introduction}
  Équivalent de la régression linéaire mais quand la sortie est binaire. \\
  L'approche est bayésienne, basée sur une étude statistique. \\
\end{frame}

\begin{frame}
  \frametitle{Le cas à 2 classes}
  On s'intéresse à faire une régression de l'\textit{"évidence"} de la variable aléatoire cible : 
  \begin{center}
    $\ln {\frac {P(1\vert X)}{1-P(1\vert X)}}=b_{0}+b_{1}x_{1}+...+b_{d}x_{d}$
  \end{center}
  où 
  \begin{itemize}
  \item $X = [x_1, ... , x_d]$ un exemple de la base
  \item $B = [b_1, ... , b_d]$ l'ensemble des paramètres de notre modèle
  \item $P(1\vert X)$ est la probabilité que X soit de classe 1
  \end{itemize}
  Les coéfficients B sont alors estimés par descente de gradient
\end{frame}

\begin{frame}
  \frametitle{Multinomial logistic Regression}
  Problème à K classes :
  \begin{itemize}
  \item (K-1) prédicteurs binaire en utilisant une classe 'pivot'
  \item Maximum de vraisemblance pour la prédiction finale
  \end{itemize}
\end{frame}
