\begin{frame}
  \frametitle{Réseaux de Neurones Récurrents}
  \imgtw[0.7]{nn-rnn-comparaison}
\end{frame}

\begin{frame}
  \frametitle{Réseaux de Neurones Récurrents}
  \imgtw[0.7]{rnn-unfold}
  $h_{t}=\sigma_{h}(U*x_{t}+V*h_{t-1}+b_{h})$ \\
  $o_{t}=\sigma_{o}(W*h_{t}+b_{o})$ \\
  \begin{itemize}
  \item $x_{t}$ : vecteur d'entrée
  \item $h_t$ : vecteur de la couche cachée
  \item $o_{t}$ : vecteur de sortie
  \item $U$, $V$, $W$, $b_h$ et $b_o$ : matrices et vecteurs (paramètres)
  \item $\sigma_{h}$ et $\sigma_o$ : fonctions d'activation (ReLu)
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Réseaux de Neurones Récurrents}
  Prédiction de la suite d'une séquence (ex: modèle de langage)
  \imgtw[0.7]{rnn-language-model}
  Peut être utilisé pour générer de nouvelles séquences
\end{frame}

\begin{frame}
  \frametitle{Réseaux de Neurones Récurrents}
  Prédiction d'une classe
  \imgth[0.7]{rnn-predict-class-2}
\end{frame}

\begin{frame}
  \frametitle{Réseaux de Neurones Récurrents}
  Génération d'une séquence (seq2seq)
  \imgtw[0.7]{rnn-seq2seq}
\end{frame}

\begin{frame}
  \frametitle{Réseaux de Neurones Récurrents}
  Le problème du gradient qui disparaît (vanishing gradient)
  \imgtw{vanishing-gradient-rnn}
\end{frame}
