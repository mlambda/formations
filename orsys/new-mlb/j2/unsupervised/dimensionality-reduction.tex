\begin{frame}
  \frametitle{Réduction de la dimensionalité}
    \begin{center}
      Comment appréhender des données en grande dimension ?
    \end{center}
    \[
    X = \begin{bmatrix}
      X_{1,1} & X_{1,2} & \dots  & X_{1,D} \\
      X_{2,1} & X_{2,2} & \dots  & X_{2,D} \\
      \vdots & \vdots & \ddots & \vdots \\
      X_{N,1} & X_{N,2} & \dots  & X_{N,D}
    \end{bmatrix}
    \]
\end{frame}

\begin{frame}
  \frametitle{La malédiction des grandes dimensions !}
    \begin{center}
      Nombre d'extrémités dans une espace de dimension : \\
      $\;$ \\
      \begin{tabular}{|c|c|c|c|c|c|c|}
        \hline
        dim & 1 & 2 & 3 & 4 & 5 & ... \\
        \hline
        pts & 2 & 4 & 8 & 16 & 32 & ...\\
        \hline
      \end{tabular}
    \end{center}
\end{frame}

\begin{frame}
  \frametitle{Réduction de la dimensionalité}
    \begin{itemize}
    \item Séléction de dimensions
    \item Projections linéaires (ACP, LDA, ...)
    \item Projections non-linéaires (kernels, neural network embeddings, ...)
    \end{itemize}
\end{frame}
