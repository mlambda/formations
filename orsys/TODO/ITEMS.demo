1) INTELLIGENCE ARTIFICIELLE
domaine.
x- Le fantasme de l'Intelligence Artificielle et la réalité d'aujourd'hui.
x- Historique, concepts de base et applications de l'intelligence artificielle loin des fantasmes portés par ce
x- Tâche intellectuelle versus algorithmes.
x- Types d'actions : classification, régression, clustering, estimation de densité, réduction de dimensionnalité.
- Intelligence collective : agréger une connaissance partagée par de nombreux agents virtuels.
- Algorithmes génétiques : faire évoluer une population d'agents virtuels par sélection.
x- Machine Learning : présentation et principaux algorithmes (XGBoost, Random Forest).

5) DATASET
x- Qu'est-ce qu'un Dataset ?
x- Stocker/contrôler la donnée : surveiller les biais, nettoyer/convertir sans s'interdire des retours en arrière.
x- Comprendre la donnée : représentation des outils statistiques permettant une vision d'une donnée, sa distribution...
- Formater une donnée : décider d'un format d'entrée et de sortie, faire le lien avec la qualification du problème.
x- Préparer la donnée : définition des Train Set, Validation Set et Test Set.
x- Mettre en place une structure permettant de garantir que les algorithmes utilisés sont réellement pertinents (ou non).
Echanges
- Définition d'un Dataset et sa différence avec un BDD usuel

4) MACHINE LEARNING
x- Condition sur les données : volumétrie, dimensionnement, équilibre entre les classes, description
x- Donnée brute vs features travaillées : que choisir ?
x- Machine Learning versus Deep Learning : les algorithmes plus anciens du Machine Learning ou les réseaux de neurones ?
x- Qualifier le problème : Unsupervised Learning versus Supervised Learning.
x- Qualifier la solution d'un problème : comprendre la distance entre une affirmation et le résultat d'un algorithme.
Etude de cas
- Qualification d'une problématique pouvant être traitée avec l'IA

3) APPLICATIONS MACHINE LEARNING
x- Classification de données. Les différents scénarios : donnée brute, image, son, texte, etc.
x- Les enjeux d'une classification de données et les choix impliqués par un modèle de classification.
x- Outils de classification : des réseaux de type Multilayer Perceptron ou Convolutional Neural Network.
Machine Learning.
x- Prédiction d'information et donnée séquentielle/temporelle. Enjeux et limites d'une prédiction d'information.
x- Règles structurelles au sein de la donnée pouvant permettre une logique de prédiction. Outils usuels de prédiction.
x- Transformation/génération de données. Opération de réinterprétation d'une donnée : débruitage, segmentation d'image...
x- Opération de transformation sur un même format : traduction de texte d'une langue à une autre...
x- Opération de génération de donnée "originale" : Neural Style, génération d'images à partir de présentations
textuelles.
x- Reinforcement Learning : contrôle d'un environnement.
x- Experience Replay et apprentissage de jeux vidéo par un réseau de neurones.
Démonstration
- Classification d'images médicales. Prévision des images suivant une séquence vidéo. Contrôle de simulations numériques.

2) RESEAUX DE NEURONES
x- Rappel de bases mathématiques.
x- Qu'est-ce qu'un réseau de neurones ?
x- Qu'est-ce que l'apprentissage d'un réseau de neurones ? Deep versus shallow network, overfit, underfit, convergence.
x- L'apprentissage d'un réseau de neurones : fonctions de coût, back-propagation, stochastic gradient descent...
x- Approximer une fonction par un réseau de neurones : présentation et exemples.
x- Approximer une distribution par un réseau de neurones : présentation et exemples.
x- Génération de représentations internes au sein d'un réseau de neurones.
x- Généralisation des résultats d'un réseau de neurones.
- Data Augmentation : comment équilibrer un dataset ?
- Initialisations et régularisations d'un réseau de neurones : L1/L2 Regularization, Batch Normalization.
x- Révolution du Deep Learning : généricité des outils et des problématiques.
Démonstration
- Approximation d'une fonction et d'une distribution par un réseau de neurones.
- Présentation d'un algorithme de classification et de ses limites.

4) Convolutional Neural Networks (CNN)
x- Présentation des CNNs : principes fondamentaux et applications.
- Fonctionnement fondamental d'un CNN : couche convolutionnelle, utilisation d'un kernel, padding et stride...
- Architectures CNN ayant porté l'état de l'art en classification d'images : LeNet, VGG Networks, Network in Network...
x- Utilisation d'un modèle d'attention.
x- Application à un cas de figure de classification usuel (texte ou image).
- CNNs pour la génération : super-résolution, segmentation pixel à pixel.
- Principales stratégies d'augmentation des Feature Maps pour la génération d'une image.
Etude de cas
- Innovations apportées par chaque architecture CNN et leurs applications plus globales (convolution 1x1 ou connexions résiduelles).

5) Recurrent Neural Networks (RNN)
x- Présentation des RNNs : principes fondamentaux et applications.
x- Fonctionnement fondamental du RNN : hidden activation, back propagation through time, unfolded version.
x- Évolutions vers les GRU (Gated Recurrent Units) et LSTM (Long Short Term Memory).
x- Problèmes de convergence et vanising gradient.
x- Types d'architectures classiques : prédiction d'une série temporelle, classification...
x- Architecture de type RNN Encoder Decoder. Utilisation d'un modèle d'attention.
x- Applications NLP : word/character encoding, traduction.
x- Applications vidéo : prédiction de la prochaine image générée d'une séquence vidéo.
Démonstration
- Différents états et évolutions apportées par les architectures Gated Recurrent Units et Long Short Term
Memory.

6) Modèles générationnels : VAE et GAN
- Présentation des modèles générationnels Variational AutoEncoder (VAE) et Generative Adversarial Networks (GAN).
x- Auto-encoder : réduction de dimensionnalité et génération limitée.
- Variational AutoEncoder : modèle générationnel et approximation de la distribution d'une donnée.
- Définition et utilisation de l'espace latent. Reparameterization trick.
x- Fondamentaux du Generative Adversarial Networks.
- Convergence d'un GAN et difficultés rencontrées.
- Convergence améliorée : Wasserstein GAN, BeGAN. Earth Moving Distance.
x- Applications de génération d'images ou de photographies, génération de texte, super résolution.
Démonstration
- Applications des modèles générationnels et utilisation de l'espace latent.

7) Deep Reinforcement Learning
x- Reinforcement Learning.
x- Utilisation d'un réseau de neurones pour approximer la fonction d'état.
x- Deep Q Learning : experience replay et application au contrôle d'un jeu vidéo.
- Optimisations de la politique d'apprentissage. On-policy et off-policy. Actor critic architecture. A3C.
x- Applications : contrôle d'un jeu vidéo simple ou d'un système numérique.
Démonstration
- Contrôle d'un agent dans un environnement défini par un état et des actions possibles.

6) METHODOLOGIE
x- Méthodologie pour avancer dans la recherche d'une meilleure solution à un problème ML/DL.
- Choix d'une direction de recherche, localisation de publications ou de projets similaires existants.
x- Itérations successives depuis les algorithmes les plus simples jusqu'aux architectures les plus complexes.
- Conservation d'un banc de comparaison transversal.
- Arriver à une solution optimale.
Etude de cas
- Grouper et balancer un ensemble de solutions pour obtenir une solution optimale.

7) OUTILS
- Quels outils existe-t-il aujourd'hui ?
- Quels outils pour la recherche et quels outils pour l'industrie ?
- De Keras/Lasagne à Caffe en passant par Torch, Theano, Tensorflow ou Apache Spark ou Hadoop.
- Industrialiser un réseau de neurones par un encadrement strict de son processus et un monitoring continu.
- Mise en place de réapprentissages successifs pour conserver un réseau à jour et optimal.
- Former des utilisateurs à la compréhension du réseau.
- Outils de gestion de donnée : Apache Spark, Apache Hadoop.
- Outils Machine Learning usuel : Numpy, Scipy, Sci-kit.
- Frameworks DL haut niveau : PyTorch, Keras, Lasagne.
- Frameworks DL bas niveau : Theano, Torch, Caffe, Tensorflow.
Démonstration
- Applications et limites des outils présentés (Apache Spark, Apache Hadoop, Numpy, Scipy, Sci-kit, PyTorch, Keras, Lasagne, Theano, Torch, Caffe, Tensorflow)
- Mise en place de réapprentissages successifs.