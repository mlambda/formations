\documentclass{formation}

\title{Réseaux de neurones}

\date{7 décembre 2017}

\begin{document}

\maketitle
\begin{frame}
  \frametitle{Objectifs}
  \begin{itemize}
  \item Comprendre l'architecture de réseaux de neurones simples
  \item Démystifier la rétropropagation des gradients
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Réseau de neurones}
  Intuition:
  \begin{tabbing}
    1M neurones \=\kill
    1 neurone \>= 1 calcul simple\\
    1M neurones \>= 1M calculs simples\\
    \>= 1 calcul complexe\\
  \end{tabbing}
\end{frame}

\begin{frame}
  \frametitle{Réseau de neurones}
  \imgtw{neural-network}
\end{frame}

\begin{frame}
  \frametitle{Neurone biologique}
  \imgtw{biological-neuron}
\end{frame}

\begin{frame}
  \frametitle{Neurone artificiel}
  Simulation extrêmement basique d'un neurone: somme pondérée +
  activation.
  \imgth[0.6]{neuron-model}
\end{frame}

\begin{frame}
  \frametitle{Linéarité}
  Présence exclusive de sommes pondérées (combinaisons linéaires) →
  linéarité du réseau quelque soit sa profondeur (une combinaison
  linéaire de combinaisons linéaires est une combinaison linéaire).

  → nécessité d'introduire des non-linéarités (sigmoid, tanh, ReLU, …)
  \imgcmw{7}{xor}
\end{frame}

\begin{frame}
  \frametitle{Apprentissage}
  Modification des poids des neurones pour que les sommes pondérées
  activées expriment la fonction souhaitée.
\end{frame}

\begin{frame}
  \frametitle{Métrique}
  Pour quantifier la qualité des poids du réseau, définition d'une
  fonction de perte à partir de données annotées.

  \[
    L(\hat{y}, y)
  \]

  Plus cette perte est proche de 0, meilleurs sont les poids de notre
  réseau.
\end{frame}


\begin{frame}
  \frametitle{Apprentissage --- modélisation}
  Apprendre = minimiser la fonction de perte.

  \[
    \argmin_{\hat{y}}L(\hat{y}, y)
  \]
\end{frame}

\begin{frame}
  \frametitle{Apprentissage --- optimisation par descente du gradient}
  Minimisation de $L$ en soustrayant pour chaque poids $w$ une partie
  du gradient par rapport à la perte ($\alpha$ est appelé pas
  d'apprentissage).

  \[
    w \leftarrow w - \alpha \frac{\partial L}{\partial w}
  \]
  
\end{frame}

\begin{frame}
  \frametitle{Règle de chainage --- cas simple}
  \imgth[.5]{monopath}

  \[
    \frac{\partial z}{\partial x} = \frac{\partial z}{\partial y}
    \frac{\partial y}{\partial x}
  \]
\end{frame}

\begin{frame}
  \frametitle{Règle de chainage --- deux chemins}
  \imgth[.5]{multipaths}

  \[
    \frac{\partial z}{\partial x} = \frac{\partial z}{\partial y_1}
    \frac{\partial y_1}{\partial x} + \frac{\partial z}{\partial y_2}
    \frac{\partial y_2}{\partial x}
  \]
\end{frame}

\begin{frame}
  \frametitle{Règle de chainage --- chemins multiples}
  \imgth[.5]{multipaths-general}

  \[
    \frac{\partial z}{\partial x} = \sum_{i = 1}^{n} \frac{\partial
      z}{\partial y_i} \frac{\partial y_i}{\partial x}
  \]
\end{frame}

\begin{frame}
  \frametitle{Règle de chainage --- graphe dirigé acyclique}
  \imgth[.5]{multipaths-recursion}

  Pointillés = dépendance indirecte

  \[
    \frac{\partial z}{\partial x} = \sum_{i = 1}^{n} \frac{\partial
      z}{\partial y_i} \frac{\partial y_i}{\partial x}
  \]
\end{frame}

\begin{frame}
  \frametitle{Règle de chainage --- Exercice}
  \imgth[.5]{multipaths-practice}

  \begin{enumerate}
  \item Dans quel ordre doit-on calculer les dérivées partielles ?
  \item Quelle est la chaîne de calcul pour trouver la dérivée du
    poids $x_1$ par rapport à l'erreur $z$ ?
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{Règle de chainage --- question 1}
  \imgth[.5]{multipaths-practice}
  Dans quel ordre doit-on calculer les dérivées partielles ?
  \pause
  \begin{enumerate}[<+->]
  \item $\partiald{z}{y_1}, \partiald{z}{y_3}, \partiald{z}{i_3}$
  \item
    $\partiald{y_1}{y_2}, \partiald{y_1}{x_1}, \partiald{y_1}{i_1},
    \partiald{y_3}{x_1}, \partiald{y_3}{i_3}$
  \item $\partiald{y_2}{x_1}$
  \item $\partiald{x_1}{i_1}$,$\partiald{x_1}{i_2}$,$\partiald{x_1}{i_3}$
  \end{enumerate}
\end{frame}
\begin{frame}
  \frametitle{Règle de chainage --- question 2}
  \imgth[.5]{multipaths-practice} Quelle est la chaîne de calcul pour
  trouver la dérivée du
  poids $x_1$ par rapport à l'erreur $z$ ?\\
  \pause
  \[
    \frac{\partial z}{\partial y_1}
    \frac{\partial y_1}{\partial x_1}
    \pause
    +
    \frac{\partial z}{\partial y_1}\frac{\partial y_1}{\partial
      y_2}\frac{\partial y_2}{\partial x_1}
    \pause
    +
    \frac{\partial z}{\partial y_3}
    \frac{\partial y_3}{\partial x_1}
  \]
\end{frame}

\begin{frame}
  \frametitle{Zoom sur les non-linéarités}
  Analyser une non-linéarité:
  \begin{itemize}
  \item saturante ou non
  \item propriétés de la dérivée
  \item blocage possible ou non
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Sigmoid}
  \begin{itemize}
  \item Définition: $\sigma(x) = \frac{1}{1 + e^{-x}}$
  \item Dérivée: $\sigma'(x) = \sigma(x).(1 - \sigma(x))$
  \item Fonction saturante
  \end{itemize}
  \alert{$\sigma'(x) \leq 0.25$}
  \imgcmw{5}{sigmoid}
\end{frame}

\begin{frame}
  \frametitle{Tanh}
  \begin{itemize}
  \item Définition: $\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$
  \item Dérivée: $\tanh'(x) = 1 - \tanh^2(x)$
  \item Fonction saturante
  \end{itemize}
  \imgcmw{7}{tanh}
\end{frame}

\begin{frame}
  \frametitle{ReLU}
  \begin{itemize}
  \item Définition: $\ReLU(x) = \max(0, x)$
  \item Dérivée: \(\ReLU'(x) =
    \begin{cases}
      0 & \text{si}\ x < 0\\
      1 & \text{si}\ x \geq 0
    \end{cases}
    \)
  \item Fonction semi-saturante
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Zoom sur les fonctions de perte}
  Analyser une fonction de perte:
  \begin{itemize}
  \item Classification ou régression
  \item Log ou linéaire
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Perte L1}
  \[
    \l1(x, y) = \frac{\sum_{i = 1}^n \lvert y_i - x_i \lvert}{n}
  \]

  \begin{itemize}
  \item Régression
  \item Peu utilisée car pénalise de la même manière toutes les
    amplitudes d'erreur
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Perte L2}
  \[
    \L2(x, y) = \frac{\sum_{i = 1}^n \lvert y_i - x_i \lvert^2}{n}
  \]
  \begin{itemize}
  \item Régression
  \item Perte la plus utilisée
  \item Pénalise plus fortement les grandes erreurs
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Entropie croisée binaire}
  $y \in \{0, 1\}^n$, $x \in [0,1]^n$.
  \[
    \ECB(x, y) = -\frac{\sum_{i=1}^n\left(y_i\log x_i + (1 - y_i)\log 1 - x_i\right)}{n}
  \]
    
  \begin{itemize}
  \item Classification
  \item Utilisée quand les sorties ($x_i$) sont indépendantes
  \item Logarithmique
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Entropie croisée}
  $c$ index de la classe correcte, $x$ distribution de proba sur les classes.
  \begin{align*}
    \EC(x, c) & = -\frac{\log e^{x_c}}{\sum_j e^{x_j}} \\
              & = -x_c + \log\sum_j e^{x_j} \\
  \end{align*}
  
  \begin{itemize}
  \item Classification
  \item Utilisée quand les sorties sont liées
  \item Logarithmique
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Et bien d'autres}
  \begin{itemize}
  \item Pour gérer des embeddings (distance cosinus)
  \item Pour gérer des classements
  \item Pertes multi-termes
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Conclusion}
  \begin{itemize}
  \item Neurone = somme pondérée + activation
  \item Apprentissage = trouver les bons poids des sommes
  \item Métrique = fonction de perte
  \item Technique = rétropropagation des gradients
  \item Non linéarités et pertes classiques ont quelques propriétés à
    connaître
  \end{itemize}
\end{frame}
\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
