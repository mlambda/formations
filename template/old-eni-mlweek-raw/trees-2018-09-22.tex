\documentclass{formation}
\title{Arbres de décision}
\subtitle{Module 5}

\begin{document}

\maketitle

\section{Objectifs}

\begin{frame}
  \frametitle{Objectifs}

  \begin{itemize}
  \item construire un arbre de décision aussi bien pour la régression
    que pour la classification
  \item combiner plusieurs arbres efficacement avec Random Forest
  \item améliorer séquentiellement des arbres avec le Gradient
    Boosting
  \end{itemize}
\end{frame}

\section{Arbres de décision}

\begin{frame}
  \frametitle{Introduction}
  Modèle de classification ou regréssion qui classe un input dans une
  de ses feuilles pour rendre sa prédiction :

  \begin{columns}
    \begin{column}{.5\textwidth}
      \begin{forest}
        [âge > 20
          [{sexe = H}
            [0]
            [1]
          ]
          [1]
        ]
      \end{forest}
    \end{column}
    \begin{column}{.5\textwidth}
      \begin{itemize}
      \item $p(\begin{bmatrix}H \\ 27\end{bmatrix}) = 0$
      \item $p(\begin{bmatrix}H \\ 7\end{bmatrix}) = 1$
      \item $p(\begin{bmatrix}F \\ 27\end{bmatrix}) = 1$
      \end{itemize}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}
  \frametitle{Avantages}
  Les arbres de décision
  \begin{itemize}[<+->]
  \item gèrent les inputs numériques comme catégoriels
  \item ne nécessitent pas que la variable d'output soit normalement
    distribuée (regression linéaire)
  \item sont interprétables
  \item sont très rapides durant l'inférence
  \item ne nécessitent pas de normalisation des données
  \item leur apprentissage est hautement parallèlisable
  \end{itemize}
  \onslide<+->{→ Couteau-suisse du machine learning tabulaire.}
\end{frame}

\begin{frame}
  \frametitle{Désavantages}

  \begin{itemize}[<+->]
  \item peuvent overfit les données, mais l'ensembling résoud ce
    problème
  \item sont sensibles aux déséquilibres de classe
  \end{itemize}
  \onslide<+->{→ Si les classes ne sont pas équilibrées, peut-être les
    resampler.}
\end{frame}

\begin{frame}
  \frametitle{Arbres de classification}
  \begin{columns}
    \begin{column}{.5\textwidth}
      \begin{forest}
        [ âge > 20
          [{sexe = H}
            [0]
            [1]
          ]
          [1]
        ]
      \end{forest}
    \end{column}
    \begin{column}{.5\textwidth}
      \begin{itemize}
      \item $p(\begin{bmatrix}H \\ 27\end{bmatrix}) = 0$
      \item $p(\begin{bmatrix}H \\ 7\end{bmatrix}) = 1$
      \item $p(\begin{bmatrix}F \\ 27\end{bmatrix}) = 1$
      \end{itemize}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}
  \frametitle{Arbres de régression}
  \begin{columns}
    \begin{column}{.5\textwidth}
      \begin{forest}
        [nuageux
          [froid
            [0]
            [bretagne
              [18]
              [8]
            ]
          ]
          [0]
        ]
      \end{forest}
    \end{column}
    \begin{column}{.5\textwidth}
      \begin{itemize}
      \item \(
        p(\begin{bmatrix}
          \text{nuageux} \\
          \text{tiède} \\
          \text{nord}
        \end{bmatrix}) = 8
        \)
      \item \(
        p(\begin{bmatrix}
          \text{nuageux} \\
          \text{chaud} \\
          \text{bretagne}
        \end{bmatrix}) = 18
        \)
      \item \(
        p(\begin{bmatrix}
          \text{nuageux} \\
          \text{froid} \\
          \text{bretagne}
        \end{bmatrix}) = 0
        \)
      \end{itemize}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}
  \frametitle{Apprendre un arbre de décision}
  
  Approche \og top-down\fg, procédure récursive :
  \begin{itemize}[<+->]
  \item créer un nœud de départ qui contient toutes les instances du
    training set
  \item tant qu'il reste des nœuds non-traités :
    \begin{itemize}
    \item choisir un nœud non traité
    \item si le nœud remplit des conditions de feuille finale, ne rien
      faire
    \item sinon, créer deux branches à partir du nœud non traité
      pour répartir les instances dans deux nouveaux nœuds
    \end{itemize}
  \end{itemize}

  \onslide<+->{Conditions de feuilles finales : contient $n_{min}$
    éléments, est déjà à profondeur $p_{max}$, splitterait sans
    décroître assez l'impureté…}
\end{frame}

\begin{frame}
  \frametitle{Décision rendue}
  
  En fonction de la tâche, une fois arrivé dans la feuille de fin :
  \begin{description}
  \item[Classification] classe majoritaire
  \item[Régression] moyenne des valeurs cibles
  \end{description}
\end{frame}

\begin{frame}
  \frametitle{Splits possibles}

  Splits possibles d'une feature donnée :

  \begin{description}
  \item[Catégorielle] chaque catégorie vs le reste
  \item[Ordinale/Continue] milieu de chaque valeur \green{(!!!)} ou
    quantiles
  \end{description}
\end{frame}

\begin{frame}
  \frametitle{Évaluation de la qualité d'un split}

  En fonction de la tâche :

  \begin{description}
  \item[Régression] coût si on rendait la moyenne des instances
    comme résultat
  \item[Classification] impureté de Gini. Pour chaque instance $i$ :
    \[
      p_i\sum_{k\neq i} p_k
    \]
    → risque de misclassification
  \end{description}
\end{frame}

\begin{frame}
  \frametitle{Exemple — démarrage}
  \begin{columns}
    \begin{column}{.5\textwidth}
      ID, jardinage, jeux vidéos, chapeaux, âge
      \[
        \begin{bmatrix}
          1 & 0 & 1 & 1 & 13  \\
          2 & 0 & 1 & 0 & 14 \\
          3 & 0 & 1 & 0 & 15 \\
          4 & 1 & 1 & 1 & 25 \\
          5 & 0 & 1 & 1 & 35 \\
          6 & 1 & 0 & 0 & 49 \\
          7 & 1 & 1 & 1 & 68 \\
          8 & 1 & 0 & 0 & 71 \\
          9 & 1 & 0 & 1 & 73 \\
        \end{bmatrix}
      \]
    \end{column}
    \begin{column}{.5\textwidth}
      Première étape : création du nœud de départ
      \\[1cm]
      \begin{forest}
        [{1, 2, 3, 4, 5, 6, 7, 8, 9} [{}]]
      \end{forest}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}
  \frametitle{Exemple — split}
  \begin{columns}
    \begin{column}{.5\textwidth}
      ID, jardinage, jeux vidéos, chapeaux, âge
      \[
        \begin{bmatrix}
          1 & 0 & 1 & 1 & 13  \\
          2 & 0 & 1 & 0 & 14 \\
          3 & 0 & 1 & 0 & 15 \\
          4 & 1 & 1 & 1 & 25 \\
          5 & 0 & 1 & 1 & 35 \\
          6 & 1 & 0 & 0 & 49 \\
          7 & 1 & 1 & 1 & 68 \\
          8 & 1 & 0 & 0 & 71 \\
          9 & 1 & 0 & 1 & 73 \\
        \end{bmatrix}
      \]
    \end{column}
    \begin{column}{.5\textwidth}
      Split du premier nœud. Il faut tester 3 splits. Split sur
      jardinage :
      \\[1cm]
      \begin{forest}
        [{1, 2, 3, 4, 5, 6, 7, 8, 9}
          [{4, 6, 7, 8, 9},
          edge label={node[midway,left,font=\scriptsize]{jardinage}}
            [{$\hat{y}=57,2$} [{$L=80,8$}]]]
          [{1, 2, 3, 5},
          edge label={node[midway,right,font=\scriptsize]{$\neg$ jardinage}}
            [{$\hat{y}=19,25$} [{$L=31,5$}]]]
        ]
      \end{forest}

      Loss totale : $122,3$
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}
  \frametitle{Exemple — split}
  \begin{columns}
    \begin{column}{.5\textwidth}
      ID, jardinage, jeux vidéos, chapeaux, âge
      \[
        \begin{bmatrix}
          1 & 0 & 1 & 1 & 13  \\
          2 & 0 & 1 & 0 & 14 \\
          3 & 0 & 1 & 0 & 15 \\
          4 & 1 & 1 & 1 & 25 \\
          5 & 0 & 1 & 1 & 35 \\
          6 & 1 & 0 & 0 & 49 \\
          7 & 1 & 1 & 1 & 68 \\
          8 & 1 & 0 & 0 & 71 \\
          9 & 1 & 0 & 1 & 73 \\
        \end{bmatrix}
      \]
    \end{column}
    \begin{column}{.5\textwidth}
      Split du premier nœud. Il faut tester 3 splits. Split sur
      chapeaux :
      \\[1cm]
      \begin{forest}
        [{1, 2, 3, 4, 5, 6, 7, 8, 9}
          [{1, 2, 3, 4, 5, 7},%
          edge label={node[midway,left,font=\scriptsize]{jeux vidéos}}
            [{$\hat{y}=28,3$} [{$L=92,6$}]]]
          [{6, 8, 9},
          edge label={node[midway,right,font=\scriptsize]{$\neg$ jeux vidéos}}
            [{$\hat{y}=64,3$} [{$L=30,7$}]]]
        ]
      \end{forest}

      Loss totale : $123,3$
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}
  \frametitle{Exemple — split}
  \begin{columns}
    \begin{column}{.5\textwidth}
      ID, jardinage, jeux vidéos, chapeaux, âge
      \[
        \begin{bmatrix}
          1 & 0 & 1 & 1 & 13  \\
          2 & 0 & 1 & 0 & 14 \\
          3 & 0 & 1 & 0 & 15 \\
          4 & 1 & 1 & 1 & 25 \\
          5 & 0 & 1 & 1 & 35 \\
          6 & 1 & 0 & 0 & 49 \\
          7 & 1 & 1 & 1 & 68 \\
          8 & 1 & 0 & 0 & 71 \\
          9 & 1 & 0 & 1 & 73 \\
        \end{bmatrix}
      \]
    \end{column}
    \begin{column}{.5\textwidth}
      Split du premier nœud. Il faut tester 3 splits. Split sur
      jeux vidéos :
      \\[1cm]
      \begin{forest}
        [{1, 2, 3, 4, 5, 6, 7, 8, 9}
          [{1, 4, 5, 7, 9},%
          edge label={node[midway,left,font=\scriptsize]{chapeaux}}
            [{$\hat{y}=42,8$} [{$L=110,8$}]]]
          [{2, 3, 6, 8},
          edge label={node[midway,right,font=\scriptsize]{$\neg$ chapeaux}}
            [{$\hat{y}=37,25$} [{$L=91$}]]]
        ]
      \end{forest}

      Loss totale : $201,8$
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}
  \frametitle{Exemple — split}
  \begin{columns}
    \begin{column}{.5\textwidth}
      ID, jardinage, jeux vidéos, chapeaux, âge
      \[
        \begin{bmatrix}
          1 & 0 & 1 & 1 & 13  \\
          2 & 0 & 1 & 0 & 14 \\
          3 & 0 & 1 & 0 & 15 \\
          4 & 1 & 1 & 1 & 25 \\
          5 & 0 & 1 & 1 & 35 \\
          6 & 1 & 0 & 0 & 49 \\
          7 & 1 & 1 & 1 & 68 \\
          8 & 1 & 0 & 0 & 71 \\
          9 & 1 & 0 & 1 & 73 \\
        \end{bmatrix}
      \]
    \end{column}
    \begin{column}{.5\textwidth}
      \begin{description}
      \item[122,3] jardinage
      \item[123,3] jeux vidéos
      \item[201,8] chapeaux
      \end{description}

      → On split donc sur jardinage
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}
  \frametitle{Exemple — split}
  \begin{columns}
    \begin{column}{.5\textwidth}
      ID, jardinage, jeux vidéos, chapeaux, âge
      \[
        \begin{bmatrix}
          1 & 0 & 1 & 1 & 13  \\
          2 & 0 & 1 & 0 & 14 \\
          3 & 0 & 1 & 0 & 15 \\
          4 & 1 & 1 & 1 & 25 \\
          5 & 0 & 1 & 1 & 35 \\
          6 & 1 & 0 & 0 & 49 \\
          7 & 1 & 1 & 1 & 68 \\
          8 & 1 & 0 & 0 & 71 \\
          9 & 1 & 0 & 1 & 73 \\
        \end{bmatrix}
      \]
    \end{column}
    \begin{column}{.5\textwidth}
      Résultat après le premier split :\\[1cm]

      \begin{forest}
        [{1, 2, 3, 4, 5, 6, 7, 8, 9}
          [{4, 6, 7, 8, 9},
          edge label={node[midway,left,font=\scriptsize]{jardinage}}]
          [{1, 2, 3, 5},
          edge label={node[midway,right,font=\scriptsize]{$\neg$ jardinage}}]
        ]
      \end{forest}

      \onslide<+->{À vous de jouer !}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}
  \frametitle{Limiter l'overfit}

  Fait par :

  \begin{itemize}
  \item la profondeur maximum
  \item le nombre minimum d'instances dans chaque feuille
  \item un baisse suffisante d'impureté
  \item le nombre minimum d'instances pour split
  \item le pruning
  \end{itemize}
\end{frame}

\section{Random Forest}

\begin{frame}
  \frametitle{Introduction}
  \begin{itemize}
  \item les arbres de décision overfit facilement
  \item ils sont rapides à apprendre
  \item en combiner beaucoup est faisable et réduit la variance
  \end{itemize}

  → création d'une forêt (ensemble d'arbres) aléatoire
\end{frame}

\begin{frame}
  \frametitle{But}

  Produire des arbres décorrélés et moyenner leurs prédictions pour
  réduire la variance.
\end{frame}

\begin{frame}
  \frametitle{Outil 1 — bagging (row sampling)}

  \textbf{B}oostrap \textbf{agg}regat\textbf{ing} (Bagging) :

  \begin{itemize}
  \item tirer un échantillon du dataset avec replacement
  \item entraîner un arbre sur cet échantillon
  \item répéter $B$ fois
  \end{itemize}

  Le bagging s'appelle aussi row sampling.

  \begin{figure}
    \centering \imgth[.4]{tree-training-modes}
    \scriptsize{\href{https://quantdare.com/what-is-the-difference-between-bagging-and-boosting/}%
        {quantdare.com/what-is-the-difference-between-bagging-and-boosting/}}
    
  \end{figure}

\end{frame}

\begin{frame}
  \frametitle{Outil 2 — random subspace method (column sampling)}

  \begin{itemize}
  \item à chaque split, considérer seulement un sous-ensemble des
    features
  \item valeurs conseillées :
    \begin{itemize}
    \item classification : $\lvert\sqrt m\rvert$ features par split
    \item regréssion: $\lvert\frac{m}{3}\rvert$ features par split, 5
      exemples par node minimum
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Bénéfice 1 — importance des features}

  Gini importance : score moyen de diminution d'impureté pour chaque
  variable dans tous les splits où elle est utilisée.
\end{frame}

\begin{frame}
  \frametitle{Bénéfice 2 — proximités}

  Pour les paires de points d'un dataset : ajouter 1 quand elles se
  trouvent dans une feuille, normaliser par le nombre d'arbres.

  \begin{itemize}
  \item métrique pour clustering
  \item imputation de valeurs manquantes (médiane → RF → médiane entre
    proximités → RF → …)
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Bénéfice 3 — variance des prédictions}

  Chaque prédiction est faite par tous les arbres de la forêt : série
  statistique.

  → Calcul de la variance possible
\end{frame}

\section{Gradient boosted trees}

\begin{frame}
  \frametitle{Introduction}

  Arbres qui s'améliorent successivement.
  \begin{figure}
    \centering \imgth[.4]{tree-training-modes}
    \scriptsize{\href{https://quantdare.com/what-is-the-difference-between-bagging-and-boosting/}%
        {quantdare.com/what-is-the-difference-between-bagging-and-boosting/}}
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Approximation 1}

  \begin{enumerate}
  \item partir d'un arbre grossier
  \item entraîner un nouvel arbre sur les résiduels du premier
  \item concaténer le nouvel arbre au premier
  \item \texttt{goto 2.}
  \end{enumerate}

  \green{Quel est l'effet d'entraîner sur les résiduels ?}
\end{frame}

\begin{frame}
  \frametitle{Approximation 2}

  \begin{enumerate}
  \item partir d'un arbre grossier
  \item entraîner un nouvel arbre sur les \textbf{pseudo-résiduels} du
    premier
  \item concaténer le nouvel arbre au premier
  \item \texttt{goto 2.}
  \end{enumerate}

\end{frame}

\begin{frame}
  \frametitle{Pseudo-résiduels}
  \begin{itemize}
  \item chosir une fonction de coût
  \item calculer les pas de descente de gradient étant donné les
    couples $(\hat{y}_i, y_i)$
  \item \textbf{se servir de ces valeurs comme de résiduels}
  \end{itemize}

  → Intérêt : pouvoir utiliser n'importe quel loss dérivable.
\end{frame}

\begin{frame}
  \frametitle{Approximation 3}
  \begin{enumerate}
  \item partir d'un arbre grossier
  \item entraîner un nouvel arbre sur les pseudo-résiduels du premier
  \item \textbf{calculer un multiplicateur pour que l'arbre produit
      minimise le coût}
  \item concaténer le nouvel arbre au premier
  \item \texttt{goto 2.}
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{Véritable modèle}
  \begin{enumerate}
  \item partir d'un arbre grossier
  \item entraîner un nouvel arbre sur les pseudo-résiduels du premier
  \item calculer un multiplicateur pour que l'arbre produit minimise
    le coût
  \item \textbf{appliquer un learning rate}
  \item concaténer le nouvel arbre au premier
  \item \texttt{goto 2.}
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{Idée à retenir}

  \begin{itemize}
  \item séquence d'arbres qui s'entrainent à corriger les erreurs de
    l'arbre d'avant
  \item modélisation de la correction de l'erreur par un pas de
    descente de gradient pour plus de flexibilité.
  \end{itemize}
   
\end{frame}

\begin{frame}
  \frametitle{Extensions}

  \begin{itemize}
  \item row sampling
  \item column sampling
  \item tree structure cost
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Avantages}

  \begin{itemize}
  \item modèle extrêmement performant et versatile
  \item entrainement parallélisable
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Désavantages}

  Sujet à l'overfit si pas assez régularisé (tree structure cost) et
  randomisé (row \& column sampling)
\end{frame}

\section{Conclusion}
\begin{frame}
  \frametitle{Conclusion}

  \begin{itemize}
  \item les arbres sont interprétables, rapides à entraîner,
    combinables.
  \item random forest combine des arbres faibles en un prédicteur
    versatile
  \item les arbres boostés appliquent des corrections séquentielles à
    un arbre initial pour aboutir aux modèles d'arbres les plus
    performants
  \end{itemize}
\end{frame}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
