\documentclass{formation}
\title{Arbres de décision}
\subtitle{Module 5}

\begin{document}

\maketitle

\section{Objectifs}

\begin{frame}
  \frametitle{Objectifs}
  \begin{itemize}
  \item construire un arbre de décision aussi bien pour la régression
    que pour la classification
  \item combiner plusieurs arbres efficacement avec Random Forest
  \end{itemize}
\end{frame}

\section{Arbres de décision}

\begin{frame}
  \frametitle{Introduction}
  Modèle de classification ou regréssion qui classe un input dans une
  de ses feuilles pour rendre sa prédiction :
  \imgtw[0.9]{decision-tree-2}
\end{frame}

\begin{frame}
  \frametitle{Avantages}
  Les arbres de décision
  \begin{itemize}[<+->]
  \item gèrent les inputs numériques comme catégoriels
  \item ne nécessitent pas que la variable d'output soit normalement
    distribuée (regression linéaire)
  \item sont interprétables
  \item sont très rapides durant l'inférence
  \item ne nécessitent pas de normalisation des données
  \item leur apprentissage est hautement parallèlisable
  \end{itemize}
  \onslide<+->{→ Couteau-suisse du machine learning tabulaire.}
\end{frame}

\begin{frame}
  \frametitle{Désavantages}

  \begin{itemize}[<+->]
  \item peuvent overfit les données, mais l'ensembling résoud ce
    problème
  \item sont sensibles aux déséquilibres de classe
  \end{itemize}
  \onslide<+->{→ Si les classes ne sont pas équilibrées, peut-être les
    resampler.}
\end{frame}

\begin{frame}
  \frametitle{Arbres de classification}
  \imgtw[0.9]{decision-tree-2}
\end{frame}

\begin{frame}
  \frametitle{Arbres de régression}
  \imgtw[0.9]{regression-tree}
\end{frame}

\begin{frame}
  \frametitle{Apprendre un arbre de décision}
  
  Approche \og top-down\fg, procédure récursive :
  \begin{itemize}[<+->]
  \item créer un nœud de départ qui contient toutes les instances du
    training set
  \item tant qu'il reste des nœuds non-traités :
    \begin{itemize}
    \item choisir un nœud non traité
    \item si le nœud remplit des conditions de feuille finale, ne rien
      faire
    \item sinon, créer deux branches à partir du nœud non traité
      pour répartir les instances dans deux nouveaux nœuds
    \end{itemize}
  \end{itemize}

  \onslide<+->{Conditions de feuilles finales : contient $n_{min}$
    éléments, est déjà à profondeur $p_{max}$, splitterait sans
    décroître assez l'entropie…}
\end{frame}

\begin{frame}
  \frametitle{Décision rendue}
  
  En fonction de la tâche, une fois arrivé dans la feuille de fin :
  \begin{description}
  \item[Classification] classe majoritaire
  \item[Régression] moyenne des valeurs cibles
  \end{description}
\end{frame}

\begin{frame}
  \frametitle{Splits possibles}

  Splits possibles d'une feature donnée :

  \begin{description}
  \item[Catégorielle] chaque catégorie vs le reste
  \item[Ordinale/Continue] milieu de chaque valeur ou
    quantiles
  \end{description}
\end{frame}

\begin{frame}
  \frametitle{Évaluation de la qualité d'un split}

  En fonction de la tâche :

  \begin{description}
  \item[Régression] coût si on rendait la moyenne des instances
    comme résultat
    \[
      Loss = \sum|\hat{y}-y| \approx variance
    \]
  \item[Classification] Entropie de Shannon :
    \[
    Loss = -\sum_{x \in X}P_x*\log_2(P_x)
    \]
    $= 0 \Rightarrow$ il n'y a pas d'incertitude \\
    maximale quand on a une distribution uniforme
    
  \end{description}
\end{frame}

\begin{frame}
  \frametitle{Exemple — démarrage}
  \begin{columns}
    \begin{column}{.5\textwidth}
      ID, jardinage, jeux vidéos, chapeaux, âge
      \[
        \begin{bmatrix}
          1 & 0 & 1 & 1 & 13  \\
          2 & 0 & 1 & 0 & 14 \\
          3 & 0 & 1 & 0 & 15 \\
          4 & 1 & 1 & 1 & 25 \\
          5 & 0 & 1 & 1 & 35 \\
          6 & 1 & 0 & 0 & 49 \\
          7 & 1 & 1 & 1 & 68 \\
          8 & 1 & 0 & 0 & 71 \\
          9 & 1 & 0 & 1 & 73 \\
        \end{bmatrix}
      \]
    \end{column}
    \begin{column}{.5\textwidth}
      Première étape : création du nœud de départ
      \\[1cm]
      \begin{forest}
        [{1, 2, 3, 4, 5, 6, 7, 8, 9} [{}]]
      \end{forest}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}
  \frametitle{Exemple — split}
  \begin{columns}
    \begin{column}{.5\textwidth}
      ID, jardinage, jeux vidéos, chapeaux, âge
      \[
        \begin{bmatrix}
          1 & 0 & 1 & 1 & 13  \\
          2 & 0 & 1 & 0 & 14 \\
          3 & 0 & 1 & 0 & 15 \\
          4 & 1 & 1 & 1 & 25 \\
          5 & 0 & 1 & 1 & 35 \\
          6 & 1 & 0 & 0 & 49 \\
          7 & 1 & 1 & 1 & 68 \\
          8 & 1 & 0 & 0 & 71 \\
          9 & 1 & 0 & 1 & 73 \\
        \end{bmatrix}
      \]
    \end{column}
    \begin{column}{.5\textwidth}
      Split du premier nœud. Il faut tester 3 splits. Split sur
      jardinage :
      \\[1cm]
      \begin{forest}
        [{1, 2, 3, 4, 5, 6, 7, 8, 9}
          [{4, 6, 7, 8, 9},
          edge label={node[midway,left,font=\scriptsize]{jardinage}}
            [{$\hat{y}=57,2$} [{$L=80,8$}]]]
          [{1, 2, 3, 5},
          edge label={node[midway,right,font=\scriptsize]{$\neg$ jardinage}}
            [{$\hat{y}=19,25$} [{$L=31,5$}]]]
        ]
      \end{forest}

      Loss totale : $122,3$
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}
  \frametitle{Exemple — split}
  \begin{columns}
    \begin{column}{.5\textwidth}
      ID, jardinage, jeux vidéos, chapeaux, âge
      \[
        \begin{bmatrix}
          1 & 0 & 1 & 1 & 13  \\
          2 & 0 & 1 & 0 & 14 \\
          3 & 0 & 1 & 0 & 15 \\
          4 & 1 & 1 & 1 & 25 \\
          5 & 0 & 1 & 1 & 35 \\
          6 & 1 & 0 & 0 & 49 \\
          7 & 1 & 1 & 1 & 68 \\
          8 & 1 & 0 & 0 & 71 \\
          9 & 1 & 0 & 1 & 73 \\
        \end{bmatrix}
      \]
    \end{column}
    \begin{column}{.5\textwidth}
      Split du premier nœud. Il faut tester 3 splits. Split sur
      jeux vidéos :
      \\[1cm]
      \begin{forest}
        [{1, 2, 3, 4, 5, 6, 7, 8, 9}
          [{1, 2, 3, 4, 5, 7},%
          edge label={node[midway,left,font=\scriptsize]{jeux vidéos}}
            [{$\hat{y}=28,3$} [{$L=92,6$}]]]
          [{6, 8, 9},
          edge label={node[midway,right,font=\scriptsize]{$\neg$ jeux vidéos}}
            [{$\hat{y}=64,3$} [{$L=30,7$}]]]
        ]
      \end{forest}

      Loss totale : $123,3$
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}
  \frametitle{Exemple — split}
  \begin{columns}
    \begin{column}{.5\textwidth}
      ID, jardinage, jeux vidéos, chapeaux, âge
      \[
        \begin{bmatrix}
          1 & 0 & 1 & 1 & 13  \\
          2 & 0 & 1 & 0 & 14 \\
          3 & 0 & 1 & 0 & 15 \\
          4 & 1 & 1 & 1 & 25 \\
          5 & 0 & 1 & 1 & 35 \\
          6 & 1 & 0 & 0 & 49 \\
          7 & 1 & 1 & 1 & 68 \\
          8 & 1 & 0 & 0 & 71 \\
          9 & 1 & 0 & 1 & 73 \\
        \end{bmatrix}
      \]
    \end{column}
    \begin{column}{.5\textwidth}
      Split du premier nœud. Il faut tester 3 splits. Split sur
      chapeaux :
      \\[1cm]
      \begin{forest}
        [{1, 2, 3, 4, 5, 6, 7, 8, 9}
          [{1, 4, 5, 7, 9},%
          edge label={node[midway,left,font=\scriptsize]{chapeaux}}
            [{$\hat{y}=42,8$} [{$L=110,8$}]]]
          [{2, 3, 6, 8},
          edge label={node[midway,right,font=\scriptsize]{$\neg$ chapeaux}}
            [{$\hat{y}=37,25$} [{$L=91$}]]]
        ]
      \end{forest}

      Loss totale : $201,8$
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}
  \frametitle{Exemple — split}
  \begin{columns}
    \begin{column}{.5\textwidth}
      ID, jardinage, jeux vidéos, chapeaux, âge
      \[
        \begin{bmatrix}
          1 & 0 & 1 & 1 & 13  \\
          2 & 0 & 1 & 0 & 14 \\
          3 & 0 & 1 & 0 & 15 \\
          4 & 1 & 1 & 1 & 25 \\
          5 & 0 & 1 & 1 & 35 \\
          6 & 1 & 0 & 0 & 49 \\
          7 & 1 & 1 & 1 & 68 \\
          8 & 1 & 0 & 0 & 71 \\
          9 & 1 & 0 & 1 & 73 \\
        \end{bmatrix}
      \]
    \end{column}
    \begin{column}{.5\textwidth}
      \begin{description}
      \item[122,3] jardinage
      \item[123,3] jeux vidéos
      \item[201,8] chapeaux
      \end{description}

      → On split donc sur jardinage
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}
  \frametitle{Exemple — split}
  \begin{columns}
    \begin{column}{.5\textwidth}
      ID, jardinage, jeux vidéos, chapeaux, âge
      \[
        \begin{bmatrix}
          1 & 0 & 1 & 1 & 13  \\
          2 & 0 & 1 & 0 & 14 \\
          3 & 0 & 1 & 0 & 15 \\
          4 & 1 & 1 & 1 & 25 \\
          5 & 0 & 1 & 1 & 35 \\
          6 & 1 & 0 & 0 & 49 \\
          7 & 1 & 1 & 1 & 68 \\
          8 & 1 & 0 & 0 & 71 \\
          9 & 1 & 0 & 1 & 73 \\
        \end{bmatrix}
      \]
    \end{column}
    \begin{column}{.5\textwidth}
      Résultat après le premier split :\\[1cm]

      \begin{forest}
        [{1, 2, 3, 4, 5, 6, 7, 8, 9}
          [{4, 6, 7, 8, 9},
          edge label={node[midway,left,font=\scriptsize]{jardinage}}]
          [{1, 2, 3, 5},
          edge label={node[midway,right,font=\scriptsize]{$\neg$ jardinage}}]
        ]
      \end{forest}

      \onslide<+->{À vous de jouer !}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}
  \frametitle{Limiter l'overfit}

  Fait par :

  \begin{itemize}
  \item la profondeur maximum
  \item le nombre minimum d'instances dans chaque feuille
  \item une baisse d'entropie maximale à chaque split
  \item le nombre minimum d'instances pour split
  \item le pruning
  \end{itemize}
\end{frame}

\section{Random Forest}

\begin{frame}
  \frametitle{Introduction}
  \begin{itemize}
  \item les arbres de décision overfit facilement
  \item ils sont rapides à apprendre
  \item en combiner beaucoup est faisable et réduit la variance
  \end{itemize}

  → création d'une forêt (ensemble d'arbres) aléatoire
\end{frame}

\begin{frame}
  \frametitle{But}
  Produire des arbres décorrélés et moyenner leurs prédictions pour
  réduire la variance.
\end{frame}

\begin{frame}
  \frametitle{Outil 1 — bagging (row sampling)}

  \textbf{B}oostrap \textbf{agg}regat\textbf{ing} (Bagging) :

  \begin{itemize}
  \item tirer un échantillon du dataset avec replacement
  \item entraîner un arbre sur cet échantillon
  \item répéter $B$ fois
  \end{itemize}

  Le bagging s'appelle aussi row sampling.

\end{frame}

\begin{frame}
  \frametitle{Outil 2 — random subspace method (column sampling)}

  \begin{itemize}
  \item à chaque split, considérer seulement un sous-ensemble des
    features
  \item valeurs conseillées :
    \begin{itemize}
    \item classification : $\lvert\sqrt m\rvert$ features par split
    \item regréssion: $\lvert\frac{m}{3}\rvert$ features par split, 5
      exemples par node minimum
    \end{itemize}
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Random Forest}
  \imgtw[1.05]{random-forest-full}
\end{frame}

\begin{frame}
  \frametitle{Random Forest}
  \begin{itemize}
  \item Pas de sur-apprentissage en augmentant le nombre d'arbres
  \item Une fois appris, le modèle est très rapide
  \end{itemize}
\end{frame}

\section{Conclusion}
\begin{frame}
  \frametitle{Conclusion}
  \begin{itemize}
  \item les arbres sont interprétables, rapides à entraîner,
    combinables.
  \item random forest combine des arbres faibles en un prédicteur
    versatile
  \end{itemize}
\end{frame}

\section{Gradient boosted trees}

\begin{frame}
  \frametitle{Introduction}

  Arbres qui s'améliorent successivement.
  \begin{figure}
    \centering \imgth[.4]{tree-training-modes}
    \scriptsize{\href{https://quantdare.com/what-is-the-difference-between-bagging-and-boosting/}%
        {quantdare.com/what-is-the-difference-between-bagging-and-boosting/}}
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Approximation 1}

  \begin{enumerate}
  \item partir d'un arbre grossier
  \item entraîner un nouvel arbre sur les résiduels du premier
  \item concaténer le nouvel arbre au premier
  \item \texttt{goto 2.}
  \end{enumerate}

  \green{Quel est l'effet d'entraîner sur les résiduels ?}
\end{frame}

\begin{frame}
  \frametitle{Approximation 2}

  \begin{enumerate}
  \item partir d'un arbre grossier
  \item entraîner un nouvel arbre sur les \textbf{pseudo-résiduels} du
    premier
  \item concaténer le nouvel arbre au premier
  \item \texttt{goto 2.}
  \end{enumerate}

\end{frame}

\begin{frame}
  \frametitle{Pseudo-résiduels}
  \begin{itemize}
  \item chosir une fonction de coût
  \item calculer les pas de descente de gradient étant donné les
    couples $(\hat{y}_i, y_i)$
  \item \textbf{se servir de ces valeurs comme de résiduels}
  \end{itemize}

  → Intérêt : pouvoir utiliser n'importe quel loss dérivable.
\end{frame}

\begin{frame}
  \frametitle{Approximation 3}
  \begin{enumerate}
  \item partir d'un arbre grossier
  \item entraîner un nouvel arbre sur les pseudo-résiduels du premier
  \item \textbf{calculer un multiplicateur pour que l'arbre produit
      minimise le coût}
  \item concaténer le nouvel arbre au premier
  \item \texttt{goto 2.}
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{Véritable modèle}
  \begin{enumerate}
  \item partir d'un arbre grossier
  \item entraîner un nouvel arbre sur les pseudo-résiduels du premier
  \item calculer un multiplicateur pour que l'arbre produit minimise
    le coût
  \item \textbf{appliquer un learning rate}
  \item concaténer le nouvel arbre au premier
  \item \texttt{goto 2.}
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{Idée à retenir}

  \begin{itemize}
  \item séquence d'arbres qui s'entrainent à corriger les erreurs de
    l'arbre d'avant
  \item modélisation de la correction de l'erreur par un pas de
    descente de gradient pour plus de flexibilité.
  \end{itemize}
   
\end{frame}

\begin{frame}
  \frametitle{Extensions}

  \begin{itemize}
  \item row sampling
  \item column sampling
  \item tree structure cost
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Avantages}

  \begin{itemize}
  \item modèle extrêmement performant et versatile
  \item entrainement parallélisable
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Désavantages}

  Sujet à l'overfit si pas assez régularisé (tree structure cost) et
  randomisé (row \& column sampling)
\end{frame}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
