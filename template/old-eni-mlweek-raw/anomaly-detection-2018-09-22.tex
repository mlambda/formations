\documentclass{formation}
\title{Détection d'anomalies}
\subtitle{Module 6}

\begin{document}

\maketitle

\section{Objectifs}

\begin{frame}
  \frametitle{Objectifs}

  \begin{itemize}
  \item comprendre les différents modes de détection d'anomalie
  \item détecter des anomalies locales et globales
  \item utiliser les méthodes applicables aux grands datasets
  \end{itemize}
\end{frame}

\section{Introduction}

\begin{frame}
  \frametitle{Introduction}

  \begin{itemize}[<+->]
  \item avant, beaucoup utilisée en préprocessing. \green{Pourquoi ?}
  \item récemment beaucoup moins le cas. \green{Pourquoi ?}
  \item très utilisée en :
    \begin{itemize}
    \item détection d'intrusion
    \item détection de fraude
    \item prévention de fuite de données
    \item monitoring de patients
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Définition}
  \begin{itemize}
  \item une anomalie diffère de la norme par ses features
  \item les anomalies sont rares comparées aux instances normales
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Modes de détection d'anomalie}

  \imgtw[.8]{anomaly-modes}

\end{frame}

\begin{frame}
  \frametitle{Mode supervisé}

  Problème de classification normal. Réseaux de neurones et SVM très
  performants.

  \green{Qu'en est-il des arbres ?}

\end{frame}

\begin{frame}
  \frametitle{Modes semi-supervisé}

  Détection de nouveauté. Pas traité ici. One-class SVM très utilisé.
\end{frame}

\begin{frame}
  \frametitle{Mode non-supervisé}

  Méthodes très nombreuses :

  \begin{itemize}[<+->]
  \item statistiques
  \item par voisinnage
  \item par réseaux de neurones
  \item par clustering
  \item par arbres
  \end{itemize}

  \onslide<+->{Souvent lourdes à calculer}
\end{frame}

\begin{frame}
  \frametitle{Mode non-supervisé}

  Programme :

  \begin{itemize}[<+->]
  \item méthode classique (lente)
  \item méthode par clustering
  \item méthode statistique
  \item méthode par arbres
  \item méthode par réseaux de neurones
  \end{itemize}
\end{frame}

\section{Local Outlier Factor}

\begin{frame}
  \frametitle{Local Outlier Factor}

  \begin{columns}
    \begin{column}{.7\tw}
      \begin{itemize}[<+->]
      \item anomalies locales
      \item basé sur les k voisins du point
      \item définit une \og atteignabilité\fg{} par les distances de
        ces voisins
      \item calcule un ratio moyen d'atteignabilité du point et de ses
        voisins
      \end{itemize}
    \end{column}
    \begin{column}{.3\tw}
      \imgtw{lof-density}
    \end{column}
  \end{columns}
  \onslide<+->{→ Anomalie si le ratio moyen d'atteignabilité est
    beaucoup plus faible que celui de ses plus proches voisins}
\end{frame}

\begin{frame}
  \frametitle{Local Outlier Factor}

  \imgtw[.7]{lof}
\end{frame}

\begin{frame}
  \frametitle{Désavantages}
  \begin{itemize}
  \item lent (quadratique)
  \item a des priors sur la distribution des données
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Prior}
  \imgtw[.65]{lof-distance}
\end{frame}

\section{Unweighted Cluster-Based Outlier Factor}

\begin{frame}
  \frametitle{Unweighted Cluster-Based Outlier Factor}

  \begin{itemize}[<+->]
  \item anomalies globales
  \item fonctionne comme LOF mais voisinnage = cluster
  \item clusters calculés avec k-means
  \end{itemize}
  \onslide<+->{→ Anomalie si le ratio moyen d'atteignabilité est
    beaucoup plus faible que celui dans son cluster}
\end{frame}

\begin{frame}
  \frametitle{Unweighted Cluster-Based Outlier Factor}

  \imgtw[.8]{ucblof}
\end{frame}

\section{Histogram-based Outlier Score}

\begin{frame}
  \frametitle{Histogram-Based Outlier Score}

  \begin{itemize}[<+->]
  \item calculer un histogramme pour chaque feature
  \item $HBOS(x) = \sum_{f\in F}\log(\frac{1}{hist_f(x)})$
  \item linéaire sur les données au train, instantané au test
  \item suppose l'indépendance des features \red{(!!!)}
  \item efficace sur les anomalies globales
  \end{itemize}
\end{frame}

\section{Isolation forest}

\begin{frame}
  \frametitle{Isolation tree}
  \begin{itemize}
  \item arbre aléatoire (comme random forest mais le split est
    aléatoire, ExtraTree)
  \item but : isoler une anomalie plus vite qu'un exemple normal
  \item petit chemin pour arriver à une feuille : anomalie
  \end{itemize}

  → Se sert du fait que les features des anomalies ne sont pas
  distribuées comme les autres.
\end{frame}

\begin{frame}
  \frametitle{Isolation forest}

  \begin{itemize}
  \item forêt d'isolation trees
  \item construits sur des sous-échantillons sans replacement des
    données
  \item sous-échantillons plus petits que dans random forest
    typiquement, pour mieux isoler les anomalies
  \item converge souvent vite : 100 arbres souvent suffisants
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Isolation forest}
  \imgtw[.9]{isolation-forest}
\end{frame}

\section{Auto-encodeurs}

\begin{frame}
  \frametitle{Introduction}

  \begin{itemize}[<+->]
  \item auto-encodeur = réseau de neurone
  \item input = output : le réseau apprend à reproduire
  \item pénalisé quelque part pour éviter la copie
  \item anomalie si le réseau reproduit mal
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Introduction}
  \imgtw{autoencoder}
\end{frame}

\begin{frame}
  \frametitle{Auto-encodeur \og standard\fg}

  \begin{itemize}
  \item apprend parce que $\vect{z}$ est plus petit que $\vect{X}$ :
    compression
  \item dur à entrainer : éviter la mémorisation
  \item très faisable cependant avec un recherche d'hyperparamètres
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Auto-encodeur débruiteur}

  \imgtw{denoising-ae}
\end{frame}

\begin{frame}
  \frametitle{Auto-encodeur débruiteur}
  \begin{itemize}
  \item apprend parce que $\vect{X}$ est bruité
  \item (apprend parce que $\vect{z}$ est plus petit que $\vect{X}$ :
    compression)
  \item plus facile à entrainer : la mémorisation devient compliquée
    pour le réseau, en fonction du type de bruit
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Auto-encodeur variationnel}
  \imgtw{vae}
\end{frame}

\begin{frame}
  \frametitle{Auto-encodeur variationnel}
  \begin{itemize}
  \item apprend parce que $\vect{z}$ est une gaussienne : mémorisation
    dure
  \item (apprend parce que $\vect{z}$ est plus petit que $\vect{X}$ :
    compression)
  \item intéressant : vecteurs des écart-types = très bonne
    information pour les anomalies
  \end{itemize}
  → Deux moyens de scorer une anomalie : erreur de reconstruction ou
  écarts-types élevés de $\vect{z}$
\end{frame}
% https://www.kaggle.com/victorambonati/unsupervised-anomaly-detection
\section{Conclusion}

\begin{frame}
  \frametitle{Conclusion}

  \begin{itemize}
  \item plusieurs modes de détection d'anomalies
  \item méthodes globales ou locales
  \item état de l'art : isolation forest et auto-encodeurs
    variationnels
  \end{itemize}
\end{frame}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
