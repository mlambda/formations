\documentclass{formation}

\title{Réseaux de neurones adversaires}

\begin{document}

\maketitle

\begin{frame}
  \frametitle{Objectifs}
  \begin{itemize}
  \item Comprendre un nouveau paradigme d'apprentissage
  \item Découvrir ce que Yann LeCun a appelé la plus grande avancée en
    ML de cette décennie
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Problème}
  \begin{itemize}
  \item Un réseau de neurones apprend des fonctions complexes
  \item Toutefois sa métrique d'évaluation est très simple
  \item Elle dépend aussi du domaine
  \item Elle est \og hardcodée \fg{} par l'humain
  \end{itemize}
  → Un apprentissage est toujours limité par une fonction de perte
  imparfaite.
\end{frame}

\begin{frame}
  \frametitle{Solution}
  Évaluer le réseau de neurones avec un autre réseau de neurones et
  non une fonction de perte.~\cite{Goodfellow2014}
\end{frame}

\begin{frame}
  \frametitle{Les acteurs}
  \begin{itemize}
  \item Un réseau générateur
  \item Un réseau discriminateur
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Le jeu}
  \imgtw{gan-schema}
\end{frame}

\begin{frame}
  \frametitle{Plus formellement}
  Minimax: entraîner les deux réseaux en maximisant des objectifs
  opposés.
  \begin{enumerate}
  \item Maximisation de l'efficacité du discriminateur sur les données
  \item Maximisation des erreurs du discriminateur sur les données
    générées
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{Entraînement du discriminateur}
  \[
    \max_D \underset{x \sim \mathcal{P}_r}{\mathbb{E}}[\log(D(x))] +
    \underset{\tilde{x} \sim \mathcal{P}_g}{\mathbb{E}}[\log(1 -
    D(\tilde{x}))]
  \]
\end{frame}

\begin{frame}
  \frametitle{Entraînement du discriminateur}
  \[
    \red{\max_D \underset{x \sim \mathcal{P}_r}{\mathbb{E}}[\log(D(x))]}
    + \underset{\tilde{x} \sim \mathcal{P}_g}{\mathbb{E}}[\log(1 - D(\tilde{x}))]
  \]
\end{frame}

\begin{frame}
  \frametitle{Entraînement du discriminateur}
  \[
    \red{\max_D} \underset{x \sim
      \mathcal{P}_r}{\mathbb{E}}[\log(D(x))] +
    \red{\underset{\tilde{x} \sim \mathcal{P}_g}{\mathbb{E}}[\log(1 -
      D(\tilde{x}))]}
  \]
\end{frame}

\begin{frame}
  \frametitle{Entraînement du générateur}
  \[
    \min_G \underset{\tilde{x} \sim \mathcal{P}_g}{\mathbb{E}}[\log(1
    - D(\tilde{x}))]
  \]
\end{frame}

\begin{frame}
  \frametitle{Convergence}
  \imgtw{cyclegan}
\end{frame}

\begin{frame}
  \frametitle{Extension: Adversarially Learned Inference}
  Version adversaire du paradigme supervisé
  classique~\cite{Dumoulin2016}
  \begin{itemize}
  \item Discriminateur prédit si un décodeur a produit un couple
    $(\tilde{x}, y)$ ou si un encodeur a produit un couple
    $(x, \tilde{y})$
  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Extension: Pix2Pix}
  Traductions d'image à image supervisée~\cite{Isola2016}.
\end{frame}

\begin{frame}
  \frametitle{Extension: CycleGAN}
  Traductions d'image à image \textbf{non supervisée}~\cite{Zhu2017}.
\end{frame}

\begin{frame}[allowframebreaks]
  \bibliography{papers}
\end{frame}

\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
