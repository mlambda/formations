\documentclass{formation}
\title{Deep Learning par la Pratique}
\subtitle{Apprentissage d'un Réseau de Neurones}

\begin{document}

\maketitle

\begin{frame}
  \frametitle{Rappel}
  Les données en entrée doivent être normalisées :
  \huge
  \begin{center}
    \[
    X_{norm} = \frac{X - mean(X_{train})}{std(X_{train})}
    \]
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Rappel}
  1 - initialisation \textbf{aléatoire} du modèle
  \newline
  2 - Tant que(critère arret == 0)
  \begin{itemize}
  \item Selection aléatoire d'un \textbf{batch} de données
  \item \textbf{Forward} : Passe avant du \textbf{batch} dans le modèle
  \item Calcul de l'erreur par rapport aux sorties attendues
  \item \textbf{Backward} : Rétropropagation du gradient de l'erreur en fonction des paramèrtres dans le modèle (mise à jour du modèle)
  \item Calcul critère arret
  \end{itemize}
  3 - Calcul de l'erreur sur un échantillon de données  \textbf{qui n'ont JAMAIS été vues par le modèle pendant l'apprentissage !}
\end{frame}

\begin{frame}
  \frametitle{Fonction de perte (\textbf{Loss} function)}
  \begin{center}
    \huge{Quelle fonction de perte (Loss) choisir ?}
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Fonction de perte (\textbf{Loss} function)}
  Exemple facile : ``Prédire la température demain'' \\
  \newline
  \newline
  Erreur = $T_{pred} - T_{real}$
\end{frame}


\begin{frame}
  \frametitle{Fonction de perte (\textbf{Loss} function)}
  Exemple difficile : ``Prédire la catégorie d'une image'' \\
  \newline
  $\approx$ Distance entre la sortie et la cible
  \newline
  \newline
  \underline{\textbf{Sortie :}}
  \newline
  \begin{tabular}{|*{10}{c|}}
    \hline
    0.0  & 0.1  & 0.4  & 0.0  & 0.0  & 0.2  & 0.1  & 0.0  & 0.2  & 0.0 \\
    \hline
  \end{tabular}
  \newline
  \newline
  \underline{\textbf{Cible :}}
  \newline
  \begin{tabular}{|*{10}{c|}}
    \hline
    0.0  & 0.0  & \textbf{\red{1.0}}  & 0.0  & 0.0  & 0.0  & 0.0  & 0.0  & 0.0  & 0.0  \\
    \hline
  \end{tabular}
\end{frame}

\begin{frame}
  \frametitle{Fonction de perte (\textbf{Loss} function)}
  Calcul du gradient de l'erreur par rapport aux paramètres:
  \[
  grad_i=\frac{\partial{Err}}{\partial{w_i}}
  \]
  Mise à jour :
  \[
  w_i' = w_i - \gamma * grad_i
  \]
  où : $0 < \gamma < 1$ (learning rate)
  \imgtw[0.6]{gradient}
\end{frame}

\begin{frame}
  \frametitle{Fonction de perte (\textbf{Loss} function)}
  Regression Loss Functions:
  \begin{itemize}
  \item Erreur \textbf{absolue}
  \item Erreur \textbf{lissée}
  \item ...
  \end{itemize}
  Classification Loss Functions:
  \begin{itemize}
  \item Entropies-croisées
  \item Log vraisemblance
  \item Loss à marge
  \item ...
  \end{itemize}
  Embedding Loss Functions:
  \begin{itemize}
  \item L1(,L2,...)
  \item Distance cosinus
  \item ...
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Gradient et mise à jour}
  Exemple de descente de gradient sur un cas simple : la regression
  \begin{itemize}
  \item $y = a.X+b$
  \item L = $\frac{1}{2n}\sum_{i=[1..n]}( y_i^* - y_i )^2$
  \item L = $\frac{1}{2n}\sum_{i=[1..n]}( y_i^* - (a.x_i+b) )^2$
  \item ...
  \item $\frac{\partial{L}}{\partial{a}} = \frac{1}{n}\sum_{i=[1..n]}(a.X+b - y^*).X$
  \item $\frac{\partial{L}}{\partial{b}} = \frac{1}{n}\sum_{i=[1..n]}(a.X+b - y^*)$
  \end{itemize}
  \textbf{M.A.J :}
  \begin{itemize}
  \item $a = a - \gamma\frac{\partial{L}}{\partial{a}}$
  \item $b = b - \gamma\frac{\partial{L}}{\partial{b}}$
  \item où $1 > \gamma > 0$ (learning rate)
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Gradient et mise à jour}
  initialisation au hasard ( $\gamma$ = 0.01 )
  \begin{itemize}
  \item a = 0.58 ($\hat{a} = 3.0$)
  \item b = 0.25 ($\hat{b} = 0.5$)
  \end{itemize}
  \imgth[0.8]{regression-1}
\end{frame}

\begin{frame}
  \frametitle{Gradient et mise à jour}
  \begin{itemize}
  \item a = 0.58 ($\hat{a} = 3.0$)
  \item b = 0.25 ($\hat{b} = 0.5$)
  \end{itemize}
  \imgth[0.8]{regression-error}
\end{frame}

\begin{frame}
  \frametitle{Gradient et mise à jour}
  \begin{itemize}
  \item a = 1.50 ($\hat{a} = 3.0$)
  \item b = 0.35 ($\hat{b} = 0.5$)
  \end{itemize}
  \imgth[0.8]{regression-2}
\end{frame}

\begin{frame}
  \frametitle{Gradient et mise à jour}
  \begin{itemize}
  \item a = 2.10 ($\hat{a} = 3.0$)
  \item b = 0.40 ($\hat{b} = 0.5$)
  \end{itemize}
  \imgth[0.8]{regression-3}
\end{frame}

\begin{frame}
  \frametitle{Gradient et mise à jour}
  \begin{itemize}
  \item a = 2.48 ($\hat{a} = 3.0$)
  \item b = 0.43 ($\hat{b} = 0.5$)
  \end{itemize}
  \imgth[0.8]{regression-4}
\end{frame}

\begin{frame}
  \frametitle{Gradient et mise à jour}
  \begin{itemize}
  \item a = 2.73 ($\hat{a} = 3.0$)
  \item b = 0.46 ($\hat{b} = 0.5$)
  \end{itemize}
  \imgth[0.8]{regression-5}
\end{frame}

\begin{frame}
  \frametitle{Gradient et mise à jour}
  \begin{itemize}
  \item a = 2.89 ($\hat{a} = 3.0$)
  \item b = 0.47 ($\hat{b} = 0.5$)
  \end{itemize}
  \imgth[0.8]{regression-6}
\end{frame}

\begin{frame}
  \frametitle{Gradient et mise à jour}
  \begin{itemize}
  \item a = 2.99 ($\hat{a} = 3.0$)
  \item b = 0.48 ($\hat{b} = 0.5$)
  \end{itemize}
  \imgth[0.8]{regression-7}
\end{frame}

\begin{frame}
  \frametitle{Gradient et mise à jour}
  \begin{itemize}
  \item a = 3.06 ($\hat{a} = 3.0$)
  \item b = 0.49 ($\hat{b} = 0.5$)
  \end{itemize}
  \imgth[0.8]{regression-8}
\end{frame}

\begin{frame}
  \frametitle{Gradient et mise à jour}
  \begin{itemize}
  \item a = 3.10 ($\hat{a} = 3.0$)
  \item b = 0.49 ($\hat{b} = 0.5$)
  \end{itemize}
  \imgth[0.8]{regression-9}
\end{frame}

\begin{frame}
  \frametitle{Gradient et mise à jour}
  \begin{itemize}
  \item a = 3.13 ($\hat{a} = 3.0$)
  \item b = 0.50 ($\hat{b} = 0.5$)
  \end{itemize}
  \imgth[0.8]{regression-10}
\end{frame}

\begin{frame}
  \frametitle{Convergence}
  Convergence du modèle vers l'optimal ?
\end{frame}

\begin{frame}
  \frametitle{Convergence}
  \imgtw[0.9]{surface-parametre-erreur1}
\end{frame}

\begin{frame}
  \frametitle{Convergence}
  \imgtw[0.9]{surface-parametre-erreur2}
\end{frame}

\begin{frame}
  \frametitle{Convergence}
  \imgth[0.9]{surface-parametre-erreur3}
\end{frame}

\begin{frame}
  \frametitle{Evaluation des performances}
  \begin{align*}
    &\mathit{Précision}_i = \frac{\mathit{vrai\:positifs}_i}{\mathit{vrai\:positifs}_i+\mathit{faux\:positif}_i} \\
    &\\
    &\mathit{Précision} = \frac{\sum\limits_{i=1}^n \mathit{précision}_i}{n} \\
    &\\
    &\mathit{Rappel}_i = \frac{\mathit{vrai\:positifs}_i}{\mathit{vrai\:positifs}_i+\mathit{faux\:negatifs}_i} \\
    &\\
    &\mathit{Rappel} = \frac{\sum\limits_{i=1}^n \mathit{rappel}_i}{n}
  \end{align*}
\end{frame}

\begin{frame}
  \frametitle{Evaluation des performances}
  \imgth{precisionrecall}
\end{frame}

\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
